cd /tmp/Nopol_Bears_debezium-debezium_351465554-354212780;
export JAVA_TOOL_OPTIONS="-Dfile.encoding=UTF8 -Duser.language=en-US -Duser.country=US -Duser.language=en";
TZ="America/New_York"; export TZ;
export PATH="/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/:$PATH";
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/";
time java -Xmx4048m -cp /home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar fr.inria.lille.repair.Main \
	--mode repair \
	--type pre_then_cond \
	--oracle angelic \
	--synthesis smt \
	--flocal gzoltar \
	--json \
	--solver z3 \
	--solver-path /home/tdurieux/RepairThemAll/script/../libs/z3/build/z3 \
	--complianceLevel 8 \
	--source debezium-connector-mongodb/src/main:support/checkstyle/src/main:debezium-core/src/main:debezium-assembly-descriptors/src/main:debezium-connector-postgres/src/main:debezium-connector-mysql/src/main:debezium-embedded/src/main \
	--classpath "debezium-connector-mongodb/target/classes:support/checkstyle/target/classes:debezium-core/target/classes:debezium-assembly-descriptors/target/classes:debezium-connector-postgres/target/classes:debezium-connector-mysql/target/classes:debezium-embedded/target/classes:debezium-connector-mongodb/target/test-classes:debezium-core/target/test-classes:debezium-connector-postgres/target/test-classes:debezium-connector-mysql/target/test-classes:debezium-embedded/target/test-classes:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/com/github/shyiko/mysql-binlog-connector-java/0.13.0/mysql-binlog-connector-java-0.13.0.jar:/home/tdurieux/.m2/repository/mil/nga/wkb/1.0.2/wkb-1.0.2.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver/3.4.2/mongodb-driver-3.4.2.jar:/home/tdurieux/.m2/repository/org/mongodb/bson/3.4.2/bson-3.4.2.jar:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver-core/3.4.2/mongodb-driver-core-3.4.2.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka_2.11/1.0.1/kafka_2.11-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/home/tdurieux/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/tdurieux/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/org/apache/curator/curator-test/2.11.0/curator-test-2.11.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/com/puppycrawl/tools/checkstyle/6.7/checkstyle-6.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/tdurieux/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/tdurieux/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/tdurieux/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.3/commons-beanutils-core-1.8.3.jar:/home/tdurieux/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/home/tdurieux/.m2/repository/commons-cli/commons-cli/1.3/commons-cli-1.3.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/18.0/guava-18.0.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/postgresql/postgresql/42.0.0/postgresql-42.0.0.jar:/home/tdurieux/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar";
	echo "\n\nNode: `hostname`\n";
	echo "\n\nDate: `date`\n";
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8 -Duser.language=en-US -Duser.country=US -Duser.language=en
13:43:42.710 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Source files: [debezium-connector-mongodb/src/main, support/checkstyle/src/main, debezium-core/src/main, debezium-assembly-descriptors/src/main, debezium-connector-postgres/src/main, debezium-connector-mysql/src/main, debezium-embedded/src/main]
13:43:42.714 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Classpath: [file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mongodb/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/support/checkstyle/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-core/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-assembly-descriptors/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-postgres/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mysql/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-embedded/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mongodb/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-core/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-postgres/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mysql/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-embedded/target/test-classes/, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/github/shyiko/mysql-binlog-connector-java/0.13.0/mysql-binlog-connector-java-0.13.0.jar, file:/home/tdurieux/.m2/repository/mil/nga/wkb/1.0.2/wkb-1.0.2.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver/3.4.2/mongodb-driver-3.4.2.jar, file:/home/tdurieux/.m2/repository/org/mongodb/bson/3.4.2/bson-3.4.2.jar, file:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver-core/3.4.2/mongodb-driver-core-3.4.2.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka_2.11/1.0.1/kafka_2.11-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar, file:/home/tdurieux/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar, file:/home/tdurieux/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/org/apache/curator/curator-test/2.11.0/curator-test-2.11.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/16.0.1/guava-16.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/com/puppycrawl/tools/checkstyle/6.7/checkstyle-6.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar, file:/home/tdurieux/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar, file:/home/tdurieux/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar, file:/home/tdurieux/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.3/commons-beanutils-core-1.8.3.jar, file:/home/tdurieux/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar, file:/home/tdurieux/.m2/repository/commons-cli/commons-cli/1.3/commons-cli-1.3.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/18.0/guava-18.0.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/postgresql/postgresql/42.0.0/postgresql-42.0.0.jar, file:/home/tdurieux/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar]
13:43:42.715 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Statement type: PRE_THEN_COND
13:43:42.715 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Args: [io.debezium.config.ConfigurationTest, io.debezium.connector.mongodb.CollectionIdTest, io.debezium.connector.mongodb.ConnectionIT, io.debezium.connector.mongodb.FiltersTest, io.debezium.connector.mongodb.ModuleTest, io.debezium.connector.mongodb.MongoClientsIT, io.debezium.connector.mongodb.MongoDbConnectorIT, io.debezium.connector.mongodb.MongoDbConnectorTest, io.debezium.connector.mongodb.MongoUtilTest, io.debezium.connector.mongodb.RecordMakersTest, io.debezium.connector.mongodb.ReplicaSetsTest, io.debezium.connector.mongodb.ReplicatorIT, io.debezium.connector.mongodb.SourceInfoTest, io.debezium.connector.mongodb.TopicSelectorTest, io.debezium.connector.mongodb.transforms.MongoDataConverterTest, io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelopeTest, io.debezium.connector.mysql.BinlogReaderBufferIT, io.debezium.connector.mysql.BinlogReaderIT, io.debezium.connector.mysql.ChainedReaderTest, io.debezium.connector.mysql.ConnectionIT, io.debezium.connector.mysql.FiltersTest, io.debezium.connector.mysql.GtidSetTest, io.debezium.connector.mysql.MetadataIT, io.debezium.connector.mysql.MySqlConnectorIT, io.debezium.connector.mysql.MySqlConnectorJsonIT, io.debezium.connector.mysql.MySqlConnectorRegressionIT, io.debezium.connector.mysql.MySqlConnectorTest, io.debezium.connector.mysql.MySqlDdlParserTest, io.debezium.connector.mysql.MySqlFixedLengthBinaryColumnIT, io.debezium.connector.mysql.MySqlGeometryIT, io.debezium.connector.mysql.MySqlGeometryTest, io.debezium.connector.mysql.MySqlSchemaTest, io.debezium.connector.mysql.MySqlTableMaintenanceStatementsIT, io.debezium.connector.mysql.MySqlTaskContextIT, io.debezium.connector.mysql.MySqlTaskContextTest, io.debezium.connector.mysql.MySqlUnsignedIntegerConverterTest, io.debezium.connector.mysql.MySqlUnsignedIntegerIT, io.debezium.connector.mysql.MySqlValueConvertersTest, io.debezium.connector.mysql.ReadBinLogIT, io.debezium.connector.mysql.SnapshotReaderIT, io.debezium.connector.mysql.SourceInfoTest, io.debezium.connector.mysql.TableConvertersTest, io.debezium.connector.postgresql.CustomTypeEncodingTest, io.debezium.connector.postgresql.PostgresConnectorIT, io.debezium.connector.postgresql.PostgresConnectorTaskIT, io.debezium.connector.postgresql.PostgresSchemaIT, io.debezium.connector.postgresql.RecordsSnapshotProducerIT, io.debezium.connector.postgresql.RecordsStreamProducerIT, io.debezium.connector.postgresql.SnapshotWithOverridesProducerIT, io.debezium.connector.postgresql.SourceInfoTest, io.debezium.connector.postgresql.connection.PostgresConnectionIT, io.debezium.connector.postgresql.connection.ReplicationConnectionIT, io.debezium.connector.postgresql.connection.wal2json.ISODateTimeFormatTest, io.debezium.connector.simple.SimpleSourceConnectorOutputTest, io.debezium.data.EnvelopeTest, io.debezium.document.ArraySerdesTest, io.debezium.document.DocumentSerdesTest, io.debezium.document.DocumentTest, io.debezium.document.JacksonArrayReadingAndWritingTest, io.debezium.document.JacksonWriterTest, io.debezium.document.PathsTest, io.debezium.embedded.EmbeddedEngineTest, io.debezium.embedded.OffsetCommitPolicyTest, io.debezium.function.BufferedBlockingConsumerTest, io.debezium.function.PredicatesTest, io.debezium.kafka.KafkaClusterTest, io.debezium.kafka.ZookeeperServerTest, io.debezium.relational.ColumnEditorTest, io.debezium.relational.SelectorsTest, io.debezium.relational.TableEditorTest, io.debezium.relational.TableIdParserTest, io.debezium.relational.TableSchemaBuilderTest, io.debezium.relational.TableTest, io.debezium.relational.ddl.DataTypeGrammarParserTest, io.debezium.relational.ddl.DataTypeParserTest, io.debezium.relational.ddl.DdlChangesTest, io.debezium.relational.ddl.DdlParserSql2003Test, io.debezium.relational.history.FileDatabaseHistoryTest, io.debezium.relational.history.HistoryRecordTest, io.debezium.relational.history.KafkaDatabaseHistoryTest, io.debezium.relational.history.MemoryDatabaseHistoryTest, io.debezium.relational.mapping.ColumnMappersTest, io.debezium.relational.mapping.MaskStringsTest, io.debezium.relational.mapping.TruncateStringsTest, io.debezium.text.PositionTest, io.debezium.text.TokenStreamTest, io.debezium.text.XmlCharactersTest, io.debezium.time.ConversionsTest, io.debezium.time.TemporalsTest, io.debezium.transforms.ByLogicalTableRouterTest, io.debezium.transforms.UnwrapFromEnvelopeTest, io.debezium.util.ElapsedTimeStrategyTest, io.debezium.util.HashCodeTest, io.debezium.util.SchemaNameAdjusterTest, io.debezium.util.StringsTest, io.debezium.util.TestingTest]
13:43:42.717 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Config: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=true, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=PRE_THEN_COND, synthesis=SMT, oracle=ANGELIC, solver=Z3, solverPath='/home/tdurieux/RepairThemAll/script/../libs/z3/build/z3', projectSources=[debezium-connector-mongodb/src/main, support/checkstyle/src/main, debezium-core/src/main, debezium-assembly-descriptors/src/main, debezium-connector-postgres/src/main, debezium-connector-mysql/src/main, debezium-embedded/src/main], projectClasspath='[Ljava.net.URL;@45c8e616', projectTests=[io.debezium.config.ConfigurationTest, io.debezium.connector.mongodb.CollectionIdTest, io.debezium.connector.mongodb.ConnectionIT, io.debezium.connector.mongodb.FiltersTest, io.debezium.connector.mongodb.ModuleTest, io.debezium.connector.mongodb.MongoClientsIT, io.debezium.connector.mongodb.MongoDbConnectorIT, io.debezium.connector.mongodb.MongoDbConnectorTest, io.debezium.connector.mongodb.MongoUtilTest, io.debezium.connector.mongodb.RecordMakersTest, io.debezium.connector.mongodb.ReplicaSetsTest, io.debezium.connector.mongodb.ReplicatorIT, io.debezium.connector.mongodb.SourceInfoTest, io.debezium.connector.mongodb.TopicSelectorTest, io.debezium.connector.mongodb.transforms.MongoDataConverterTest, io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelopeTest, io.debezium.connector.mysql.BinlogReaderBufferIT, io.debezium.connector.mysql.BinlogReaderIT, io.debezium.connector.mysql.ChainedReaderTest, io.debezium.connector.mysql.ConnectionIT, io.debezium.connector.mysql.FiltersTest, io.debezium.connector.mysql.GtidSetTest, io.debezium.connector.mysql.MetadataIT, io.debezium.connector.mysql.MySqlConnectorIT, io.debezium.connector.mysql.MySqlConnectorJsonIT, io.debezium.connector.mysql.MySqlConnectorRegressionIT, io.debezium.connector.mysql.MySqlConnectorTest, io.debezium.connector.mysql.MySqlDdlParserTest, io.debezium.connector.mysql.MySqlFixedLengthBinaryColumnIT, io.debezium.connector.mysql.MySqlGeometryIT, io.debezium.connector.mysql.MySqlGeometryTest, io.debezium.connector.mysql.MySqlSchemaTest, io.debezium.connector.mysql.MySqlTableMaintenanceStatementsIT, io.debezium.connector.mysql.MySqlTaskContextIT, io.debezium.connector.mysql.MySqlTaskContextTest, io.debezium.connector.mysql.MySqlUnsignedIntegerConverterTest, io.debezium.connector.mysql.MySqlUnsignedIntegerIT, io.debezium.connector.mysql.MySqlValueConvertersTest, io.debezium.connector.mysql.ReadBinLogIT, io.debezium.connector.mysql.SnapshotReaderIT, io.debezium.connector.mysql.SourceInfoTest, io.debezium.connector.mysql.TableConvertersTest, io.debezium.connector.postgresql.CustomTypeEncodingTest, io.debezium.connector.postgresql.PostgresConnectorIT, io.debezium.connector.postgresql.PostgresConnectorTaskIT, io.debezium.connector.postgresql.PostgresSchemaIT, io.debezium.connector.postgresql.RecordsSnapshotProducerIT, io.debezium.connector.postgresql.RecordsStreamProducerIT, io.debezium.connector.postgresql.SnapshotWithOverridesProducerIT, io.debezium.connector.postgresql.SourceInfoTest, io.debezium.connector.postgresql.connection.PostgresConnectionIT, io.debezium.connector.postgresql.connection.ReplicationConnectionIT, io.debezium.connector.postgresql.connection.wal2json.ISODateTimeFormatTest, io.debezium.connector.simple.SimpleSourceConnectorOutputTest, io.debezium.data.EnvelopeTest, io.debezium.document.ArraySerdesTest, io.debezium.document.DocumentSerdesTest, io.debezium.document.DocumentTest, io.debezium.document.JacksonArrayReadingAndWritingTest, io.debezium.document.JacksonWriterTest, io.debezium.document.PathsTest, io.debezium.embedded.EmbeddedEngineTest, io.debezium.embedded.OffsetCommitPolicyTest, io.debezium.function.BufferedBlockingConsumerTest, io.debezium.function.PredicatesTest, io.debezium.kafka.KafkaClusterTest, io.debezium.kafka.ZookeeperServerTest, io.debezium.relational.ColumnEditorTest, io.debezium.relational.SelectorsTest, io.debezium.relational.TableEditorTest, io.debezium.relational.TableIdParserTest, io.debezium.relational.TableSchemaBuilderTest, io.debezium.relational.TableTest, io.debezium.relational.ddl.DataTypeGrammarParserTest, io.debezium.relational.ddl.DataTypeParserTest, io.debezium.relational.ddl.DdlChangesTest, io.debezium.relational.ddl.DdlParserSql2003Test, io.debezium.relational.history.FileDatabaseHistoryTest, io.debezium.relational.history.HistoryRecordTest, io.debezium.relational.history.KafkaDatabaseHistoryTest, io.debezium.relational.history.MemoryDatabaseHistoryTest, io.debezium.relational.mapping.ColumnMappersTest, io.debezium.relational.mapping.MaskStringsTest, io.debezium.relational.mapping.TruncateStringsTest, io.debezium.text.PositionTest, io.debezium.text.TokenStreamTest, io.debezium.text.XmlCharactersTest, io.debezium.time.ConversionsTest, io.debezium.time.TemporalsTest, io.debezium.transforms.ByLogicalTableRouterTest, io.debezium.transforms.UnwrapFromEnvelopeTest, io.debezium.util.ElapsedTimeStrategyTest, io.debezium.util.HashCodeTest, io.debezium.util.SchemaNameAdjusterTest, io.debezium.util.StringsTest, io.debezium.util.TestingTest], complianceLevel=8, outputFolder=., json=true}
13:43:42.717 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Available processors (cores): 24
13:43:42.724 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Free memory: 1 GB
13:43:42.724 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Maximum memory: 3 GB
13:43:42.724 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Total memory available to JVM: 1 GB
13:43:42.724 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Java version: 1.8.0_181
13:43:42.725 [main] INFO  fr.inria.lille.repair.nopol.NoPol - JAVA_HOME: /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/
13:43:42.725 [main] INFO  fr.inria.lille.repair.nopol.NoPol - PATH: /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/bin:/usr/games
13:43:55.299 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:201 which is executed by 67 tests
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:201
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #2
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:450 which is executed by 67 tests
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:450
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #3
13:43:55.300 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:449 which is executed by 67 tests
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:449
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #4
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:407 which is executed by 67 tests
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:407
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #5
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:406 which is executed by 73 tests
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:406
13:43:55.301 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #6
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:405 which is executed by 73 tests
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:405
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #7
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:402 which is executed by 73 tests
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:402
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #8
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:260 which is executed by 73 tests
13:43:55.302 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:260
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #9
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:259 which is executed by 73 tests
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:259
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #10
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:55 which is executed by 49 tests
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:55
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #11
13:43:55.303 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:617 which is executed by 49 tests
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:617
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #12
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:615 which is executed by 49 tests
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:615
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #13
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:614 which is executed by 49 tests
13:43:55.304 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:614
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #14
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:607 which is executed by 49 tests
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:607
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #15
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:600 which is executed by 49 tests
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:600
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #16
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:593 which is executed by 49 tests
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:593
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #17
13:43:55.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:591 which is executed by 49 tests
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:591
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #18
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:590 which is executed by 49 tests
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:590
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #19
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:589 which is executed by 49 tests
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:589
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #20
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:588 which is executed by 49 tests
13:43:55.306 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:588
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #21
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:587 which is executed by 49 tests
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:587
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #22
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:580 which is executed by 49 tests
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:580
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #23
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:570 which is executed by 49 tests
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:570
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #24
13:43:55.307 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:569 which is executed by 49 tests
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:569
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #25
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:568 which is executed by 49 tests
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:568
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #26
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:567 which is executed by 49 tests
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:567
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #27
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:560 which is executed by 49 tests
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:560
13:43:55.308 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #28
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:559 which is executed by 49 tests
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:559
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #29
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:558 which is executed by 49 tests
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:558
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #30
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:557 which is executed by 49 tests
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:557
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #31
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:556 which is executed by 49 tests
13:43:55.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:556
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #32
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:554 which is executed by 49 tests
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:554
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #33
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:553 which is executed by 49 tests
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:553
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #34
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:549 which is executed by 49 tests
13:43:55.310 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:549
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #35
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:548 which is executed by 49 tests
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:548
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #36
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:547 which is executed by 49 tests
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:547
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #37
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:546 which is executed by 49 tests
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:546
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #38
13:43:55.311 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:543 which is executed by 49 tests
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:543
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #39
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:539 which is executed by 49 tests
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:539
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #40
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:538 which is executed by 49 tests
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:538
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #41
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:537 which is executed by 49 tests
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:537
13:43:55.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #42
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:535 which is executed by 49 tests
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:535
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #43
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:534 which is executed by 49 tests
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:534
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #44
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:533 which is executed by 49 tests
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:533
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #45
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:532 which is executed by 49 tests
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:532
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #46
13:43:55.313 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:531 which is executed by 49 tests
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:531
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #47
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:529 which is executed by 49 tests
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:529
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #48
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:526 which is executed by 49 tests
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:526
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #49
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:525 which is executed by 49 tests
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:525
13:43:55.314 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #50
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:524 which is executed by 49 tests
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:524
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #51
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:523 which is executed by 49 tests
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:523
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #52
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:521 which is executed by 49 tests
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:521
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #53
13:43:55.315 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:518 which is executed by 49 tests
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:518
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #54
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:513 which is executed by 49 tests
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:513
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #55
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:512 which is executed by 49 tests
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:512
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #56
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:510 which is executed by 49 tests
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:510
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #57
13:43:55.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:509 which is executed by 49 tests
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:509
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #58
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:507 which is executed by 49 tests
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:507
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #59
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:431 which is executed by 49 tests
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:431
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #60
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:287 which is executed by 49 tests
13:43:55.317 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:287
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #61
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:286 which is executed by 49 tests
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:286
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #62
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:285 which is executed by 49 tests
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:285
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #63
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:259 which is executed by 49 tests
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:259
13:43:55.318 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #64
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:256 which is executed by 49 tests
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:256
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #65
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:255 which is executed by 49 tests
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:255
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #66
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:247 which is executed by 49 tests
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:247
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #67
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:246 which is executed by 49 tests
13:43:55.319 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:246
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #68
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:245 which is executed by 49 tests
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:245
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #69
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:235 which is executed by 49 tests
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:235
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #70
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:233 which is executed by 49 tests
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:233
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #71
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:227 which is executed by 49 tests
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:227
13:43:55.320 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #72
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:224 which is executed by 49 tests
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:224
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #73
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:223 which is executed by 49 tests
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:223
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #74
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:217 which is executed by 49 tests
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:217
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #75
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:216 which is executed by 49 tests
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:216
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #76
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:215 which is executed by 49 tests
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:215
13:43:55.321 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #77
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:214 which is executed by 49 tests
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:214
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #78
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:213 which is executed by 49 tests
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:213
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #79
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:212 which is executed by 49 tests
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:212
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #80
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:209 which is executed by 49 tests
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:209
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #81
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:205 which is executed by 49 tests
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:205
13:43:55.322 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #82
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:146 which is executed by 49 tests
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:146
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #83
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:138 which is executed by 49 tests
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:138
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #84
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:134 which is executed by 49 tests
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:134
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #85
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:133 which is executed by 49 tests
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:133
13:43:55.323 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #86
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:132 which is executed by 49 tests
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:132
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #87
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:126 which is executed by 49 tests
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:126
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #88
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:121 which is executed by 49 tests
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:121
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #89
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:117 which is executed by 49 tests
13:43:55.324 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:117
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #90
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:116 which is executed by 49 tests
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:116
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #91
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:106 which is executed by 49 tests
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:106
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #92
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:102 which is executed by 49 tests
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:102
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #93
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:99 which is executed by 49 tests
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:99
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #94
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:95 which is executed by 49 tests
13:43:55.325 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:95
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #95
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:93 which is executed by 49 tests
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:93
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #96
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:86 which is executed by 49 tests
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:86
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #97
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:85 which is executed by 49 tests
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:85
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #98
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:79 which is executed by 49 tests
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:79
13:43:55.326 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #99
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:38 which is executed by 49 tests
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:38
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #100
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:37 which is executed by 49 tests
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:37
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #101
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:35 which is executed by 49 tests
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:35
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #102
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:34 which is executed by 49 tests
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:34
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #103
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:33 which is executed by 49 tests
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:33
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #104
13:43:55.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:32 which is executed by 49 tests
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:32
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #105
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:31 which is executed by 49 tests
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:31
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #106
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:265 which is executed by 49 tests
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:265
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #107
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:261 which is executed by 49 tests
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:261
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #108
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:260 which is executed by 49 tests
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:260
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #109
13:43:55.328 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:256 which is executed by 49 tests
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:256
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #110
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:144 which is executed by 49 tests
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:144
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #111
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:142 which is executed by 49 tests
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:142
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #112
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:136 which is executed by 49 tests
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:136
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #113
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:135 which is executed by 49 tests
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:135
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #114
13:43:55.329 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:134 which is executed by 49 tests
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:134
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #115
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:133 which is executed by 49 tests
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:133
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #116
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:132 which is executed by 49 tests
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:132
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #117
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:131 which is executed by 49 tests
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:131
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #118
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:129 which is executed by 49 tests
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:129
13:43:55.330 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #119
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:127 which is executed by 49 tests
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:127
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #120
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:122 which is executed by 49 tests
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:122
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #121
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:121 which is executed by 49 tests
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:121
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #122
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:116 which is executed by 49 tests
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:116
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #123
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:107 which is executed by 49 tests
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:107
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #124
13:43:55.331 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:93 which is executed by 49 tests
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:93
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #125
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:91 which is executed by 49 tests
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:91
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #126
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:90 which is executed by 49 tests
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:90
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #127
13:43:55.332 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostChooserFactory:20 which is executed by 49 tests
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostChooserFactory:20
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #128
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostChooserFactory:19 which is executed by 49 tests
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostChooserFactory:19
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #129
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.SingleHostChooser:26 which is executed by 49 tests
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.SingleHostChooser:26
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #130
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostRequirement:11 which is executed by 50 tests
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostRequirement:11
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #131
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:525 which is executed by 49 tests
13:43:55.333 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:525
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #132
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:523 which is executed by 49 tests
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:523
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #133
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:499 which is executed by 49 tests
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:499
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #134
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:490 which is executed by 49 tests
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:490
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #135
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:488 which is executed by 49 tests
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:488
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #136
13:43:55.334 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:485 which is executed by 49 tests
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:485
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #137
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:475 which is executed by 49 tests
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:475
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #138
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.PSQLState:16 which is executed by 49 tests
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.PSQLState:16
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #139
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:40 which is executed by 49 tests
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:40
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #140
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:29 which is executed by 49 tests
13:43:55.335 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:29
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #141
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:25 which is executed by 49 tests
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:25
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #142
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:21 which is executed by 49 tests
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:21
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #143
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:34 which is executed by 49 tests
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:34
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #144
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:33 which is executed by 49 tests
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:33
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #145
13:43:55.336 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:32 which is executed by 49 tests
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:32
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #146
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:62 which is executed by 49 tests
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:62
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #147
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:59 which is executed by 49 tests
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:59
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #148
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:58 which is executed by 49 tests
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:58
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #149
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:52 which is executed by 49 tests
13:43:55.337 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:52
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #150
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:39 which is executed by 49 tests
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:39
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #151
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:24 which is executed by 49 tests
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:24
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #152
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1017 which is executed by 49 tests
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1017
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #153
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1016 which is executed by 49 tests
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1016
13:43:55.338 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #154
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1015 which is executed by 49 tests
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1015
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #155
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1008 which is executed by 49 tests
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1008
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #156
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1007 which is executed by 49 tests
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1007
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #157
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1006 which is executed by 49 tests
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1006
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #158
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1001 which is executed by 49 tests
13:43:55.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1001
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #159
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver$1:86 which is executed by 49 tests
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver$1:86
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #160
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver$1:88 which is executed by 49 tests
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver$1:88
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #161
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:49 which is executed by 49 tests
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:49
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #162
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:48 which is executed by 49 tests
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:48
13:43:55.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #163
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:47 which is executed by 49 tests
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:47
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #164
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:45 which is executed by 49 tests
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:45
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #165
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:47 which is executed by 48 tests
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:47
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #166
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:44 which is executed by 48 tests
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:44
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #167
13:43:55.341 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:41 which is executed by 48 tests
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:41
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #168
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:35 which is executed by 48 tests
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:35
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #169
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:34 which is executed by 48 tests
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:34
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #170
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:111 which is executed by 187 tests
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:111
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #171
13:43:55.342 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:107 which is executed by 187 tests
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:107
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #172
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:103 which is executed by 187 tests
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:103
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #173
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:68 which is executed by 187 tests
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:68
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #174
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:281 which is executed by 187 tests
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:281
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #175
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.ContextSelectorStaticBinder:104 which is executed by 187 tests
13:43:55.343 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.ContextSelectorStaticBinder:104
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #176
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:34 which is executed by 187 tests
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:34
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #177
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:30 which is executed by 187 tests
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:30
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #178
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:45 which is executed by 187 tests
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:45
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #179
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:124 which is executed by 187 tests
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:124
13:43:55.344 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #180
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:122 which is executed by 187 tests
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:122
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #181
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:118 which is executed by 187 tests
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:118
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #182
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:117 which is executed by 187 tests
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:117
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #183
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:113 which is executed by 187 tests
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:113
13:43:55.345 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #184
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:107 which is executed by 187 tests
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:107
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #185
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:299 which is executed by 188 tests
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:299
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #186
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:297 which is executed by 188 tests
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:297
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #187
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:293 which is executed by 188 tests
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:293
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #188
13:43:55.346 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:270 which is executed by 188 tests
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:270
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #189
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:269 which is executed by 188 tests
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:269
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #190
13:43:55.347 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.AbstractSourceInfo:26 which is executed by 26 tests
1884225868
13:43:55.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #191
13:43:55.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:125 which is executed by 159 tests
13:43:55.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:125
13:43:55.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #192
13:43:55.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mongodb.Module:22 which is executed by 25 tests
-1984916851
13:43:55.780 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #193
13:43:55.781 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:302 which is executed by 21 tests
380667182
13:43:55.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #194
13:43:55.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:269 which is executed by 21 tests
380667182
13:43:56.118 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #195
13:43:56.118 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:372 which is executed by 21 tests
380667182
13:43:56.280 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #196
13:43:56.281 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:337 which is executed by 21 tests
380667182
13:43:56.433 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #197
13:43:56.433 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:42 which is executed by 22 tests
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:42
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #198
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:39 which is executed by 22 tests
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:39
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #199
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:26 which is executed by 20 tests
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:26
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #200
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:25 which is executed by 20 tests
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:25
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #201
13:43:56.434 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:330 which is executed by 20 tests
380667182
13:43:56.583 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #202
13:43:56.583 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:329 which is executed by 20 tests
380667182
13:43:56.743 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,292 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,295 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,296 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,296 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,296 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,325 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,325 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,325 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,325 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,325 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,344 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,344 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,345 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,345 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,345 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,364 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,364 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,364 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,364 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,364 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,386 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,386 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,386 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,386 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,387 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,409 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,409 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,409 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,409 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,409 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,427 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,427 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,427 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,427 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,427 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,446 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,446 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,446 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,446 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,446 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,459 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,459 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,459 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,460 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,460 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,471 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,472 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,472 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,472 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,472 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:57,484 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,484 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,484 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,484 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,484 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,495 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,495 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,496 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,496 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,496 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,505 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,505 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,505 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,505 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,505 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,514 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,514 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,514 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,514 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,514 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,526 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,526 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,526 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,526 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,526 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,538 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,538 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,538 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,538 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,539 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,548 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,548 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,548 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,548 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,548 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,556 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,556 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,556 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,556 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,557 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,565 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,565 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,565 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,565 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,566 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,574 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,574 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,574 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,574 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:57,574 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:43:57.577 [pool-4-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (801 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:43:59.098 [pool-5-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (195 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:43:59.102 [pool-6-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:43:59.107 [pool-7-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:43:59.109 [pool-8-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:43:59.110 [pool-3-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:43:59.110 [pool-3-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:43:59.110 [pool-3-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:329.
13:43:59.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #203
13:43:59.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:328 which is executed by 20 tests
380667182
13:43:59.248 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #204
13:43:59.248 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:315 which is executed by 20 tests
380667182
13:43:59.379 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,698 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,701 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,701 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,701 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,701 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,720 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,720 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,720 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,720 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,720 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,735 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,735 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,735 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,735 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,735 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,749 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,749 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,749 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,749 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,749 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,764 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,764 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,764 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,764 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,764 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,779 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,779 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,779 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,779 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,779 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,793 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,793 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,793 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,793 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,793 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,808 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,808 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,808 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,808 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,808 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,821 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,821 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,821 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,821 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,821 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,833 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,834 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,834 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,834 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,834 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:43:59,846 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,846 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,846 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,846 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,846 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,855 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,855 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,855 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,855 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,855 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,865 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,865 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,865 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,866 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,866 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,875 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,875 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,875 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,875 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,875 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,885 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,885 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,885 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,885 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,885 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,896 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,896 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,896 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,896 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,896 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,904 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,904 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,904 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,904 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,904 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,915 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,915 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,915 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,915 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,915 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,922 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,923 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,923 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,923 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,923 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,930 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,930 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,930 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,930 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:43:59,930 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:43:59.931 [pool-10-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (541 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:00.730 [pool-11-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (201 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:00.734 [pool-12-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:00.736 [pool-13-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:00.739 [pool-14-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:00.739 [pool-9-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:00.740 [pool-9-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:00.740 [pool-9-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:315.
13:44:00.740 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #205
13:44:00.740 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:314 which is executed by 20 tests
380667182
13:44:00.888 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,172 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,174 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,174 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,174 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,174 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,192 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,192 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,192 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,192 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,192 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,208 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,208 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,208 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,208 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,208 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,220 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,220 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,220 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,220 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,220 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,239 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,239 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,239 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,239 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,239 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,257 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,257 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,258 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,258 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,258 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,275 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,275 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,275 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,275 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,275 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,293 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,294 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,294 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,294 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,294 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,309 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,309 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,309 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,309 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,310 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,324 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,324 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,324 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,324 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,324 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:01,334 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,334 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,334 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,334 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,334 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,341 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,342 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,342 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,342 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,342 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,350 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,350 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,351 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,351 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,351 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,358 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,358 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,358 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,359 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,359 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,366 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,366 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,366 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,366 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,366 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,376 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,376 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,376 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,376 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,376 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,383 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,383 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,383 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,383 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,383 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,391 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,391 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,391 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,391 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,391 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,398 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,398 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,398 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,398 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,398 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,404 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,405 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,405 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,405 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:01,405 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:01.405 [pool-16-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (508 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:02.107 [pool-17-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (176 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:02.111 [pool-18-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:02.113 [pool-19-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:02.115 [pool-20-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:02.116 [pool-15-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:02.116 [pool-15-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:02.116 [pool-15-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:314.
13:44:02.116 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #206
13:44:02.116 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:313 which is executed by 20 tests
380667182
13:44:02.249 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:02,976 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,979 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,979 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,979 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,979 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:02,995 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,995 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,995 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,995 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:02,995 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,007 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,007 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,007 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,007 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,007 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,020 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,020 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,020 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,020 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,020 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,033 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,033 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,033 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,033 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,034 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,050 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,050 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,051 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,051 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,051 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,065 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,065 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,065 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,065 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,066 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,080 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,080 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,080 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,080 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,080 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,095 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,095 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,095 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,095 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,095 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,106 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,106 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,106 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,106 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,106 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:03,120 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,121 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,121 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,121 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,121 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,129 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,129 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,129 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,129 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,129 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,136 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,136 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,136 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,136 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,136 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,143 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,144 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,144 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,144 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,144 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,152 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,152 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,152 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,152 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,152 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,162 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,162 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,162 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,162 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,162 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,170 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,171 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,171 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,171 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,171 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,180 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,181 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,181 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,181 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,181 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,191 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,191 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,191 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,191 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,191 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,201 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,201 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,201 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,201 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:03,201 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:03.202 [pool-22-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (495 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:03.878 [pool-23-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (191 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:03.881 [pool-24-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:03.884 [pool-25-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:03.886 [pool-26-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:03.887 [pool-21-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:03.887 [pool-21-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:03.887 [pool-21-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:313.
13:44:03.887 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #207
13:44:03.887 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:312 which is executed by 20 tests
380667182
13:44:04.013 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,288 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,290 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,290 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,290 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,290 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,305 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,305 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,305 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,305 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,306 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,318 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,318 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,318 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,318 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,318 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,331 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,331 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,331 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,331 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,332 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,345 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,345 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,345 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,345 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,345 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,357 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,357 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,357 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,357 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,357 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,369 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,369 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,369 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,369 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,369 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,382 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,382 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,382 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,383 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,383 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,397 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,397 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,397 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,397 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,397 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,408 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,408 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,408 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,408 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,408 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:04,421 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,421 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,421 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,421 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,421 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,431 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,431 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,432 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,432 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,432 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,442 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,442 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,442 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,442 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,442 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,450 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,450 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,450 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,450 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,450 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,457 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,458 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,458 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,458 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,458 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,466 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,466 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,466 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,466 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,466 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,473 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,473 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,473 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,473 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,473 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,479 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,479 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,479 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,479 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,479 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,486 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,486 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,486 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,486 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,486 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,492 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,492 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,492 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,492 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:04,492 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:04.493 [pool-28-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (471 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:05.096 [pool-29-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (181 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:05.099 [pool-30-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:05.102 [pool-31-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:05.105 [pool-32-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:05.106 [pool-27-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:05.106 [pool-27-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:05.106 [pool-27-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:312.
13:44:05.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #208
13:44:05.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:385 which is executed by 20 tests
380667182
13:44:05.234 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,541 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,544 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,544 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,544 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,544 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,560 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,560 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,560 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,560 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,560 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,573 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,573 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,573 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,573 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,573 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,585 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,585 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,586 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,586 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,586 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,598 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,598 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,598 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,598 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,598 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,611 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,611 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,611 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,611 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,611 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,624 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,624 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,624 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,624 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,624 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,637 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,637 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,637 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,637 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,637 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,648 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,648 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,648 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,648 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,648 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,659 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,659 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,660 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,660 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,660 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:05,674 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,674 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,674 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,674 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,674 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,686 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,686 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,686 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,686 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,686 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,698 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,698 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,698 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,698 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,698 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,711 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,711 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,711 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,711 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,711 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,719 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,719 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,719 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,719 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,719 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,730 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,730 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,730 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,730 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,730 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,738 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,738 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,738 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,738 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,738 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,747 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,747 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,747 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,747 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,747 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,755 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,755 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,755 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,756 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,756 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,763 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,763 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,763 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,763 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:05,764 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:05.764 [pool-34-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (521 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:06.431 [pool-35-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (175 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:06.434 [pool-36-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:06.436 [pool-37-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:06.439 [pool-38-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:06.440 [pool-33-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:06.440 [pool-33-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:06.440 [pool-33-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:385.
13:44:06.440 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #209
13:44:06.440 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:384 which is executed by 20 tests
380667182
13:44:06.564 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,837 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,839 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,839 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,839 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,839 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,856 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,856 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,856 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,856 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,856 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,871 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,871 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,871 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,871 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,871 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,883 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,883 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,883 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,883 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,883 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,895 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,895 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,895 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,895 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,895 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,908 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,908 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,908 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,908 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,908 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,919 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,919 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,919 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,919 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,919 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,932 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,932 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,932 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,932 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,932 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,943 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,943 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,943 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,943 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,943 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,953 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,953 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,953 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,953 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,953 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:06,964 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,964 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,964 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,965 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,965 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,971 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,972 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,972 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,972 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,972 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,981 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,981 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,982 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,982 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,982 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,993 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,993 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,993 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,993 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:06,993 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,004 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,004 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,004 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,004 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,004 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,017 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,017 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,017 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,017 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,017 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,023 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,023 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,023 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,023 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,023 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,029 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,029 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,029 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,029 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,029 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,035 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,035 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,035 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,035 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,035 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,041 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,041 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,041 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,041 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:07,041 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:07.041 [pool-40-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (469 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:07.626 [pool-41-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (175 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:07.630 [pool-42-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:07.632 [pool-43-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:07.634 [pool-44-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:07.634 [pool-39-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:07.634 [pool-39-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:07.634 [pool-39-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:384.
13:44:07.634 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #210
13:44:07.634 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:383 which is executed by 20 tests
380667182
13:44:07.757 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,024 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,026 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,026 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,026 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,026 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,042 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,042 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,042 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,042 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,042 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,053 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,053 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,053 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,053 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,053 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,065 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,065 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,065 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,065 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,065 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,078 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,078 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,078 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,078 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,078 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,090 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,090 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,090 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,090 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,090 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,103 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,103 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,103 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,103 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,103 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,117 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,117 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,117 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,117 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,117 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,131 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,131 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,131 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,131 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,131 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,141 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,141 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,141 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,141 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,141 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:08,151 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,151 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,151 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,151 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,151 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,158 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,158 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,158 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,158 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,158 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,165 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,165 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,165 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,165 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,165 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,173 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,173 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,173 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,173 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,173 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,182 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,182 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,182 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,182 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,182 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,191 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,192 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,192 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,192 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,192 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,198 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,198 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,198 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,198 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,198 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,204 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,204 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,204 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,205 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,205 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,212 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,213 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,213 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,213 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,213 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,219 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,219 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,219 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,219 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:08,219 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:08.220 [pool-46-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (456 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:08.757 [pool-47-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (178 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:08.760 [pool-48-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:08.762 [pool-49-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:08.764 [pool-50-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:08.765 [pool-45-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:08.765 [pool-45-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:08.765 [pool-45-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:383.
13:44:08.765 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #211
13:44:08.765 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:382 which is executed by 20 tests
380667182
13:44:08.888 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,383 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,385 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,386 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,386 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,386 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,402 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,402 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,402 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,402 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,402 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,414 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,414 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,414 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,414 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,414 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,426 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,426 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,426 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,426 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,426 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,438 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,438 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,438 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,439 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,439 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,450 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,450 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,450 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,450 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,450 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,462 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,462 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,462 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,463 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,463 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,476 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,476 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,476 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,476 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,476 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,488 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,488 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,488 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,488 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,488 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,499 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,499 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,500 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,500 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,500 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-12-03 13:44:09,509 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,509 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,509 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,509 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,509 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,516 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,516 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,516 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,516 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,516 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,523 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,523 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,523 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,523 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,523 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,532 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,532 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,532 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,532 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,532 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,539 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,539 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,539 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,539 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,539 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,548 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,548 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,549 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,549 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,549 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,555 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,555 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,555 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,555 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,555 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,561 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,561 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,561 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,561 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,562 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,568 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,568 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,568 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,568 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,568 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,575 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,575 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,575 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,575 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-12-03 13:44:09,575 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
13:44:09.576 [pool-52-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (680 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:10.135 [pool-53-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (173 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:10.138 [pool-54-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:10.140 [pool-55-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
13:44:10.141 [pool-56-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

13:44:10.142 [pool-51-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:10.142 [pool-51-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
13:44:10.142 [pool-51-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:382.
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #212
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:27 which is executed by 19 tests
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:27
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #213
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:883 which is executed by 18 tests
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:883
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #214
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:823 which is executed by 18 tests
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:823
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #215
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:811 which is executed by 18 tests
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:811
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #216
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:808 which is executed by 18 tests
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:808
13:44:10.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #217
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:806 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:806
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #218
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:805 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:805
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #219
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:804 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:804
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #220
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:803 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:803
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #221
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:802 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:802
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #222
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:800 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:800
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #223
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:799 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:799
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #224
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:758 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:758
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #225
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:755 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:755
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #226
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:754 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:754
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #227
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:751 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:751
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #228
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:750 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:750
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #229
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:748 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:748
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #230
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:731 which is executed by 18 tests
13:44:10.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:731
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #231
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:711 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:711
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #232
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:709 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:709
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #233
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:707 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:707
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #234
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:706 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:706
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #235
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:705 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:705
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #236
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:699 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:699
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #237
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:698 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:698
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #238
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:694 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:694
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #239
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:693 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:693
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #240
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:688 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:688
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #241
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:687 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:687
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #242
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:685 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:685
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #243
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:683 which is executed by 18 tests
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:683
13:44:10.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #244
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:682 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:682
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #245
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:680 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:680
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #246
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:679 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:679
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #247
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:677 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:677
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #248
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:671 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:671
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #249
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:670 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:670
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #250
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:668 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:668
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #251
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:667 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:667
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #252
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:665 which is executed by 18 tests
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:665
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #253
13:44:10.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:663 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:663
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #254
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:661 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:661
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #255
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:625 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:625
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #256
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:623 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:623
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #257
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:614 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:614
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #258
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:612 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:612
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #259
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:606 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:606
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #260
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:602 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:602
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #261
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:600 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:600
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #262
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:579 which is executed by 18 tests
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:579
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #263
13:44:10.146 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:479 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:479
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #264
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:334 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:334
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #265
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:331 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:331
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #266
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:328 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:328
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #267
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:323 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:323
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #268
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:319 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:319
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #269
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:317 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:317
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #270
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:313 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:313
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #271
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:311 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:311
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #272
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:307 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:307
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #273
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:224 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:224
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #274
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:216 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:216
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #275
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:215 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:215
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #276
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:213 which is executed by 18 tests
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:213
13:44:10.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #277
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:211 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:211
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #278
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:210 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:210
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #279
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:209 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:209
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #280
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:208 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:208
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #281
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:206 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:206
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #282
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:204 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:204
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #283
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:197 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:197
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #284
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:195 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:195
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #285
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:403 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:403
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #286
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:402 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:402
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #287
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:400 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:400
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #288
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:389 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:389
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #289
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:373 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:373
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #290
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:58 which is executed by 18 tests
13:44:10.148 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:58
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #291
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.CharsetMapping:687 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.CharsetMapping:687
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #292
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:202 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:202
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #293
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:94 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:94
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #294
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:87 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:87
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #295
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:83 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:83
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #296
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:81 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:81
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #297
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:77 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:77
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #298
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:72 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:72
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #299
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:567 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:567
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #300
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:565 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:565
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #301
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:553 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:553
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #302
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:550 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:550
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #303
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:548 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:548
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #304
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:546 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:546
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #305
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:436 which is executed by 18 tests
13:44:10.149 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:436
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #306
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:435 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:435
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #307
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:433 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:433
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #308
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:432 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:432
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #309
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:425 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:425
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #310
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:151 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:151
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #311
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:131 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:131
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #312
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:525 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:525
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #313
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:524 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:524
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #314
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:505 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:505
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #315
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:503 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:503
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #316
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:502 which is executed by 18 tests
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:502
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #317
13:44:10.150 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2311 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2311
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #318
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2309 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2309
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #319
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2308 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2308
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #320
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2307 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2307
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #321
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2304 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2304
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #322
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2302 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2302
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #323
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2295 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2295
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #324
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2286 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2286
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #325
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1972 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1972
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #326
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1971 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1971
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #327
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1970 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1970
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #328
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1968 which is executed by 18 tests
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1968
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #329
13:44:10.151 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1964 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1964
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #330
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1487 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1487
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #331
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1483 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1483
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #332
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1482 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1482
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #333
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1481 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1481
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #334
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1479 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1479
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #335
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1475 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1475
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #336
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1457 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1457
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #337
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1412 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1412
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #338
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1397 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1397
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #339
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1365 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1365
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #340
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1362 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1362
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #341
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1358 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1358
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #342
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1356 which is executed by 18 tests
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1356
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #343
13:44:10.152 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1355 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1355
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #344
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1344 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1344
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #345
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1342 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1342
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #346
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1340 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1340
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #347
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1339 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1339
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #348
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1335 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1335
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #349
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1331 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1331
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #350
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1292 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1292
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #351
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1291 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1291
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #352
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1273 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1273
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #353
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1261 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1261
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #354
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1260 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1260
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #355
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1235 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1235
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #356
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1227 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1227
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #357
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1180 which is executed by 18 tests
13:44:10.153 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1180
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #358
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1174 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1174
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #359
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1172 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1172
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #360
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1171 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1171
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #361
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1169 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1169
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #362
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1168 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1168
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #363
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1167 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1167
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #364
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1166 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1166
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #365
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1164 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1164
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #366
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1163 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1163
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #367
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1162 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1162
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #368
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1158 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1158
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #369
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1156 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1156
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #370
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1152 which is executed by 18 tests
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1152
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #371
13:44:10.154 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1129 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1129
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #372
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1125 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1125
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #373
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1124 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1124
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #374
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1122 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1122
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #375
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1118 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1118
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #376
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1116 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1116
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #377
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1115 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1115
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #378
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1109 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1109
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #379
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1107 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1107
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #380
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1106 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1106
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #381
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1100 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1100
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #382
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1096 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1096
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #383
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1094 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1094
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #384
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1093 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1093
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #385
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1092 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1092
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #386
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1088 which is executed by 18 tests
13:44:10.155 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1088
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #387
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:135 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:135
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #388
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:126 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:126
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #389
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:124 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:124
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #390
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1146 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1146
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #391
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1139 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1139
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #392
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1127 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1127
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #393
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1121 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1121
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #394
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1117 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1117
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #395
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1116 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1116
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #396
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1115 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1115
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #397
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1113 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1113
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #398
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1111 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1111
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #399
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1101 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1101
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #400
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1078 which is executed by 18 tests
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1078
13:44:10.156 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #401
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1067 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1067
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #402
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1052 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1052
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #403
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1050 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1050
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #404
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1048 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1048
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #405
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1046 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1046
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #406
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1044 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1044
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #407
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1043 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1043
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #408
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1040 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1040
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #409
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1039 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1039
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #410
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1037 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1037
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #411
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1035 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1035
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #412
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1026 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1026
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #413
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1023 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1023
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #414
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1020 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1020
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #415
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1018 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1018
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #416
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1016 which is executed by 18 tests
13:44:10.157 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1016
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #417
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1015 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1015
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #418
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1013 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1013
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #419
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1012 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1012
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #420
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:998 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:998
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #421
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:995 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:995
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #422
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:989 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:989
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #423
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:984 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:984
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #424
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:982 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:982
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #425
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4899 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4899
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #426
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4658 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4658
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #427
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4519 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4519
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #428
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4455 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4455
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #429
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4447 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4447
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #430
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3790 which is executed by 18 tests
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3790
13:44:10.158 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #431
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3628 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3628
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #432
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3556 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3556
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #433
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2569 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2569
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #434
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2565 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2565
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #435
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2563 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2563
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #436
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2562 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2562
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #437
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2561 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2561
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #438
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2560 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2560
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #439
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2559 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2559
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #440
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2558 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2558
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #441
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2557 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2557
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #442
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2556 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2556
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #443
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2555 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2555
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #444
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2554 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2554
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #445
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2553 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2553
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #446
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2552 which is executed by 18 tests
13:44:10.159 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2552
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #447
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2544 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2544
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #448
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2539 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2539
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #449
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2535 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2535
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #450
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2534 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2534
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #451
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2531 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2531
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #452
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2529 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2529
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #453
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2521 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2521
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #454
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2518 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2518
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #455
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2514 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2514
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #456
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2509 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2509
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #457
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2507 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2507
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #458
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2504 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2504
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #459
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2500 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2500
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #460
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2498 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2498
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #461
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2495 which is executed by 18 tests
13:44:10.160 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2495
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #462
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2494 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2494
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #463
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2492 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2492
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #464
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2490 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2490
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #465
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2489 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2489
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #466
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2488 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2488
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #467
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2487 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2487
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #468
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2486 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2486
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #469
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2485 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2485
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #470
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2483 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2483
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #471
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2479 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2479
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #472
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2477 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2477
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #473
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2475 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2475
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #474
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2417 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2417
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #475
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2246 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2246
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #476
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2210 which is executed by 18 tests
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2210
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #477
13:44:10.161 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2201 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2201
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #478
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2062 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2062
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #479
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2026 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2026
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #480
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1990 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1990
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #481
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1972 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1972
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #482
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1945 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1945
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #483
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1936 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1936
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #484
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1918 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1918
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #485
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1900 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1900
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #486
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1882 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1882
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #487
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1846 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1846
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #488
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1806 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1806
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #489
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1627 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1627
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #490
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1618 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1618
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #491
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1609 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1609
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #492
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1537 which is executed by 18 tests
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1537
13:44:10.162 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #493
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1386 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1386
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #494
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1383 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1383
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #495
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1379 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1379
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #496
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1378 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1378
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #497
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1376 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1376
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #498
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1374 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1374
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #499
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1371 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1371
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #500
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1370 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1370
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #501
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1368 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1368
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #502
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1364 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1364
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #503
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:4975 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:4975
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #504
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:3325 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:3325
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #505
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:3320 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:3320
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #506
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:486 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:486
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #507
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:485 which is executed by 18 tests
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:485
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #508
13:44:10.163 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:483 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:483
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #509
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:472 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:472
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #510
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:71 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:71
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #511
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:69 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:69
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #512
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:67 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:67
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #513
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:64 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:64
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #514
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:61 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:61
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #515
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:56 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:56
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #516
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:52 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:52
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #517
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:583 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:583
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #518
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:582 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:582
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #519
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:580 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:580
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #520
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:578 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:578
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #521
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:577 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:577
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #522
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:575 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:575
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #523
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:574 which is executed by 18 tests
13:44:10.164 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:574
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #524
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:558 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:558
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #525
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:266 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:266
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #526
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:263 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:263
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #527
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:253 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:253
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #528
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:245 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:245
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #529
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:222 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:222
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #530
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:221 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:221
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #531
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:217 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:217
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #532
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:216 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:216
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #533
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:215 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:215
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #534
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:214 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:214
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #535
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:211 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:211
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #536
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:207 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:207
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #537
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:205 which is executed by 18 tests
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:205
13:44:10.165 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #538
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:203 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:203
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #539
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:201 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:201
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #540
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:199 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:199
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #541
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:195 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:195
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #542
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:190 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:190
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #543
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:188 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:188
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #544
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:187 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:187
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #545
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:184 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:184
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #546
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:181 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:181
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #547
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:179 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:179
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #548
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:177 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:177
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #549
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:175 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:175
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #550
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:171 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:171
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #551
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:170 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:170
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #552
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:169 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:169
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #553
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:167 which is executed by 18 tests
13:44:10.166 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:167
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #554
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:165 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:165
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #555
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:164 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:164
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #556
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:157 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:157
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #557
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:154 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:154
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #558
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:152 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:152
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #559
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:148 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:148
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #560
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:146 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:146
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #561
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:142 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:142
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #562
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:140 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:140
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #563
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:137 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:137
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #564
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:136 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:136
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #565
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:134 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:134
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #566
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:132 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:132
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #567
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:120 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:120
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #568
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:98 which is executed by 18 tests
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:98
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #569
13:44:10.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:97 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:97
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #570
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:70 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:70
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #571
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:69 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:69
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #572
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:67 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:67
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #573
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Buffer:464 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Buffer:464
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #574
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Buffer:463 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Buffer:463
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #575
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:99 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:99
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #576
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:98 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:98
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #577
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:96 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:96
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #578
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:94 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:94
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #579
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:93 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:93
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #580
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:91 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:91
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #581
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:90 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:90
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #582
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:74 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:74
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #583
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:70 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:70
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #584
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:3124 which is executed by 18 tests
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:3124
13:44:10.168 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #585
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:3119 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:3119
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #586
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:786 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:786
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #587
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:780 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:780
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #588
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:772 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:772
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #589
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:283 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:283
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #590
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:282 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:282
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #591
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:281 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:281
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #592
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:278 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:278
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #593
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:277 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:277
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #594
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:273 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:273
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #595
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:271 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:271
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #596
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:217 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:217
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #597
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:216 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:216
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #598
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:215 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:215
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #599
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:214 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:214
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #600
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:200 which is executed by 18 tests
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:200
13:44:10.169 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #601
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:192 which is executed by 18 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:192
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #602
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:166 which is executed by 18 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:166
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #603
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:37 which is executed by 29 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:37
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #604
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:36 which is executed by 29 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:36
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #605
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:123 which is executed by 45 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:123
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #606
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:370 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:370
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #607
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:369 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:369
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #608
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:368 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:368
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #609
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:367 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:367
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #610
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:363 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:363
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #611
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:356 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:356
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #612
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:355 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:355
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #613
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:145 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:145
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #614
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:141 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:141
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #615
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:140 which is executed by 31 tests
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:140
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #616
13:44:10.170 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:138 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:138
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #617
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:137 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:137
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #618
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:136 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:136
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #619
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:135 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:135
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #620
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:132 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:132
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #621
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:157 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:157
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #622
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:156 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:156
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #623
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:152 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:152
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #624
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:150 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:150
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #625
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:149 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:149
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #626
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:148 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:148
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #627
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:147 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:147
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #628
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:145 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:145
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #629
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:144 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:144
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #630
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:143 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:143
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #631
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:142 which is executed by 31 tests
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:142
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #632
13:44:10.171 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:141 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:141
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #633
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:140 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:140
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #634
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:139 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:139
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #635
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:136 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:136
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #636
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:134 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:134
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #637
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:133 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:133
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #638
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:132 which is executed by 31 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:132
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #639
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:44 which is executed by 32 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:44
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #640
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:42 which is executed by 32 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:42
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #641
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:41 which is executed by 32 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:41
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #642
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:40 which is executed by 32 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:40
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #643
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:52 which is executed by 8 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:52
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #644
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:51 which is executed by 8 tests
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:51
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #645
13:44:10.172 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotMode:226 which is executed by 5 tests
380667182
13:44:10.293 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #646
13:44:10.293 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:35 which is executed by 11 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:35
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #647
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:503 which is executed by 11 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:503
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #648
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:502 which is executed by 11 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:502
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #649
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:388 which is executed by 21 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:388
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #650
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:387 which is executed by 21 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:387
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #651
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:383 which is executed by 21 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:383
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #652
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:364 which is executed by 13 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:364
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #653
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:133 which is executed by 13 tests
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:133
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #654
13:44:10.294 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:404 which is executed by 12 tests
-2099881367
13:44:10.431 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #655
13:44:10.431 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:414 which is executed by 10 tests
-2099881367
13:44:10.541 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #656
13:44:10.542 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:413 which is executed by 10 tests
-2099881367
13:44:10.651 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-12-03 13:44:11,098 INFO   ||  binding to port localhost/127.0.0.1:46535   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:11,120 INFO   ||  Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:host.name=uvb-43.sophia.grid5000.fr   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.version=1.8.0_181   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.vendor=Oracle Corporation   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.class.path=/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.io.tmpdir=/tmp   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:java.compiler=<NA>   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:os.name=Linux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:os.arch=amd64   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:os.version=4.9.0-8-amd64   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:user.name=tdurieux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:user.home=/home/tdurieux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,120 INFO   ||  Server environment:user.dir=/tmp/Nopol_Bears_debezium-debezium_351465554-354212780   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,132 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,147 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,162 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 33029
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46535
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:11,246 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:11,248 INFO   ||  Connecting to zookeeper on localhost:46535   [kafka.server.KafkaServer]
2018-12-03 13:44:11,264 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:11,268 INFO   ||  Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:host.name=uvb-43.sophia.grid5000.fr   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.version=1.8.0_181   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.vendor=Oracle Corporation   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.class.path=/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.io.tmpdir=/tmp   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:java.compiler=<NA>   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:os.name=Linux   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:os.arch=amd64   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:os.version=4.9.0-8-amd64   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:user.name=tdurieux   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:user.home=/home/tdurieux   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,268 INFO   ||  Client environment:user.dir=/tmp/Nopol_Bears_debezium-debezium_351465554-354212780   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,269 INFO   ||  Initiating client connection, connectString=localhost:46535 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2539bc11   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:11,301 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:11,306 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46535. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:11,310 INFO   ||  Socket connection established to localhost/127.0.0.1:46535, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:11,310 INFO   ||  Accepted socket connection from /127.0.0.1:57454   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:11,317 INFO   ||  Client attempting to establish new session at /127.0.0.1:57454   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,319 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:11,410 INFO   ||  Established session 0x167756289010000 with negotiated timeout 6000 for client /127.0.0.1:57454   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:11,410 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46535, sessionid = 0x167756289010000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:11,411 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:11,460 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:11,489 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:11,531 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:11,756 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:11,782 INFO   ||  Cluster ID = FJBbMvrsT7m-uOKIfOpFgQ   [kafka.server.KafkaServer]
2018-12-03 13:44:11,785 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:11,835 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:11,835 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:11,838 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:11,884 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:11,893 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:11,904 INFO   ||  Logs loading complete in 11 ms.   [kafka.log.LogManager]
2018-12-03 13:44:11,954 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:11,956 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:11,958 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:11,960 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:12,178 INFO   ||  Awaiting socket connections on localhost:33029.   [kafka.network.Acceptor]
2018-12-03 13:44:12,181 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:12,206 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,206 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,207 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,215 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:12,253 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:12,258 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,265 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,265 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:12,272 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:12,285 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:12,286 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:12,287 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:12,293 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:12,295 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:setData cxid:0x2a zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,295 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:12,299 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 4 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:12,306 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:12,324 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:12,350 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,350 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:12,350 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,353 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,354 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,354 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,355 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:12,356 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:12,356 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:12,362 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,366 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,396 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:12,400 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:12,402 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:12,405 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,406 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,406 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,407 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:12,408 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:12,410 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:12,414 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,423 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:12,423 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:12,424 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,424 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,439 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:12,441 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,33029,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:12,443 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:12,462 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:12,471 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:12,472 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:12,483 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,483 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,485 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:12,487 INFO   ||  Started Kafka server 1 at localhost:33029 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:12,492 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:33029 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:12,544 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,556 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,573 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:12,589 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:12,590 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:12,593 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:12,593 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:12,597 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:33029]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:12,597 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:12,603 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:12,608 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,614 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:12,618 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,618 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,633 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:12,657 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:12,665 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:12,699 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:12,707 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 24 ms   [kafka.log.Log]
2018-12-03 13:44:12,709 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:12,710 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:12,712 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:12,714 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:12,777 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:12,823 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:12,838 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:33029, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:12,838 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:33029, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:12,839 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:33029]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:12,842 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,842 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,845 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:33029]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,854 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,854 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,981 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:12,981 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:33029]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:12,983 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,983 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:12,984 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:12,990 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:13,093 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:13,095 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:13,119 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:13,144 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,144 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,161 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:13,172 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:13,189 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:13,222 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:13,223 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:13,223 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:13,223 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:13,223 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:13,224 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:13,224 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:13,225 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:13,239 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756289010000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:13,273 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:13,274 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:13,276 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:13,277 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-12-03 13:44:13,278 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:13,278 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:13,278 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:13,278 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:13,279 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:13,286 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:13,352 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,354 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,354 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,374 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,387 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,395 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,404 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:13,438 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,440 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,487 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,488 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,528 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:13,530 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:33029, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:13,530 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:33029, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:13,531 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:13,534 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,534 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,542 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,543 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,544 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,546 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,552 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,552 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,557 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:13,559 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,559 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:13,565 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,566 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,566 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,569 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,570 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,571 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,572 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:13,573 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:13,578 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1543862652747, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-12-03 13:44:13,579 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,579 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,583 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:13,584 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:13,592 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:13,599 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:13,602 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:13,610 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:13,611 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:13,614 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:13,617 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:13,619 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,659 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,659 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:13,659 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,660 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:13,661 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:13,662 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:13,663 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:13,663 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:13,663 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:13,663 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:13,665 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:13,665 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,665 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,666 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,666 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,666 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:13,770 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,770 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,770 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:13,771 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:13,771 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:13,771 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:13,771 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:13,772 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:13,773 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:13,773 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:13,837 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,837 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,837 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:13,961 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:14,007 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:14,007 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:14,007 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:14,007 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:14,007 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:14,031 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:14,032 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:14,033 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:14,033 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:14,033 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:14,033 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:14,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:14,201 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:14,201 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:14,201 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:14,201 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:14,204 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:14,205 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:14,206 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:14,206 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:14,206 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:14,207 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:14,208 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:14,209 INFO   ||  Processed session termination for sessionid: 0x167756289010000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:14,222 INFO   ||  Session: 0x167756289010000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:14,222 INFO   ||  EventThread shut down for session: 0x167756289010000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:14,223 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:14,223 INFO   ||  Closed socket connection for client /127.0.0.1:57454 which had sessionid 0x167756289010000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:14,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:14,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:14,835 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:14,835 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:14,835 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:15,365 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:15,835 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,835 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,835 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,838 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,838 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:15,839 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:15,876 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:15,880 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:15,884 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:15,889 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:15,891 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:15,891 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:15,891 INFO   ||  Stopped Kafka server 1 at localhost:33029   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:15,891 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:15,892 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,892 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,892 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:15,892 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:15,892 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:15,892 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:15,892 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:15,892 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:15,895 INFO   ||  binding to port localhost/127.0.0.1:33749   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:15,895 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,896 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,897 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 41711
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:33749
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:15,898 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:15,898 INFO   ||  Connecting to zookeeper on localhost:33749   [kafka.server.KafkaServer]
2018-12-03 13:44:15,898 INFO   ||  Initiating client connection, connectString=localhost:33749 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@8944b48   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:15,898 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:15,899 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:15,899 INFO   ||  Opening socket connection to server localhost/127.0.0.1:33749. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:15,899 INFO   ||  Socket connection established to localhost/127.0.0.1:33749, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:15,899 INFO   ||  Accepted socket connection from /127.0.0.1:42768   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:15,900 INFO   ||  Client attempting to establish new session at /127.0.0.1:42768   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,900 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:15,943 INFO   ||  Established session 0x16775629b980000 with negotiated timeout 6000 for client /127.0.0.1:42768   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:15,943 INFO   ||  Session establishment complete on server localhost/127.0.0.1:33749, sessionid = 0x16775629b980000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:15,943 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:15,961 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:15,994 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:16,036 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,095 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,119 INFO   ||  Cluster ID = 07fWQLi3R1S4dF9Mv86IYw   [kafka.server.KafkaServer]
2018-12-03 13:44:16,120 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:16,121 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:16,122 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:16,122 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:16,123 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:16,123 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:16,124 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:16,251 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:16,251 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:16,251 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:16,252 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:16,289 INFO   ||  Awaiting socket connections on localhost:41711.   [kafka.network.Acceptor]
2018-12-03 13:44:16,289 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:16,290 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,290 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,291 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,292 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:16,293 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:16,293 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,294 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,294 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:16,295 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:16,295 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:16,295 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:16,295 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:16,311 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:16,311 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:16,312 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:16,319 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:16,319 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,329 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:16,329 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:16,329 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:16,336 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:16,340 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:16,340 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x47 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,341 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x48 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,352 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:16,353 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,41711,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,353 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,354 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:16,354 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,354 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:16,355 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,355 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,414 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,414 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,414 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:16,414 INFO   ||  Started Kafka server 1 at localhost:41711 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:16,414 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:16,416 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:16,417 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,418 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:16,418 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:16,419 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:41711 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:16,436 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,453 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:16,462 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41711, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:16,462 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41711, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:16,462 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:41711]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:16,462 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:16,462 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:16,463 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:16,463 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,463 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:16,463 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,464 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,465 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,465 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,465 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41711]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,466 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,467 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,467 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:16,469 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,479 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:16,503 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:16,504 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:16,506 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:16,506 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:16,507 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:16,507 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:16,507 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:16,508 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:16,530 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:16,574 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:16,574 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41711]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,575 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:16,575 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:16,576 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,576 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,576 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:16,682 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:16,683 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:16,685 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41711]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:16,688 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,688 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:16,696 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,711 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,728 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:16,761 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:16,762 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:16,762 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:16,762 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:16,762 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,763 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:16,763 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:16,763 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,778 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775629b980000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:16,811 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:16,812 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:16,814 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:16,815 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-12-03 13:44:16,816 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:16,816 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:16,816 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:16,816 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:16,816 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:16,817 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:16,893 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41711 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:16,894 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:16,894 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:16,896 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:16,897 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:16,898 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:16,899 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:16,971 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:16,971 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,195 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,195 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,203 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:17,236 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:17,238 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41711, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:17,238 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41711, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:17,238 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41711]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:17,240 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,240 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,244 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41711 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,245 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,245 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,246 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,247 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,248 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,250 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,250 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,256 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,256 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,259 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41711]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:17,261 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,261 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,267 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41711 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,267 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,267 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,269 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,270 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,271 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,273 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,274 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,280 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,280 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,283 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41711]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:17,285 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,285 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,289 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41711 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,289 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,290 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,291 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,293 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,293 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,295 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,296 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,302 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,302 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,304 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41711]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:17,306 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,306 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:17,310 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41711 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,311 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,311 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,312 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,313 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,314 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,315 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:17,316 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:17,323 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,323 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,326 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:17,326 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:17,328 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:17,329 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:17,330 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:17,334 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:17,334 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:17,335 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:17,337 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:17,337 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,383 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,433 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,483 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,483 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,494 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:17,494 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:17,494 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:17,494 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:17,494 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:17,494 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:17,494 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:17,494 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,494 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,513 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,513 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,513 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:17,513 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:17,513 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:17,513 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:17,513 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:17,513 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:17,513 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:17,513 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,584 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,636 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,636 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,637 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,682 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,684 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,691 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,691 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,691 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,691 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,691 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:17,712 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:17,712 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:17,712 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:17,712 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:17,712 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:17,712 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:17,785 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:17,839 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:17,839 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:17,839 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:17,839 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:17,839 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:17,839 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:17,839 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:17,839 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:17,839 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:17,840 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:17,840 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:17,841 INFO   ||  Processed session termination for sessionid: 0x16775629b980000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:17,853 INFO   ||  Session: 0x16775629b980000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:17,853 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:17,853 INFO   ||  EventThread shut down for session: 0x16775629b980000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:17,853 WARN   ||  caught end of stream exception   [org.apache.zookeeper.server.NIOServerCnxn]
EndOfStreamException: Unable to read additional data from client sessionid 0x16775629b980000, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:17,853 INFO   ||  Closed socket connection for client /127.0.0.1:42768 which had sessionid 0x16775629b980000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:18,085 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,121 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,121 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,121 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,122 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:18,145 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:18,147 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:18,147 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:18,147 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:18,147 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:18,148 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:18,148 INFO   ||  Stopped Kafka server 1 at localhost:41711   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:18,148 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:18,149 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,149 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,149 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:18,149 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,149 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:18,149 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,149 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:18,149 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:18,151 INFO   ||  binding to port localhost/127.0.0.1:40053   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:18,151 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,152 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,153 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 36473
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:40053
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:18,154 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:18,154 INFO   ||  Connecting to zookeeper on localhost:40053   [kafka.server.KafkaServer]
2018-12-03 13:44:18,154 INFO   ||  Initiating client connection, connectString=localhost:40053 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@3425dc99   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:18,154 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:18,155 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:18,155 INFO   ||  Opening socket connection to server localhost/127.0.0.1:40053. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:18,155 INFO   ||  Socket connection established to localhost/127.0.0.1:40053, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:18,155 INFO   ||  Accepted socket connection from /127.0.0.1:60330   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:18,156 INFO   ||  Client attempting to establish new session at /127.0.0.1:60330   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,156 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:18,198 INFO   ||  Established session 0x1677562a4680000 with negotiated timeout 6000 for client /127.0.0.1:60330   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:18,198 INFO   ||  Session establishment complete on server localhost/127.0.0.1:40053, sessionid = 0x1677562a4680000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:18,198 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:18,214 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,236 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,247 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,289 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,381 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,385 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,405 INFO   ||  Cluster ID = F63Ctz6YS8etszx5G3CNRQ   [kafka.server.KafkaServer]
2018-12-03 13:44:18,406 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:18,407 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,407 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,407 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:18,408 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:18,408 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:18,409 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:18,427 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:18,427 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:18,428 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:18,428 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:18,455 INFO   ||  Awaiting socket connections on localhost:36473.   [kafka.network.Acceptor]
2018-12-03 13:44:18,456 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:18,457 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,457 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,457 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,459 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:18,459 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:18,460 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,460 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,461 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:18,461 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:18,461 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:18,461 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:18,461 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:18,464 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:18,464 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:18,464 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:18,472 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:18,472 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,481 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:18,481 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:18,481 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:18,489 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:18,492 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:18,493 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x47 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,493 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x48 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:18,505 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:18,505 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,36473,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:18,505 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,506 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:18,506 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:18,506 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:18,507 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:18,507 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,555 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,555 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,555 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:18,555 INFO   ||  Started Kafka server 1 at localhost:36473 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:18,555 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:18,557 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:18,557 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,559 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:18,559 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:18,560 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:36473 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:18,564 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,580 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:18,589 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:36473]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:18,590 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:18,590 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:18,590 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:18,590 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:18,590 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:18,591 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:18,591 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,591 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,592 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,594 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,597 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,631 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:18,631 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:18,633 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:18,634 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-12-03 13:44:18,635 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:18,635 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:18,635 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:18,635 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:18,698 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:18,731 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:18,732 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36473, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:18,732 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36473, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:18,733 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:36473]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:18,735 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,735 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,735 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36473]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,736 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,736 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,736 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:18,785 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,837 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,843 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:18,843 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36473]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,844 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:18,844 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:18,845 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,845 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,845 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:18,949 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:18,950 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:18,951 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:18,953 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,953 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:18,959 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,972 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:18,988 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:18,989 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:19,016 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:19,017 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:19,017 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:19,018 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:19,018 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:19,018 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:19,018 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:19,019 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:19,030 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562a4680000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:19,064 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:19,065 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:19,066 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:19,067 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:19,067 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:19,067 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:19,067 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:19,067 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:19,068 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:19,068 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:19,157 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,159 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,160 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,161 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,162 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:19,187 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,193 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,193 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,197 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,197 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862658725, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,198 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,198 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,199 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862658728, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,200 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862658729, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,201 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,201 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,214 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:19,216 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36473, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,216 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36473, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,216 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:19,218 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,218 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,222 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,222 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,222 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,224 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,225 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,226 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,228 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,229 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,233 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,233 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862658725, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,233 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,233 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,233 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862658728, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,234 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862658729, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,236 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,236 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,239 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:19,240 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,240 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,244 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,244 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,244 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,246 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,246 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,247 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,249 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,249 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,253 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,253 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862658725, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,254 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,254 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,254 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862658728, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,255 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862658729, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,255 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:19,258 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,258 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,261 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:19,262 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,262 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,266 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,266 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,266 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,268 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,269 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,270 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,271 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,275 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,275 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862658725, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,276 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,276 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,277 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862658728, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,277 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862658729, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,278 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:19,281 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,282 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,284 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:19,285 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,286 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:19,289 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,289 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,289 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,291 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,291 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,292 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,293 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:19,294 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:19,297 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,298 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862658725, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,298 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,298 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:19,298 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862658728, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,299 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862658729, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:19,299 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:19,301 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,301 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,304 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:19,304 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:19,306 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:19,307 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:19,307 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:19,310 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:19,310 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:19,310 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:19,311 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:19,311 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,410 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,460 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,460 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,460 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:19,460 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:19,461 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:19,461 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:19,461 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:19,461 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:19,461 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:19,461 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,461 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,461 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,461 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,461 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,491 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,491 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,491 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:19,491 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:19,492 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:19,492 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:19,492 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:19,492 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:19,492 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:19,492 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,561 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,637 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,637 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,637 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,657 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,657 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,657 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,658 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,658 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:19,692 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:19,692 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:19,692 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:19,692 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:19,692 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:19,692 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:19,761 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,789 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,812 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,836 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:19,836 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:19,836 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:19,836 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:19,837 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:19,837 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:19,837 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:19,837 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:19,837 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:19,838 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:19,838 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:19,838 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:19,839 INFO   ||  Processed session termination for sessionid: 0x1677562a4680000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:19,855 INFO   ||  Session: 0x1677562a4680000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:19,855 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:19,855 INFO   ||  EventThread shut down for session: 0x1677562a4680000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:19,856 INFO   ||  Closed socket connection for client /127.0.0.1:60330 which had sessionid 0x1677562a4680000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:19,890 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,112 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,213 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,240 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,407 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,408 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,408 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,408 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:20,435 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:20,437 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:20,437 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:20,437 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:20,437 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:20,437 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:20,438 INFO   ||  Stopped Kafka server 1 at localhost:36473   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:20,438 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:20,439 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,439 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,439 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:20,439 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,439 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:20,439 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,439 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:20,439 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:20,442 INFO   ||  binding to port localhost/127.0.0.1:34537   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:20,442 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,450 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,451 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 45483
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:34537
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:20,452 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:20,452 INFO   ||  Connecting to zookeeper on localhost:34537   [kafka.server.KafkaServer]
2018-12-03 13:44:20,452 INFO   ||  Initiating client connection, connectString=localhost:34537 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@f5db7c6   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:20,452 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:20,453 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:20,453 INFO   ||  Opening socket connection to server localhost/127.0.0.1:34537. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:20,453 INFO   ||  Socket connection established to localhost/127.0.0.1:34537, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:20,453 INFO   ||  Accepted socket connection from /127.0.0.1:53086   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:20,454 INFO   ||  Client attempting to establish new session at /127.0.0.1:53086   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,454 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:20,488 INFO   ||  Established session 0x1677562ad610000 with negotiated timeout 6000 for client /127.0.0.1:53086   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:20,488 INFO   ||  Session establishment complete on server localhost/127.0.0.1:34537, sessionid = 0x1677562ad610000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:20,488 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:20,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:20,503 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,536 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,578 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,636 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,641 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,661 INFO   ||  Cluster ID = qz8tLr12TLKhRitgm6oXJw   [kafka.server.KafkaServer]
2018-12-03 13:44:20,661 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:20,662 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,662 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,662 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:20,663 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:20,664 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:20,664 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:20,684 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:20,685 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:20,685 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:20,686 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:20,690 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,702 INFO   ||  Awaiting socket connections on localhost:45483.   [kafka.network.Acceptor]
2018-12-03 13:44:20,703 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:20,704 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,704 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,705 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,706 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:20,707 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:20,707 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,707 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,708 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:20,708 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:20,708 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:20,708 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:20,708 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:20,711 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:20,711 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:20,711 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:20,711 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:setData cxid:0x2b zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,727 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:20,736 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:20,736 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:20,737 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:20,737 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:20,740 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:20,741 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:20,741 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:20,742 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,742 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:20,742 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,742 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,744 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:20,752 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:20,753 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,45483,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:20,753 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:20,754 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:20,755 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:20,755 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:20,756 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:45483 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:20,779 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,779 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,779 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:20,779 INFO   ||  Started Kafka server 1 at localhost:45483 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:20,780 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:45483, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:20,780 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:45483, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:20,780 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:45483]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:20,781 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,781 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,782 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:45483]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,782 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,782 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,782 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:20,813 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,888 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:20,888 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:45483]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:20,889 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,889 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,889 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:20,889 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:setData cxid:0x53 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,903 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x54 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,919 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:20,929 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:20,929 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:20,929 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:20,929 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:20,929 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:20,930 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:20,930 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x5c zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,936 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x5d zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:20,942 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,964 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:20,969 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:20,970 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:20,972 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:20,972 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:20,973 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:20,973 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:20,973 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:20,973 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:20,994 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:20,995 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:20,997 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:20,998 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:20,998 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,003 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:21,019 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:21,055 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:21,069 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:21,070 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:21,070 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:21,070 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:21,070 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:21,071 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:21,071 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:21,071 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:21,077 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ad610000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:21,111 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:21,112 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:21,114 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:21,114 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:21,114 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:21,115 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:21,115 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:21,115 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:21,115 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:21,115 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:21,192 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,201 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:45483 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,201 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,201 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,202 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,203 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,204 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,205 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:21,240 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,241 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,503 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,503 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,509 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:21,537 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:21,538 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:45483, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:21,538 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:45483, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:21,539 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,540 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,540 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,543 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:45483 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,544 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,544 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,545 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,546 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,546 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,547 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,548 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,552 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,553 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,554 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,556 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,556 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,559 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:45483 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,559 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,559 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,560 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,561 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,562 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,563 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,564 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,568 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,568 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,570 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,571 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,571 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,574 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:45483 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,574 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,575 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,576 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,577 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,578 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,579 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,579 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,584 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,584 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,586 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,587 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,587 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,590 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:45483 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,591 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,591 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,592 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,592 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,593 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,593 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,594 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:21,595 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:21,599 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,599 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,601 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,602 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,602 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,607 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:45483, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:21,607 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:45483, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:21,607 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:45483]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:21,609 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,609 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,609 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45483]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:21,610 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,610 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:21,612 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:21,613 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:21,613 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:21,614 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:21,615 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:21,615 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:21,617 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:21,617 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:21,617 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:21,618 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:21,618 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,693 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,707 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,707 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,708 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:21,708 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:21,708 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:21,708 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:21,708 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:21,708 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:21,708 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:21,708 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,708 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,766 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,908 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,908 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,908 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:21,993 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,993 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:21,993 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:21,993 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:21,993 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:21,993 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:21,993 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:21,993 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:21,993 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:21,993 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,036 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,036 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,036 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,104 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,104 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,104 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,105 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,105 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,132 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:22,132 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:22,132 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:22,132 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:22,132 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:22,132 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:22,145 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,233 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:22,233 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:22,233 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:22,233 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:22,234 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:22,234 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:22,234 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:22,234 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:22,234 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:22,234 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:22,235 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:22,235 INFO   ||  Processed session termination for sessionid: 0x1677562ad610000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,252 INFO   ||  Session: 0x1677562ad610000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:22,252 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,253 INFO   ||  EventThread shut down for session: 0x1677562ad610000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:22,253 INFO   ||  Closed socket connection for client /127.0.0.1:53086 which had sessionid 0x1677562ad610000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:22,294 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,662 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,663 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,663 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,663 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:22,677 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:22,678 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:22,678 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:22,678 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:22,679 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:22,679 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:22,679 INFO   ||  Stopped Kafka server 1 at localhost:45483   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:22,679 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:22,679 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,679 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,679 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:22,679 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,679 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:22,679 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,679 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:22,680 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:22,681 INFO   ||  binding to port localhost/127.0.0.1:33619   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:22,682 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,683 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,683 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 32827
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:33619
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:22,684 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:22,684 INFO   ||  Connecting to zookeeper on localhost:33619   [kafka.server.KafkaServer]
2018-12-03 13:44:22,684 INFO   ||  Initiating client connection, connectString=localhost:33619 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@60e95a48   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:22,684 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:22,685 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:22,685 INFO   ||  Opening socket connection to server localhost/127.0.0.1:33619. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:22,685 INFO   ||  Socket connection established to localhost/127.0.0.1:33619, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:22,685 INFO   ||  Accepted socket connection from /127.0.0.1:45088   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:22,685 INFO   ||  Client attempting to establish new session at /127.0.0.1:45088   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,685 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:22,726 INFO   ||  Established session 0x1677562b61a0000 with negotiated timeout 6000 for client /127.0.0.1:45088   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:22,726 INFO   ||  Session establishment complete on server localhost/127.0.0.1:33619, sessionid = 0x1677562b61a0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:22,726 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:22,739 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,772 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,794 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,795 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,814 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,872 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,897 INFO   ||  Cluster ID = nnihn15SR96Np8OiPgFxaw   [kafka.server.KafkaServer]
2018-12-03 13:44:22,897 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:22,898 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,898 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,899 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:22,899 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:22,899 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:22,900 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:22,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,924 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:22,924 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:22,925 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:22,925 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:22,941 INFO   ||  Awaiting socket connections on localhost:32827.   [kafka.network.Acceptor]
2018-12-03 13:44:22,941 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:22,942 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,942 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,942 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,943 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:22,944 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:22,944 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,945 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,945 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:22,945 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:22,945 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:22,946 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:22,946 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:22,955 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:22,956 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:22,956 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:22,956 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,963 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:22,972 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:22,973 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:22,973 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:22,980 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:22,983 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:22,984 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x47 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,984 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x49 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,989 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:22,997 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:22,997 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,997 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,32827,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:22,997 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,997 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:22,997 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:22,997 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:22,997 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:22,997 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:22,998 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:22,998 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,001 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:23,043 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,043 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,043 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:23,043 INFO   ||  Started Kafka server 1 at localhost:32827 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:23,043 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:23,044 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:23,045 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,045 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:23,046 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:23,046 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:32827 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:23,055 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,072 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:23,080 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:32827]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:23,081 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:23,081 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:23,082 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:23,082 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:23,082 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,082 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,083 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:23,084 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:23,084 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,085 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,089 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,122 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:23,123 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:23,124 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:23,125 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:23,125 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:23,126 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:23,126 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:23,126 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:23,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,189 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:23,196 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,213 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:23,214 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:32827, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,214 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:32827, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,214 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:32827]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:23,215 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,215 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,216 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:32827]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,216 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,216 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,216 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:23,271 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,321 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,322 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:32827]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:23,322 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,322 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:23,322 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,322 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:23,427 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,428 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:23,429 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:32827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:23,430 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,430 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,436 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,447 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,464 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:23,495 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:23,496 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:23,496 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:23,497 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:23,497 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:23,497 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:23,497 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:23,498 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,514 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562b61a0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:23,547 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:23,548 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:23,550 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:23,550 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:23,550 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:23,551 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:23,551 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:23,551 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:23,551 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:23,551 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:23,634 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:32827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,634 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,634 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,636 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,637 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,637 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,638 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:23,680 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,684 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,684 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,694 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:23,696 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:32827, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,696 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:32827, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:23,696 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:32827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:23,697 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,697 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,701 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:32827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,701 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,702 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,703 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,704 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,704 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,705 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,706 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,711 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,711 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,714 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:32827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:23,715 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,715 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:23,718 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:32827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,718 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,718 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,720 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,720 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,721 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,721 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:23,722 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:23,725 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1543862663187, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-12-03 13:44:23,726 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,726 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,727 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:23,727 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:23,729 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:23,730 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:23,730 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:23,732 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:23,732 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:23,732 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:23,732 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:23,732 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,745 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,745 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,745 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:23,745 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:23,745 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:23,745 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:23,745 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:23,745 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:23,745 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:23,745 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:23,745 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,746 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,782 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,945 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,945 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,945 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:23,947 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:23,982 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,120 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,120 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,120 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:24,120 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:24,120 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:24,120 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:24,120 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:24,120 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:24,120 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:24,120 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,133 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,149 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,183 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,197 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,197 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,197 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,272 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,298 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,342 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,342 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,342 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,343 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,343 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:24,371 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:24,371 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:24,371 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:24,371 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:24,371 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:24,371 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:24,498 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:24,498 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:24,498 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:24,498 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:24,498 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:24,498 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:24,498 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:24,498 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:24,498 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:24,499 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:24,499 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:24,499 INFO   ||  Processed session termination for sessionid: 0x1677562b61a0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:24,514 INFO   ||  Session: 0x1677562b61a0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:24,514 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,514 INFO   ||  EventThread shut down for session: 0x1677562b61a0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:24,514 INFO   ||  Closed socket connection for client /127.0.0.1:45088 which had sessionid 0x1677562b61a0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:24,534 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,634 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,748 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:24,898 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,898 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,898 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,899 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,899 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:24,899 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:25,099 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,225 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,301 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,375 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,485 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,486 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,501 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,624 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,851 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:25,899 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:25,899 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:25,899 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:25,910 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:25,911 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:25,911 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:25,911 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:25,911 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:25,911 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:25,911 INFO   ||  Stopped Kafka server 1 at localhost:32827   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:25,911 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:25,912 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,912 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,912 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:25,912 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:25,912 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:25,912 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:25,912 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:25,912 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:25,914 INFO   ||  binding to port localhost/127.0.0.1:32885   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:25,914 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,915 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,916 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:32885
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:25,916 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:25,916 INFO   ||  Connecting to zookeeper on localhost:32885   [kafka.server.KafkaServer]
2018-12-03 13:44:25,917 INFO   ||  Initiating client connection, connectString=localhost:32885 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@55b7314f   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:25,917 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:25,917 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:25,917 INFO   ||  Opening socket connection to server localhost/127.0.0.1:32885. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:25,917 INFO   ||  Socket connection established to localhost/127.0.0.1:32885, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:25,917 INFO   ||  Accepted socket connection from /127.0.0.1:42548   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:25,918 INFO   ||  Client attempting to establish new session at /127.0.0.1:42548   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,918 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:25,958 INFO   ||  Established session 0x1677562c2ba0000 with negotiated timeout 6000 for client /127.0.0.1:42548   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:25,958 INFO   ||  Session establishment complete on server localhost/127.0.0.1:32885, sessionid = 0x1677562c2ba0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:25,958 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:25,969 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:26,002 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,044 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,103 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,127 INFO   ||  Cluster ID = _AUAQvsBSTeerRH1FNeqTA   [kafka.server.KafkaServer]
2018-12-03 13:44:26,127 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:26,129 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:26,129 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:26,129 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:26,130 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:26,130 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:26,130 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:26,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,454 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,473 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:26,473 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:26,474 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:26,474 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:26,489 INFO   ||  Awaiting socket connections on localhost:35091.   [kafka.network.Acceptor]
2018-12-03 13:44:26,490 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:26,491 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,491 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,492 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,493 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:26,494 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:26,494 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,495 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,495 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:26,495 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:26,496 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:26,496 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:26,496 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:26,502 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:26,503 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:26,503 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:26,503 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,503 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,511 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:26,520 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:26,520 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:26,521 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:26,527 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:26,531 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:26,531 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x47 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,531 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x48 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,544 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:26,544 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,35091,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:26,544 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,544 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,544 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,544 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,544 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,545 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:26,545 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:26,545 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,545 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,545 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:26,545 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,545 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:26,546 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,546 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,586 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,586 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,586 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:26,586 INFO   ||  Started Kafka server 1 at localhost:35091 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:26,586 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:26,587 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:26,588 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,588 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:26,589 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:26,589 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:35091 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:26,594 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,611 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:26,619 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35091, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:26,619 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35091, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:26,620 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:35091]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:26,620 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:26,620 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:26,620 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:26,620 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,620 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:26,621 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,621 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,621 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,621 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35091]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,621 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,621 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,622 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,622 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,622 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:26,627 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,661 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:26,662 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:26,663 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:26,663 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:26,664 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:26,664 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:26,664 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:26,664 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:26,727 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:26,727 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35091]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:26,728 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:26,728 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,728 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,728 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:26,754 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,804 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:26,832 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:26,833 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:26,833 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35091]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:26,835 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,835 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:26,841 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,852 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,869 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:26,900 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:26,901 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:26,901 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:26,901 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:26,901 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,902 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:26,902 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:26,902 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,919 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562c2ba0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:26,952 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:26,953 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:26,954 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:26,955 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:26,955 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:26,955 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:26,955 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:26,955 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:26,955 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:26,956 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:27,038 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35091 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,038 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,038 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,040 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,041 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,041 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,042 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:27,077 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,077 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,339 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,339 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,343 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:27,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,382 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:27,383 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35091, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:27,383 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35091, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:27,383 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35091]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:27,384 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,384 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,387 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35091 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,388 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,388 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,389 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,390 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,390 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,391 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,392 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,396 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,396 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,398 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35091]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:27,399 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,399 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,402 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35091 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,402 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,402 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,404 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,404 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,405 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,406 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,406 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,406 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,410 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,410 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,411 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35091]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:27,413 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,413 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,415 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35091 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,416 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,416 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,417 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,417 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,418 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,419 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,419 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,424 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,424 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,425 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35091]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:27,427 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,427 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:27,429 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35091 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,430 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,430 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,431 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,431 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,432 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,432 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:27,433 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:27,437 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,437 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,439 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:27,439 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:27,440 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:27,441 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:27,441 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:27,443 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:27,443 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:27,443 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:27,443 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:27,443 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:27,495 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:27,495 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:27,495 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:27,495 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:27,495 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:27,495 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:27,495 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,495 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,505 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,542 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,543 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,631 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,631 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,631 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:27,631 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:27,631 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:27,631 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:27,631 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:27,632 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:27,632 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:27,632 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,636 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,636 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,636 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,691 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,691 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,691 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,692 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,692 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:27,693 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,693 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,718 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:27,718 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:27,718 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:27,718 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:27,718 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:27,718 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:27,838 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:27,838 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:27,838 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:27,838 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:27,838 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:27,838 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:27,838 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:27,838 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:27,838 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:27,839 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:27,839 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:27,839 INFO   ||  Processed session termination for sessionid: 0x1677562c2ba0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:27,852 INFO   ||  Session: 0x1677562c2ba0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:27,852 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:27,852 INFO   ||  EventThread shut down for session: 0x1677562c2ba0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:27,853 INFO   ||  Closed socket connection for client /127.0.0.1:42548 which had sessionid 0x1677562c2ba0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:27,906 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,906 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,944 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:27,944 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,129 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:28,138 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:28,139 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:28,139 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:28,139 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:28,139 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:28,139 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:28,140 INFO   ||  Stopped Kafka server 1 at localhost:35091   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:28,140 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:28,140 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,140 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,140 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:28,140 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,140 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:28,140 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,140 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:28,140 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:28,142 INFO   ||  binding to port localhost/127.0.0.1:39759   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:28,142 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,143 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,144 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 39397
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:39759
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:28,145 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:28,145 INFO   ||  Connecting to zookeeper on localhost:39759   [kafka.server.KafkaServer]
2018-12-03 13:44:28,145 INFO   ||  Initiating client connection, connectString=localhost:39759 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2297d60c   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:28,145 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:28,145 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:28,145 INFO   ||  Opening socket connection to server localhost/127.0.0.1:39759. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:28,146 INFO   ||  Accepted socket connection from /127.0.0.1:35348   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:28,146 INFO   ||  Socket connection established to localhost/127.0.0.1:39759, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:28,146 INFO   ||  Client attempting to establish new session at /127.0.0.1:35348   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,146 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:28,230 INFO   ||  Established session 0x1677562cb6f0000 with negotiated timeout 6000 for client /127.0.0.1:35348   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:28,231 INFO   ||  Session establishment complete on server localhost/127.0.0.1:39759, sessionid = 0x1677562cb6f0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:28,231 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:28,247 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,280 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,295 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,322 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,380 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,405 INFO   ||  Cluster ID = wIUl3fYGQ7qlUqNW_pyW8w   [kafka.server.KafkaServer]
2018-12-03 13:44:28,405 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:28,406 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,407 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,407 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:28,407 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:28,408 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:28,408 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:28,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,425 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:28,425 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:28,425 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:28,428 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:28,441 INFO   ||  Awaiting socket connections on localhost:39397.   [kafka.network.Acceptor]
2018-12-03 13:44:28,441 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:28,442 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,442 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,443 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,444 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:28,445 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:28,445 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,445 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,445 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:28,445 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,445 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:28,445 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:28,446 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:28,446 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:28,455 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:28,455 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:28,455 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:28,456 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,463 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:28,472 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:28,472 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:28,473 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:28,480 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:28,483 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:28,483 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x46 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,483 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x47 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,497 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:28,497 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,39397,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:28,497 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,497 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:28,498 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:28,498 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,498 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:28,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,538 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,538 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,538 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:28,538 INFO   ||  Started Kafka server 1 at localhost:39397 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:28,538 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:28,539 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:28,540 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,541 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:28,541 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:28,541 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:39397 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:28,547 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,563 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:28,572 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:39397]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:28,573 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:28,573 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:28,573 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:28,573 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,573 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,573 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,573 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:28,574 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,574 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,575 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,580 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,614 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:28,614 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:28,615 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:28,616 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:28,616 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:28,617 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:28,617 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:28,617 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:28,658 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,678 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:28,708 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,708 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,708 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,712 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:28,714 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39397, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:28,714 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39397, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:28,714 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:39397]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:28,716 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,716 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,716 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39397]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,717 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,717 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,717 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:28,822 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:28,822 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39397]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:28,823 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,823 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,823 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:28,823 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:28,927 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:28,928 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:28,928 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39397]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:28,929 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,929 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:28,933 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,947 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:28,958 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:28,963 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:28,990 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:28,991 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:28,991 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:28,991 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:28,991 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,992 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:28,992 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:28,992 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:29,005 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562cb6f0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:29,008 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,039 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:29,039 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:29,041 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:29,041 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:29,041 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:29,041 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:29,042 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:29,042 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:29,042 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:29,043 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:29,046 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,131 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39397 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,131 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,132 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,133 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,133 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,134 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,135 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:29,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,167 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,167 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,171 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,171 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862668707, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,171 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,172 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,172 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,172 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,173 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,173 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,183 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:29,184 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39397, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,184 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39397, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,184 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39397]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:29,185 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,185 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,188 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39397 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,188 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,188 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,189 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,190 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,190 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,191 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,192 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,195 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,195 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862668707, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,195 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,195 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,196 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,196 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,197 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,198 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,199 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39397]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:29,200 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,200 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,203 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39397 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,203 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,203 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,204 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,205 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,205 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,206 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,206 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,208 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,208 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862668707, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,209 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,209 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,209 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,209 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,210 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:29,211 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,211 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,213 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39397]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:29,214 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,214 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,216 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39397 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,217 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,217 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,218 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,218 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,219 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,219 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,220 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,222 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,222 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862668707, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,223 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,223 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,223 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,223 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,223 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:29,225 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,225 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,226 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39397]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:29,227 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,227 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:29,230 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39397 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,230 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,230 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,231 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,232 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,233 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,233 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:29,234 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:29,236 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,236 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862668707, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,236 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,236 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:29,236 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,237 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862668710, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:29,238 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:29,239 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,239 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,241 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:29,241 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:29,242 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:29,243 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:29,243 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:29,244 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:29,244 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:29,245 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:29,245 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:29,245 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,295 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,344 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,347 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,395 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,445 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,445 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,445 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:29,445 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:29,446 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:29,446 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:29,446 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:29,446 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:29,446 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:29,446 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,446 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,495 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,509 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,596 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,645 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,645 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,645 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,659 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,745 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:29,832 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,832 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,832 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:29,832 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:29,832 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:29,832 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:29,832 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:29,832 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:29,832 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:29,832 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,936 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,936 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,936 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:29,997 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,010 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,043 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,043 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,043 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,043 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,043 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,074 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:30,074 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:30,075 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:30,075 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:30,075 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:30,075 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:30,110 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,146 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,176 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:30,176 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:30,176 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:30,176 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:30,176 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:30,176 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:30,176 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:30,176 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:30,176 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:30,177 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:30,177 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:30,177 INFO   ||  Processed session termination for sessionid: 0x1677562cb6f0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,188 INFO   ||  Session: 0x1677562cb6f0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:30,188 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,189 INFO   ||  EventThread shut down for session: 0x1677562cb6f0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:30,189 INFO   ||  Closed socket connection for client /127.0.0.1:35348 which had sessionid 0x1677562cb6f0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:30,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,249 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,406 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,406 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,406 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,407 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:30,415 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:30,415 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:30,416 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:30,416 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:30,416 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:30,416 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:30,416 INFO   ||  Stopped Kafka server 1 at localhost:39397   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:30,416 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:30,416 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,416 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,416 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:30,416 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,416 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:30,417 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,417 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:30,417 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:30,419 INFO   ||  binding to port localhost/127.0.0.1:39321   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:30,419 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,420 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,420 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 33141
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:39321
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:30,421 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:30,421 INFO   ||  Connecting to zookeeper on localhost:39321   [kafka.server.KafkaServer]
2018-12-03 13:44:30,421 INFO   ||  Initiating client connection, connectString=localhost:39321 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7b86e235   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:30,421 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:30,422 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:30,422 INFO   ||  Opening socket connection to server localhost/127.0.0.1:39321. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:30,422 INFO   ||  Socket connection established to localhost/127.0.0.1:39321, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:30,422 INFO   ||  Accepted socket connection from /127.0.0.1:56256   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:30,422 INFO   ||  Client attempting to establish new session at /127.0.0.1:56256   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,422 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:30,449 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,461 INFO   ||  Established session 0x1677562d4530000 with negotiated timeout 6000 for client /127.0.0.1:56256   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:30,461 INFO   ||  Session establishment complete on server localhost/127.0.0.1:39321, sessionid = 0x1677562d4530000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:30,461 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:30,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,478 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:30,511 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,552 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,611 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,611 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,635 INFO   ||  Cluster ID = ceWwF5chTsWV0ON0S1MosA   [kafka.server.KafkaServer]
2018-12-03 13:44:30,636 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:30,637 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,637 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,637 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:30,638 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:30,638 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:30,638 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:30,659 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:30,659 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:30,660 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:30,660 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:30,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,671 INFO   ||  Awaiting socket connections on localhost:33141.   [kafka.network.Acceptor]
2018-12-03 13:44:30,672 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:30,672 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,672 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,672 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,673 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:30,674 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:30,675 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,675 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,675 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:30,675 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:30,675 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:30,676 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:30,676 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:30,686 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:30,686 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:30,686 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:30,694 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:30,694 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,703 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:30,703 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:30,703 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:30,710 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:30,713 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:30,713 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,713 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,727 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:30,727 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,33141,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:30,727 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:30,729 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:30,729 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:30,730 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:33141 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:30,730 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:30,731 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,749 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,762 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,777 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,777 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,777 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:30,777 INFO   ||  Started Kafka server 1 at localhost:33141 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:30,777 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:30,778 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:33141, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:30,778 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:33141, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:30,778 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:33141]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:30,778 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:30,779 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,779 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,779 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:33141]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,780 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,780 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,780 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:30,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,883 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:30,883 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:33141]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:30,884 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,884 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,884 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:30,884 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:setData cxid:0x54 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,894 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x55 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,911 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:30,920 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:30,920 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:30,920 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:30,920 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:30,920 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:30,920 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:30,921 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x5d zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,927 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x5e zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:30,961 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:30,961 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:30,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:30,963 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:30,963 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:30,963 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:30,964 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:30,964 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:30,964 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:30,987 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:30,988 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:30,988 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:30,989 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:30,989 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,020 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,023 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:setData cxid:0x68 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:31,044 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x69 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:31,103 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:31,119 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:31,120 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:31,120 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:31,120 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:31,120 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:31,120 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:31,121 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:31,121 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x71 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:31,127 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562d4530000 type:create cxid:0x73 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:31,161 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:31,161 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:31,163 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:31,163 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:31,163 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:31,163 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:31,163 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:31,163 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:31,164 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:31,164 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:31,224 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33141 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,224 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,224 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,225 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,226 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,226 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,227 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:31,264 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,264 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,301 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,523 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,523 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,528 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:31,569 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:31,570 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:33141, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:31,570 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:33141, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:31,570 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,571 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,571 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,573 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33141 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,574 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,574 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,575 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,575 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,576 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,576 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,576 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,579 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,579 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,581 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,582 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,582 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,584 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33141 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,584 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,584 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,585 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,585 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,586 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,586 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,587 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,589 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,589 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,591 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,592 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,592 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,594 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33141 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,594 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,595 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,596 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,596 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,596 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,597 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,597 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,600 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,600 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,601 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,602 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,602 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,604 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:33141 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,605 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,605 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,605 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,606 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,606 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,607 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:31,607 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:31,609 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,610 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,611 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,613 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,613 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,617 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:33141, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:31,617 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:33141, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:31,617 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:33141]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:31,618 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,618 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,618 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:33141]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:31,619 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,619 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:31,621 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:31,621 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:31,621 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:31,623 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:31,624 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:31,624 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:31,625 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:31,625 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:31,625 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:31,626 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:31,626 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,675 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,675 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,675 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:31,675 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:31,676 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:31,676 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:31,676 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:31,676 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:31,676 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:31,676 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:31,676 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,851 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,875 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,875 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,875 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,876 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:31,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,914 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:31,914 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,006 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,006 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,006 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:32,006 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:32,006 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:32,006 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:32,006 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:32,006 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:32,006 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:32,006 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,037 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,037 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,037 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,073 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,073 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,073 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,073 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,073 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,076 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,098 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:32,098 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:32,098 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:32,098 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:32,098 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:32,098 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:32,126 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,153 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,216 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:32,216 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:32,216 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:32,216 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:32,216 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:32,216 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:32,216 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:32,216 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:32,216 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:32,217 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:32,217 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:32,217 INFO   ||  Processed session termination for sessionid: 0x1677562d4530000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,223 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,227 INFO   ||  Session: 0x1677562d4530000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:32,227 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,227 INFO   ||  EventThread shut down for session: 0x1677562d4530000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:32,228 INFO   ||  Closed socket connection for client /127.0.0.1:56256 which had sessionid 0x1677562d4530000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:32,266 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,365 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,403 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,637 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:32,644 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:32,645 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:32,645 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:32,645 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:32,645 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:32,645 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:32,645 INFO   ||  Stopped Kafka server 1 at localhost:33141   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:32,645 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:32,645 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,645 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,645 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:32,645 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,645 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:32,646 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,646 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:32,646 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:32,648 INFO   ||  binding to port localhost/127.0.0.1:36587   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:32,648 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,649 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,649 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 43827
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:36587
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:32,650 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:32,650 INFO   ||  Connecting to zookeeper on localhost:36587   [kafka.server.KafkaServer]
2018-12-03 13:44:32,650 INFO   ||  Initiating client connection, connectString=localhost:36587 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@49acd58c   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:32,650 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:32,650 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:32,651 INFO   ||  Opening socket connection to server localhost/127.0.0.1:36587. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:32,651 INFO   ||  Socket connection established to localhost/127.0.0.1:36587, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:32,651 INFO   ||  Accepted socket connection from /127.0.0.1:44386   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:32,651 INFO   ||  Client attempting to establish new session at /127.0.0.1:44386   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,651 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:32,665 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,692 INFO   ||  Established session 0x1677562dd080000 with negotiated timeout 6000 for client /127.0.0.1:44386   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:32,692 INFO   ||  Session establishment complete on server localhost/127.0.0.1:36587, sessionid = 0x1677562dd080000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:32,692 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:32,703 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,705 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,738 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,780 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,815 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,839 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,863 INFO   ||  Cluster ID = bJeNSYl8QZWvZ2iNglCcKQ   [kafka.server.KafkaServer]
2018-12-03 13:44:32,864 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:32,864 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,865 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,865 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:32,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,865 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:32,866 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:32,866 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,866 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:32,866 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:32,890 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:32,890 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:32,891 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:32,891 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:32,902 INFO   ||  Awaiting socket connections on localhost:43827.   [kafka.network.Acceptor]
2018-12-03 13:44:32,903 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:32,903 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,903 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,903 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,904 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:32,905 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:32,905 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,905 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,905 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:32,906 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:32,906 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:32,906 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:32,906 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:32,913 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:32,914 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:32,914 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:32,914 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,922 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:32,930 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:32,931 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:32,931 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:32,938 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:32,941 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:32,941 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,941 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,955 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:32,955 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,43827,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:32,955 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:32,957 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:32,958 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:32,958 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:32,958 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:43827 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:32,959 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,959 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,959 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,959 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:32,959 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:32,959 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:32,959 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:32,997 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:32,997 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:32,997 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:32,997 INFO   ||  Started Kafka server 1 at localhost:43827 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:32,997 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:32,998 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:32,998 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:33,005 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,022 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:33,030 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:43827]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:33,031 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:33,031 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:33,031 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:33,031 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:33,031 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,031 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,031 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:33,032 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:33,032 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,033 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,038 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,072 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:33,072 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:33,074 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:33,074 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:33,075 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:33,075 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:33,075 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:33,075 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:33,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,135 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:33,169 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:33,171 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43827, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,171 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43827, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,171 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:43827]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:33,172 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,172 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,172 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43827]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,173 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,173 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,173 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:33,255 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,266 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,278 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,278 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43827]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,279 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:33,279 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:33,279 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,279 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,279 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:33,383 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,384 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:33,384 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:33,385 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,385 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,389 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,405 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,422 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:33,425 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,428 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,452 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:33,453 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:33,453 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:33,453 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:33,453 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:33,453 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:33,453 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:33,454 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,455 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,463 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562dd080000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:33,469 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,497 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:33,497 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:33,499 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:33,499 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:33,499 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:33,499 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:33,499 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:33,499 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:33,499 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:33,500 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:33,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,555 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,588 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,589 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,589 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,590 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,590 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,591 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,592 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:33,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,628 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,629 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,631 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,631 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,640 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:33,641 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43827, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,641 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43827, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:33,641 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:33,643 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,643 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,646 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,646 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,646 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,647 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,648 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,648 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,649 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,649 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,653 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,653 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,655 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43827]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:33,656 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,656 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:33,659 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43827 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,660 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,660 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,661 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,661 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,662 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,662 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:33,663 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:33,665 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1543862673134, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-12-03 13:44:33,666 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,666 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,667 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:33,667 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:33,668 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:33,669 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:33,669 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:33,670 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:33,670 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:33,670 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:33,671 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:33,671 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,705 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,705 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,706 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:33,706 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:33,706 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:33,706 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:33,706 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:33,706 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:33,706 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:33,706 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:33,706 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,906 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,906 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,906 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:33,918 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,918 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,920 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:33,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,061 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,061 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,061 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:34,061 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:34,061 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:34,062 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:34,062 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:34,062 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:34,062 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:34,062 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,071 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,131 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,137 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,137 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,137 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,157 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,171 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,303 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,303 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,303 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,304 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,304 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:34,336 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:34,336 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:34,336 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:34,336 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:34,336 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:34,336 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:34,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,454 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:34,454 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:34,454 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:34,454 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:34,455 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:34,455 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:34,455 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:34,455 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:34,455 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:34,455 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:34,455 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:34,455 INFO   ||  Processed session termination for sessionid: 0x1677562dd080000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:34,472 INFO   ||  Session: 0x1677562dd080000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:34,472 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,472 INFO   ||  EventThread shut down for session: 0x1677562dd080000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:34,472 INFO   ||  Closed socket connection for client /127.0.0.1:44386 which had sessionid 0x1677562dd080000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:34,472 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,527 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,570 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,572 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,580 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,607 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:34,865 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:34,869 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:34,871 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:34,872 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:34,872 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:34,872 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:34,873 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:34,873 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:34,873 INFO   ||  Stopped Kafka server 1 at localhost:43827   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:34,873 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:34,873 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,873 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,873 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:34,873 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:34,873 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:34,873 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:34,873 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:34,873 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:34,875 INFO   ||  binding to port localhost/127.0.0.1:42591   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:34,875 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,876 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,877 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 43979
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:42591
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:34,878 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:34,878 INFO   ||  Connecting to zookeeper on localhost:42591   [kafka.server.KafkaServer]
2018-12-03 13:44:34,878 INFO   ||  Initiating client connection, connectString=localhost:42591 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@363a5005   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:34,878 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:34,878 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:34,878 INFO   ||  Opening socket connection to server localhost/127.0.0.1:42591. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:34,878 INFO   ||  Socket connection established to localhost/127.0.0.1:42591, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:34,878 INFO   ||  Accepted socket connection from /127.0.0.1:49946   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:34,879 INFO   ||  Client attempting to establish new session at /127.0.0.1:49946   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,879 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:34,923 INFO   ||  Established session 0x1677562e5bc0000 with negotiated timeout 6000 for client /127.0.0.1:49946   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:34,923 INFO   ||  Session establishment complete on server localhost/127.0.0.1:42591, sessionid = 0x1677562e5bc0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:34,923 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:34,936 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:34,969 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:34,969 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:35,008 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,011 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,069 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,070 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,094 INFO   ||  Cluster ID = Sg67NVr9QUutjHkmhpIcww   [kafka.server.KafkaServer]
2018-12-03 13:44:35,094 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:35,095 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:35,095 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:35,095 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:35,096 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:35,096 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:35,097 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:35,133 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,136 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:35,136 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:35,137 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:35,137 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:35,148 INFO   ||  Awaiting socket connections on localhost:43979.   [kafka.network.Acceptor]
2018-12-03 13:44:35,149 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:35,149 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,150 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,150 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,151 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:35,151 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:35,152 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:35,153 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,153 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,153 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:35,161 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:35,161 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:35,161 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:35,161 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:35,161 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:35,161 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:35,161 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:setData cxid:0x2b zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,177 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:35,185 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:35,186 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:35,186 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:35,187 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,189 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:35,189 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,189 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:35,190 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:35,190 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,190 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,190 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,190 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:delete cxid:0x4d zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,244 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:35,244 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:35,244 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,43979,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:35,244 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:35,245 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:35,245 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:35,246 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:35,246 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:43979 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:35,274 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,274 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,274 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:35,274 INFO   ||  Started Kafka server 1 at localhost:43979 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:35,274 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,275 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,285 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,302 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:35,311 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43979, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:35,311 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43979, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:35,311 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:43979]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:35,311 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:35,311 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:35,311 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:35,311 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,311 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:35,312 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,312 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,312 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,312 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,312 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43979]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,313 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,313 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,313 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:35,319 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,320 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,352 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:35,353 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:35,354 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:35,355 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:35,355 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:35,355 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:35,355 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:35,356 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:35,373 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,416 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:35,417 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43979]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:35,417 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:35,417 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,417 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,417 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:35,432 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,471 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,478 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,520 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:35,520 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43979]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:35,521 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:35,521 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,521 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:35,525 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,536 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,552 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:35,570 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,583 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:35,584 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:35,584 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:35,584 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:35,584 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,584 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:35,584 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:35,585 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,602 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562e5bc0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:35,636 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:35,636 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:35,638 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:35,638 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:35,638 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:35,638 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:35,638 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:35,638 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:35,639 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:35,639 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:35,659 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,724 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43979 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:35,724 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:35,724 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:35,725 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:35,726 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:35,727 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:35,728 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:35,759 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:35,759 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:35,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,809 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,860 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:35,984 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,021 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,021 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,025 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,025 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,030 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:36,054 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:36,055 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43979, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:36,055 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43979, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:36,056 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43979]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:36,057 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,057 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,059 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43979 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,059 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,059 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,060 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,060 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,061 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,062 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,062 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,065 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,065 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,067 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43979]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:36,068 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,068 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,071 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43979 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,071 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,071 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,072 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,072 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,073 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,073 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,073 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,077 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,077 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,078 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43979]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:36,079 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,079 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,082 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43979 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,082 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,082 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,083 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,084 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,084 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,085 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,085 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,089 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,089 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,090 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43979]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:36,091 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,091 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:36,094 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43979 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,094 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,094 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,095 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,095 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,095 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,096 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:36,096 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:36,099 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,099 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,100 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:36,101 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:36,102 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:36,103 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:36,103 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:36,104 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:36,105 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:36,105 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:36,105 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:36,105 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,153 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,153 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,153 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:36,153 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:36,154 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:36,154 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:36,154 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:36,154 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:36,154 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:36,154 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,154 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,172 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,172 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,204 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,205 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,322 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,333 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,354 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,354 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,354 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,380 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,425 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,495 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,495 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,495 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:36,495 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:36,495 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:36,495 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:36,495 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:36,495 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:36,495 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:36,495 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,522 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,556 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,556 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,556 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,605 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,660 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,711 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,750 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,750 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,750 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,750 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,750 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:36,776 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:36,776 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:36,776 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:36,776 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:36,776 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:36,776 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:36,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,902 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:36,902 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:36,902 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:36,902 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:36,903 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:36,903 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:36,903 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:36,903 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:36,903 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:36,903 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:36,903 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:36,904 INFO   ||  Processed session termination for sessionid: 0x1677562e5bc0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:36,919 INFO   ||  Session: 0x1677562e5bc0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:36,919 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:36,919 INFO   ||  EventThread shut down for session: 0x1677562e5bc0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:36,919 INFO   ||  Closed socket connection for client /127.0.0.1:49946 which had sessionid 0x1677562e5bc0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:36,923 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,956 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,961 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:36,973 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,036 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,095 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,096 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,096 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,096 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:37,104 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:37,105 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:37,105 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:37,105 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:37,105 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:37,105 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:37,106 INFO   ||  Stopped Kafka server 1 at localhost:43979   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:37,106 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:37,106 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,106 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,106 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:37,106 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,106 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:37,106 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,106 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:37,106 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:37,108 INFO   ||  binding to port localhost/127.0.0.1:37327   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:37,108 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,109 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,110 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35085
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:37327
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:37,110 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:37,110 INFO   ||  Connecting to zookeeper on localhost:37327   [kafka.server.KafkaServer]
2018-12-03 13:44:37,110 INFO   ||  Initiating client connection, connectString=localhost:37327 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@c1ef64e   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:37,110 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:37,111 INFO   ||  Opening socket connection to server localhost/127.0.0.1:37327. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:37,111 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:37,111 INFO   ||  Socket connection established to localhost/127.0.0.1:37327, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:37,111 INFO   ||  Accepted socket connection from /127.0.0.1:47774   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:37,111 INFO   ||  Client attempting to establish new session at /127.0.0.1:47774   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,111 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:37,154 INFO   ||  Established session 0x1677562ee750000 with negotiated timeout 6000 for client /127.0.0.1:47774   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:37,154 INFO   ||  Session establishment complete on server localhost/127.0.0.1:37327, sessionid = 0x1677562ee750000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:37,154 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:37,172 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,173 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,174 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,205 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,247 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,305 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,330 INFO   ||  Cluster ID = fdabpDLKTvujIRThOjWqxg   [kafka.server.KafkaServer]
2018-12-03 13:44:37,330 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:37,331 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,331 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,331 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:37,332 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,332 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:37,332 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:37,333 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:37,649 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,649 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,671 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:37,671 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:37,672 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:37,672 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:37,682 INFO   ||  Awaiting socket connections on localhost:35085.   [kafka.network.Acceptor]
2018-12-03 13:44:37,683 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:37,683 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,683 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,684 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,684 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:37,685 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:37,685 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,685 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,685 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:37,686 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:37,686 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:37,686 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:37,686 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:37,688 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:37,689 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:37,689 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:37,697 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:37,697 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,705 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:37,706 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:37,706 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:37,713 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:37,716 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:37,716 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,716 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x44 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,730 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:37,730 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,35085,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:37,730 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:37,732 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,732 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:37,733 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:35085 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:37,733 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:37,733 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,750 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,772 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,772 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,772 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:37,772 INFO   ||  Started Kafka server 1 at localhost:35085 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:37,772 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:37,772 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:37,773 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,780 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,797 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:37,799 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,800 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,805 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35085]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:37,806 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:37,806 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:37,806 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:37,806 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:37,806 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:37,806 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,806 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,806 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:37,806 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,807 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,813 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:37,847 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:37,847 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:37,849 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:37,849 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:37,849 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:37,850 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:37,850 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:37,850 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:37,850 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:37,910 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:37,935 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:37,936 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35085, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:37,936 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35085, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:37,936 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:35085]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:37,937 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,937 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,937 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35085]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:37,938 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,938 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:37,938 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:37,950 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,000 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,042 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,042 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35085]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:38,042 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,042 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,042 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,042 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:38,050 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,100 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,100 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,145 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,146 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:38,146 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35085]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:38,147 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,147 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,150 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:38,163 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:38,180 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:38,214 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:38,214 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:38,214 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:38,215 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:38,215 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:38,215 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:38,215 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:38,215 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:38,230 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562ee750000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:38,250 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,263 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:38,264 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:38,266 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:38,266 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:38,267 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:38,267 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:38,267 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:38,267 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:38,267 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:38,268 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:38,349 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35085 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,349 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,349 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,350 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,350 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,351 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,352 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:38,415 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,415 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,417 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,417 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862677931, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,418 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,418 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,418 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,418 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,419 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,419 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,426 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:38,427 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35085, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,427 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35085, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,428 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35085]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:38,428 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,429 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,431 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35085 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,431 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,431 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,432 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,432 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,432 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,433 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,433 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,435 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,435 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862677931, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,435 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,435 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,435 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,435 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,436 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,436 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,438 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35085]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:38,439 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,439 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,441 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35085 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,441 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,441 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,442 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,442 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,442 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,443 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,443 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,445 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,445 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862677931, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,446 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,446 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,446 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,446 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,447 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:38,449 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,449 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,451 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35085]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:38,452 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,452 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,454 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35085 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,454 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,454 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,455 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,455 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,456 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,456 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,456 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,458 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,459 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862677931, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,459 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,459 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,459 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,459 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,460 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:38,461 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,461 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,462 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35085]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:38,463 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,463 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:38,465 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35085 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,465 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,465 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,466 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,466 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,467 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,467 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:38,467 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:38,469 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,469 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862677931, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,470 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,470 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:38,470 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,470 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862677933, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:38,470 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:38,472 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,472 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,473 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:38,473 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:38,474 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:38,475 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:38,475 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:38,476 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:38,476 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:38,476 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:38,477 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:38,477 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,485 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,485 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,486 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:38,486 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:38,486 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:38,486 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:38,486 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:38,486 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:38,486 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:38,486 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,486 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,501 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,551 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,601 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,601 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,651 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,651 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,651 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,651 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,677 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,686 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,686 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,686 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,701 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,751 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,751 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,851 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,866 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,866 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,866 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:38,866 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:38,867 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:38,867 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:38,867 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:38,867 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:38,867 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:38,867 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,901 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:38,917 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,917 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,917 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:38,927 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,002 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,002 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,051 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,084 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,084 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,084 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,084 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,084 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,102 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,114 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:39,114 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:39,114 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:39,114 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:39,115 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:39,115 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:39,152 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,225 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:39,225 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:39,225 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:39,225 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:39,226 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:39,226 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:39,226 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:39,226 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:39,226 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:39,226 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:39,226 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:39,226 INFO   ||  Processed session termination for sessionid: 0x1677562ee750000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,238 INFO   ||  Session: 0x1677562ee750000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:39,238 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,238 INFO   ||  EventThread shut down for session: 0x1677562ee750000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:39,239 INFO   ||  Closed socket connection for client /127.0.0.1:47774 which had sessionid 0x1677562ee750000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:39,278 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,302 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,331 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,332 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,332 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,332 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:39,338 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:39,339 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:39,339 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:39,339 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:39,339 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:39,339 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:39,339 INFO   ||  Stopped Kafka server 1 at localhost:35085   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:39,340 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:39,340 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,340 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,340 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:39,340 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,340 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:39,340 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,340 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:39,340 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:39,342 INFO   ||  binding to port localhost/127.0.0.1:46027   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:39,342 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,343 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,343 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 43375
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46027
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:39,344 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:39,344 INFO   ||  Connecting to zookeeper on localhost:46027   [kafka.server.KafkaServer]
2018-12-03 13:44:39,344 INFO   ||  Initiating client connection, connectString=localhost:46027 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2199a0a4   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:39,344 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:39,345 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46027. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:39,345 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:39,345 INFO   ||  Socket connection established to localhost/127.0.0.1:46027, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:39,345 INFO   ||  Accepted socket connection from /127.0.0.1:35408   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:39,345 INFO   ||  Client attempting to establish new session at /127.0.0.1:35408   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,345 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:39,378 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,402 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,427 INFO   ||  Established session 0x1677562f72e0000 with negotiated timeout 6000 for client /127.0.0.1:35408   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:39,427 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46027, sessionid = 0x1677562f72e0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:39,427 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:39,444 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,452 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,477 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:39,519 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,552 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,577 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,602 INFO   ||  Cluster ID = N0L_0WP7QF2hkCeEigLHww   [kafka.server.KafkaServer]
2018-12-03 13:44:39,602 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,602 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,602 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,602 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:39,603 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,603 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,604 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:39,604 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:39,604 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:39,605 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:39,629 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:39,630 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:39,632 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:39,632 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:39,642 INFO   ||  Awaiting socket connections on localhost:43375.   [kafka.network.Acceptor]
2018-12-03 13:44:39,644 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:39,644 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,645 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,645 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,646 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:39,647 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:39,647 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,647 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,647 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:39,647 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:39,648 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:39,648 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:39,648 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:39,652 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:39,652 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:39,652 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:39,653 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,653 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,660 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:39,669 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:39,669 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:39,670 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:39,677 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:39,679 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:39,679 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,679 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,694 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:39,694 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,43375,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:39,694 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:39,695 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:39,696 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:39,696 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:39,696 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:43375 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:39,697 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,697 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,697 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,697 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:39,697 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:39,697 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:39,697 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,702 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,703 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,744 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,744 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,744 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:39,744 INFO   ||  Started Kafka server 1 at localhost:43375 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:39,744 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:39,744 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43375, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:39,744 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43375, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:39,744 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:43375]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:39,745 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:39,745 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,745 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,746 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43375]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,746 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,746 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,746 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:39,752 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,803 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,850 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:39,850 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:43375]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:39,850 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,850 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,850 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:39,851 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:setData cxid:0x54 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,853 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,853 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,860 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x55 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,877 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:39,886 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:39,886 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:39,886 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:39,886 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:39,887 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:39,887 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:39,887 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x5d zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,894 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x5e zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,927 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:39,928 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:39,929 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:39,929 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:39,930 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:39,930 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:39,930 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:39,930 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:39,953 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:39,953 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:39,954 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:39,954 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:39,955 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,955 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:39,958 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:setData cxid:0x68 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:39,977 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x69 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:40,003 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,011 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:40,027 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:40,028 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:40,028 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:40,028 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:40,028 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:40,028 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:40,028 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:40,029 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x71 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:40,035 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677562f72e0000 type:create cxid:0x72 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:40,069 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:40,069 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:40,070 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:40,071 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:40,071 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:40,071 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:40,071 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:40,071 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:40,071 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:40,072 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:40,079 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,103 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,153 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,153 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43375 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,159 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,160 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,160 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,161 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:40,179 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,195 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,196 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,203 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,253 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,354 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,456 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,456 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,460 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:40,499 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:40,500 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43375, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:40,500 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43375, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:40,500 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,501 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,501 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,503 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43375 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,504 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,504 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,504 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,504 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,505 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,505 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,506 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,506 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,508 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,508 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,510 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,511 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,511 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,513 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43375 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,513 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,513 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,514 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,514 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,514 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,515 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,515 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,518 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,518 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,519 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,520 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,520 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,522 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43375 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,522 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,522 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,523 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,523 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,523 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,526 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,526 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,528 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,529 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,529 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,531 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:43375 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,532 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,532 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,532 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,532 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,533 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,533 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:40,533 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:40,536 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,536 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,537 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,538 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,538 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,542 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:43375, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:40,542 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:43375, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:40,542 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:43375]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:40,543 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,543 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,543 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43375]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:40,544 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,544 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:40,546 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:40,546 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:40,546 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:40,548 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:40,548 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:40,548 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:40,550 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:40,550 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:40,550 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:40,550 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:40,550 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,554 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,599 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,647 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,647 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,647 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:40,647 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:40,647 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:40,647 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:40,648 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:40,648 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:40,648 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:40,648 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,648 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,650 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,700 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,704 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,704 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,750 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,754 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,804 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,804 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,848 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,848 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,848 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,854 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,855 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,900 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:40,933 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,933 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,933 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:40,933 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:40,933 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:40,933 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:40,933 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:40,933 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:40,933 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:40,933 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,997 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,997 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:40,997 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,001 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,005 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,005 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,044 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,044 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,044 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,046 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,046 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:41,079 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:41,079 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:41,079 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:41,079 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:41,079 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:41,079 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:41,081 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,105 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,105 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,155 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,155 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,181 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,205 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,214 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:41,214 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:41,214 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:41,214 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:41,214 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:41,214 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:41,214 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:41,214 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:41,214 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:41,214 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:41,214 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:41,215 INFO   ||  Processed session termination for sessionid: 0x1677562f72e0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:41,227 INFO   ||  Session: 0x1677562f72e0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:41,227 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,227 INFO   ||  EventThread shut down for session: 0x1677562f72e0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:41,227 INFO   ||  Closed socket connection for client /127.0.0.1:35408 which had sessionid 0x1677562f72e0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:41,255 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,451 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,455 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,505 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,555 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,603 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,603 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,603 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,604 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,604 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,604 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:41,605 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,605 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,856 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,906 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,932 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,956 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:41,957 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,006 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,006 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,006 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,106 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,206 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,207 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,207 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,252 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,283 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,303 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,457 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,604 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:42,604 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:42,604 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:42,610 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:42,611 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:42,611 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:42,611 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:42,611 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:42,611 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:42,612 INFO   ||  Stopped Kafka server 1 at localhost:43375   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:42,612 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:42,612 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,612 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,612 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:42,612 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,612 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:42,612 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,612 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:42,612 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:42,614 INFO   ||  binding to port localhost/127.0.0.1:37089   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:42,614 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,615 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,616 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40781
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:37089
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:42,616 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:42,616 INFO   ||  Connecting to zookeeper on localhost:37089   [kafka.server.KafkaServer]
2018-12-03 13:44:42,616 INFO   ||  Initiating client connection, connectString=localhost:37089 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@37c400ec   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:42,616 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:42,618 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:42,618 INFO   ||  Opening socket connection to server localhost/127.0.0.1:37089. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:42,618 INFO   ||  Accepted socket connection from /127.0.0.1:57572   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:42,618 INFO   ||  Socket connection established to localhost/127.0.0.1:37089, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:42,619 INFO   ||  Client attempting to establish new session at /127.0.0.1:57572   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,619 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:42,656 INFO   ||  Established session 0x167756303f60000 with negotiated timeout 6000 for client /127.0.0.1:57572   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:42,656 INFO   ||  Session establishment complete on server localhost/127.0.0.1:37089, sessionid = 0x167756303f60000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:42,657 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:42,672 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,705 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,707 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,707 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,747 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,784 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,805 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,830 INFO   ||  Cluster ID = W6_GIWVTTOOen2nVI2zoLQ   [kafka.server.KafkaServer]
2018-12-03 13:44:42,830 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:42,831 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:42,832 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:42,833 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:42,833 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:42,834 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:42,834 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:42,854 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:42,854 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:42,854 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:42,855 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:42,858 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,867 INFO   ||  Awaiting socket connections on localhost:40781.   [kafka.network.Acceptor]
2018-12-03 13:44:42,867 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:42,867 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,868 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,868 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,869 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:42,870 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:42,870 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,870 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,870 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:42,870 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:42,870 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:42,871 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:42,871 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:42,880 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:42,880 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:42,880 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:42,880 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,888 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:42,897 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:42,897 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:42,897 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:42,905 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:42,907 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:42,907 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,907 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,922 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:42,922 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40781,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:42,922 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:42,923 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:42,923 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:42,924 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40781 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:42,924 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:42,924 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,958 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,958 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,958 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:42,963 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:42,963 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:42,963 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:42,963 INFO   ||  Started Kafka server 1 at localhost:40781 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:42,963 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:42,964 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:42,964 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,972 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,988 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:42,997 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40781]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:42,997 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:42,997 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:42,997 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:42,997 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:42,998 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:42,998 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:42,998 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:42,998 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:42,998 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:42,999 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:43,005 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:43,038 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:43,039 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:43,040 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:43,040 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:43,041 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:43,041 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:43,041 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:43,041 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:43,058 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,058 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,101 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:43,108 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,108 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,127 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:43,128 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40781, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,128 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40781, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,128 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40781]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:43,129 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,129 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,129 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40781]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,130 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,130 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,130 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:43,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,208 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,208 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,234 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,234 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40781]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:43,234 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,234 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,234 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:43,234 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:43,258 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,337 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,337 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40781]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:43,337 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:43,339 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,339 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,342 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:43,355 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:43,371 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:43,385 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,402 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:43,403 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:43,403 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:43,403 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:43,403 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:43,403 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:43,403 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:43,404 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:43,408 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,408 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,413 INFO   ||  Got user-level KeeperException when processing sessionid:0x167756303f60000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:43,447 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:43,447 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:43,448 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:43,449 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:43,449 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:43,449 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:43,449 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:43,449 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:43,449 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:43,450 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:43,454 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,505 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40781 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,543 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,544 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,544 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,545 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:43,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,620 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,620 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,622 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,622 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,629 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:43,630 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40781, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,630 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40781, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:43,630 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40781]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:43,631 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,631 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,633 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40781 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,633 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,633 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,634 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,634 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,635 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,636 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,636 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,638 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,638 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,640 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40781]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:43,640 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,640 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:43,642 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40781 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,643 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,643 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,643 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,644 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,644 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,645 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:43,645 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:43,646 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1543862683100, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-12-03 13:44:43,647 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,647 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,648 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:43,648 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:43,649 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:43,650 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:43,650 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:43,651 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:43,651 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:43,651 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:43,652 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:43,652 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,659 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,670 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,670 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,670 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:43,670 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:43,670 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:43,670 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:43,670 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:43,670 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:43,670 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:43,670 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:43,671 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,685 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,750 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,751 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,809 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,809 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,809 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,850 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,852 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,870 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,870 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,870 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:43,909 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:43,910 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,044 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,044 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,044 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:44,044 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:44,044 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:44,044 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:44,044 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:44,044 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:44,044 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:44,044 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,052 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,059 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,101 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,109 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,136 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,136 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,136 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,160 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,256 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,260 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,260 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,268 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,268 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,268 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,268 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,268 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:44,303 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:44,303 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:44,303 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:44,303 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:44,303 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:44,303 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:44,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,360 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,412 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:44,412 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:44,413 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:44,413 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:44,413 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:44,413 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:44,413 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:44,413 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:44,413 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:44,413 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:44,413 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:44,414 INFO   ||  Processed session termination for sessionid: 0x167756303f60000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:44,430 INFO   ||  Session: 0x167756303f60000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:44,430 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,430 INFO   ||  EventThread shut down for session: 0x167756303f60000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:44,430 INFO   ||  Closed socket connection for client /127.0.0.1:57572 which had sessionid 0x167756303f60000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:44,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,460 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,460 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,552 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,587 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,707 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,811 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,831 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,831 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,831 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:44,833 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:44,839 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:44,840 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:44,840 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:44,840 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:44,840 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:44,840 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:44,841 INFO   ||  Stopped Kafka server 1 at localhost:40781   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:44,841 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:44,841 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,841 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,841 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:44,841 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:44,841 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:44,841 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:44,841 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:44,841 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:44,843 INFO   ||  binding to port localhost/127.0.0.1:46185   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:44,843 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,844 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,844 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 38545
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46185
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:44,845 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:44,845 INFO   ||  Connecting to zookeeper on localhost:46185   [kafka.server.KafkaServer]
2018-12-03 13:44:44,845 INFO   ||  Initiating client connection, connectString=localhost:46185 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@525be1a   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:44,845 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:44,845 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:44,846 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46185. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:44,846 INFO   ||  Socket connection established to localhost/127.0.0.1:46185, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:44,846 INFO   ||  Accepted socket connection from /127.0.0.1:42360   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:44,846 INFO   ||  Client attempting to establish new session at /127.0.0.1:42360   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,846 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:44,887 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,905 INFO   ||  Established session 0x16775630cab0000 with negotiated timeout 6000 for client /127.0.0.1:42360   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:44,905 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46185, sessionid = 0x16775630cab0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:44,906 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:44,911 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,911 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:44,919 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:44,952 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:45,011 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,011 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,035 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,094 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,119 INFO   ||  Cluster ID = kZDblS0vRgegz4eRidDrmA   [kafka.server.KafkaServer]
2018-12-03 13:44:45,119 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:45,119 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:45,120 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:45,120 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:45,120 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:45,121 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:45,121 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:45,143 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:45,143 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:45,144 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:45,144 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:45,154 INFO   ||  Awaiting socket connections on localhost:38545.   [kafka.network.Acceptor]
2018-12-03 13:44:45,155 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:45,155 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,155 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,155 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,156 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:45,157 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:45,157 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,157 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,158 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:45,158 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:45,158 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:45,158 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:45,158 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:45,160 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:45,161 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:45,161 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:45,169 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:45,169 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,177 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:45,178 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:45,178 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:45,185 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:45,188 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:45,188 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x44 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,188 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x45 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,202 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:45,202 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,38545,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:45,202 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:45,204 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,204 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:45,205 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:38545 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:45,205 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,205 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,212 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,247 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,247 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,247 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:45,247 INFO   ||  Started Kafka server 1 at localhost:38545 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:45,247 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:45,248 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:45,248 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,260 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,277 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:45,286 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38545, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:45,286 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38545, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:45,286 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:38545]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:45,286 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:45,286 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:45,286 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:45,286 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,286 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:45,287 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,287 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,287 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,287 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,287 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38545]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,288 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,288 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,288 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:45,294 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,303 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,312 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,327 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:45,328 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:45,329 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:45,329 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:45,330 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:45,331 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:45,331 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:45,331 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:45,361 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,361 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,392 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:45,392 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38545]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:45,393 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:45,393 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,393 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,393 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:45,438 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,495 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:45,496 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38545]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:45,496 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:45,497 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,497 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:45,501 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,519 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,535 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:45,565 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:45,565 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:45,565 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:45,566 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:45,566 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,566 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:45,566 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:45,566 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,577 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775630cab0000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:45,611 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:45,611 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:45,612 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:45,613 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:45,613 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:45,613 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:45,613 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:45,613 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:45,613 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:45,613 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:45,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,699 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38545 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:45,699 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:45,699 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:45,700 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:45,701 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:45,701 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:45,702 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:45,708 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,732 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:45,732 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:45,739 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,762 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,762 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,812 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:45,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,000 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,000 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,004 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:46,012 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,012 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,013 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,035 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:46,036 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38545, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:46,036 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38545, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:46,036 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38545]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:46,037 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,037 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,039 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38545 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,040 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,040 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,040 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,040 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,041 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,042 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,042 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,044 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,044 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,045 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38545]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:46,046 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,046 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,048 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38545 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,048 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,048 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,049 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,049 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,049 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,050 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,050 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,052 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,052 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,053 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38545]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:46,054 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,054 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,056 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38545 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,057 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,057 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,057 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,057 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,058 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,058 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,058 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,062 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,062 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,063 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38545]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:46,064 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,064 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:46,066 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38545 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,066 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,066 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,067 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,067 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,067 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:46,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:46,071 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,071 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,072 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:46,072 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:46,074 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:46,074 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:46,075 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:46,076 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:46,076 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:46,076 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:46,077 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:46,077 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,113 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,125 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,158 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,158 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,158 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:46,158 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:46,158 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:46,158 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:46,158 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:46,158 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:46,158 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:46,158 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,158 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,163 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,164 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,176 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,213 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,326 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,358 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,358 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,358 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,361 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,467 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,467 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,467 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:46,467 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:46,467 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:46,467 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:46,467 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:46,467 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:46,467 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:46,467 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,476 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,490 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,537 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,537 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,537 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,555 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,555 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,555 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,556 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,556 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:46,560 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,582 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:46,582 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:46,583 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:46,583 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:46,583 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:46,583 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:46,590 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,684 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:46,684 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:46,684 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:46,684 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:46,684 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:46,684 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:46,684 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:46,684 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:46,684 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:46,685 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:46,685 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:46,685 INFO   ||  Processed session termination for sessionid: 0x16775630cab0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:46,702 INFO   ||  Session: 0x16775630cab0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:46,702 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:46,702 INFO   ||  EventThread shut down for session: 0x16775630cab0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:46,703 INFO   ||  Closed socket connection for client /127.0.0.1:42360 which had sessionid 0x16775630cab0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:46,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,764 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,764 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,864 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,864 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,914 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,914 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:46,964 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,028 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,120 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:47,126 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:47,127 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:47,127 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:47,127 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:47,127 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:47,127 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:47,128 INFO   ||  Stopped Kafka server 1 at localhost:38545   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:47,128 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:47,128 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,128 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,128 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:47,128 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,128 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:47,128 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,128 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:47,128 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:47,130 INFO   ||  binding to port localhost/127.0.0.1:36103   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:47,130 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,131 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,131 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40707
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:36103
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:47,132 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:47,132 INFO   ||  Connecting to zookeeper on localhost:36103   [kafka.server.KafkaServer]
2018-12-03 13:44:47,132 INFO   ||  Initiating client connection, connectString=localhost:36103 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5129fe19   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:47,132 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:47,132 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:47,132 INFO   ||  Opening socket connection to server localhost/127.0.0.1:36103. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:47,133 INFO   ||  Socket connection established to localhost/127.0.0.1:36103, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:47,133 INFO   ||  Accepted socket connection from /127.0.0.1:49318   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:47,133 INFO   ||  Client attempting to establish new session at /127.0.0.1:49318   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,133 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:47,178 INFO   ||  Established session 0x1677563159a0000 with negotiated timeout 6000 for client /127.0.0.1:49318   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:47,178 INFO   ||  Session establishment complete on server localhost/127.0.0.1:36103, sessionid = 0x1677563159a0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:47,178 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:47,197 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,212 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,214 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,215 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,230 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,272 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,330 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,355 INFO   ||  Cluster ID = IxAi0JXLTKuaLaP8jIHnzg   [kafka.server.KafkaServer]
2018-12-03 13:44:47,355 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:47,355 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,356 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,356 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:47,356 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:47,357 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:47,357 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:47,365 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,376 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:47,377 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:47,377 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:47,378 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:47,387 INFO   ||  Awaiting socket connections on localhost:40707.   [kafka.network.Acceptor]
2018-12-03 13:44:47,388 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:47,388 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,388 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,389 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,390 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:47,390 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:47,390 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,391 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,391 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:47,391 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:47,391 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:47,391 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:47,391 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:47,397 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:47,397 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:47,397 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:47,397 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,405 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:47,414 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:47,414 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:47,414 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:47,421 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:47,423 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:47,423 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,424 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,438 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:47,438 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40707,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:47,438 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:47,440 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:47,440 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:47,441 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40707 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:47,441 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,441 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,480 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,480 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,480 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:47,480 INFO   ||  Started Kafka server 1 at localhost:40707 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:47,480 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:47,481 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:47,481 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,488 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:47,505 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:47,513 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40707]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:47,514 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:47,514 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:47,514 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:47,514 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,514 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:47,514 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,514 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,515 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,515 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,515 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,516 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,521 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,555 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:47,555 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:47,557 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:47,557 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:47,557 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:47,557 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:47,557 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:47,558 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:47,561 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,618 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:47,628 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,651 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:47,652 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40707, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:47,652 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40707, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:47,652 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40707]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:47,653 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,653 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,653 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40707]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,654 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,654 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,654 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:47,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,692 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,715 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,742 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,757 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:47,757 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40707]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:47,758 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:47,758 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,758 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,758 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:47,861 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:47,862 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40707]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:47,862 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:47,863 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,863 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:47,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,866 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,880 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,896 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:47,916 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,929 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:47,929 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:47,930 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:47,930 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:47,930 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:47,930 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,930 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:47,931 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:47,931 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,946 INFO   ||  Got user-level KeeperException when processing sessionid:0x1677563159a0000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:47,980 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:47,980 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:47,982 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:47,982 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:47,982 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:47,982 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:47,982 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:47,982 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:47,982 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:47,983 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:48,016 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,016 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,065 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40707 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,065 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,065 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,066 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,066 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,066 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,066 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,067 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,067 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,068 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:48,105 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,105 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,107 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,107 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862687646, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,108 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,108 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,108 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,108 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,109 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,109 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,115 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:48,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,116 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40707, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,116 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40707, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,116 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40707]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:48,117 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,117 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,119 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40707 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,120 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,120 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,121 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,121 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,121 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,122 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,122 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,125 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,125 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862687646, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,125 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,125 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,125 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,126 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,127 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,127 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,128 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40707]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:48,129 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,129 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,131 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40707 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,132 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,132 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,132 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,133 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,133 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,133 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,134 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,135 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,135 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862687646, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,136 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,136 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,136 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,136 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,136 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:48,138 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,138 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,140 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40707]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:48,141 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,141 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,144 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40707 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,144 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,144 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,145 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,145 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,146 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,147 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,147 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,149 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,149 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862687646, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,149 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,150 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,150 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,150 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,150 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:48,152 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,152 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,154 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40707]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:48,155 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,155 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:48,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40707 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,158 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,159 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,159 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,159 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,160 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:48,160 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:48,162 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,162 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1543862687646, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,162 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,162 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:48,162 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,163 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1543862687649, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:48,164 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,164 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-12-03 13:44:48,165 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,165 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,166 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:48,166 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:48,168 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:48,168 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:48,168 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:48,169 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:48,169 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:48,170 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:48,171 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:48,171 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,191 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,191 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,191 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:48,191 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:48,191 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:48,191 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:48,191 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:48,191 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:48,191 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:48,191 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,191 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,270 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,370 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,391 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,391 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,391 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,543 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,559 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,559 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,559 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:48,559 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:48,559 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:48,559 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:48,559 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:48,559 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:48,559 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:48,559 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,566 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,570 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,617 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,617 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,617 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,620 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,693 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,730 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,764 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,781 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,789 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,789 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,789 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,789 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,789 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:48,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,822 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:48,822 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:48,822 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:48,822 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:48,822 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:48,822 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:48,867 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,921 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,940 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:48,940 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:48,940 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:48,940 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:48,940 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:48,940 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:48,940 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:48,940 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:48,940 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:48,941 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:48,941 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:48,941 INFO   ||  Processed session termination for sessionid: 0x1677563159a0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:48,955 INFO   ||  Session: 0x1677563159a0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:48,955 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:48,955 INFO   ||  EventThread shut down for session: 0x1677563159a0000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:48,955 INFO   ||  Closed socket connection for client /127.0.0.1:49318 which had sessionid 0x1677563159a0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:48,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:48,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,015 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,017 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,071 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,117 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,167 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,217 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,268 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,315 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,318 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,356 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:49,362 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:49,363 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:49,363 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:49,363 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:49,363 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:49,363 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:49,364 INFO   ||  Stopped Kafka server 1 at localhost:40707   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:49,364 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:49,364 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,364 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,364 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:49,364 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,364 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:49,364 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,364 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:49,364 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-12-03 13:44:49,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,366 INFO   ||  binding to port localhost/127.0.0.1:43249   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:49,366 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,367 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,368 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 39029
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:43249
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:49,368 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:49,368 INFO   ||  Connecting to zookeeper on localhost:43249   [kafka.server.KafkaServer]
2018-12-03 13:44:49,369 INFO   ||  Initiating client connection, connectString=localhost:43249 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@70bc7023   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:49,369 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:49,369 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:49,369 INFO   ||  Opening socket connection to server localhost/127.0.0.1:43249. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:49,369 INFO   ||  Socket connection established to localhost/127.0.0.1:43249, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:49,369 INFO   ||  Accepted socket connection from /127.0.0.1:37210   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:49,369 INFO   ||  Client attempting to establish new session at /127.0.0.1:37210   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,370 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:49,416 INFO   ||  Established session 0x16775631e570000 with negotiated timeout 6000 for client /127.0.0.1:37210   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:49,416 INFO   ||  Session establishment complete on server localhost/127.0.0.1:43249, sessionid = 0x16775631e570000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:49,416 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:49,427 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,445 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,460 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:49,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:49,502 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,561 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:49,585 INFO   ||  Cluster ID = 6LIzbZwfQ9iaNViSquvuig   [kafka.server.KafkaServer]
2018-12-03 13:44:49,585 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:49,586 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,586 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,586 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:49,587 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:49,587 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:49,587 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,231 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:50,231 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:50,232 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:50,232 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:50,242 INFO   ||  Awaiting socket connections on localhost:39029.   [kafka.network.Acceptor]
2018-12-03 13:44:50,243 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:50,243 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,243 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,244 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,245 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:50,245 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:50,245 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,246 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,246 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:50,246 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:50,246 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:50,246 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:50,246 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:50,260 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:50,261 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:50,261 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:50,268 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:50,269 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,277 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:50,277 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:50,278 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:50,285 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:50,288 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:50,288 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x46 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,288 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x47 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,302 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:50,302 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,39029,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,302 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:50,303 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:50,303 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,303 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,310 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,352 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,352 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,352 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:50,352 INFO   ||  Started Kafka server 1 at localhost:39029 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:50,352 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:50,353 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39029, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:50,353 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39029, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:50,353 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:39029]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:50,353 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:50,354 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:50,355 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:50,355 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:39029 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:50,355 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,355 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,355 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39029]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,356 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,356 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,356 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:50,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,459 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:50,459 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39029]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-12-03 13:44:50,460 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,460 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,460 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-12-03 13:44:50,460 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:setData cxid:0x53 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,469 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x54 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,485 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:50,494 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:50,494 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:50,494 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-12-03 13:44:50,494 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,495 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:50,495 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,495 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x5c zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,502 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x5d zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,535 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:50,536 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:50,538 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:50,538 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:50,538 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:50,539 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-12-03 13:44:50,539 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:50,539 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:50,563 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:50,563 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:50,563 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-12-03 13:44:50,564 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,564 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:50,568 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,585 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,610 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:50,667 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:50,677 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:50,678 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:50,678 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:50,678 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:50,678 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,678 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:50,678 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:50,679 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,685 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775631e570000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:50,719 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:50,719 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:50,720 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:50,721 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:50,721 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:50,721 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:50,721 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:50,721 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:50,721 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:50,721 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:50,779 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:50,779 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:50,779 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:50,780 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:50,780 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:50,781 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:50,781 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:50,818 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:50,818 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,067 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,067 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,070 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:51,097 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:51,098 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39029, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:51,098 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39029, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:51,098 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,099 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,099 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,101 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,101 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,101 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,102 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,102 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,102 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,103 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,103 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,105 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,105 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,106 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,107 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,107 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,109 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,109 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,109 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,110 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,110 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,111 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,111 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,111 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,114 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,114 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,115 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,116 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,116 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,117 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,118 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,118 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,118 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,118 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,119 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,119 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,119 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,122 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,122 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,123 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,123 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,123 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,125 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39029 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,126 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,126 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,126 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,126 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,127 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,127 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:51,128 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:51,130 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,130 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,132 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,133 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,133 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,136 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39029, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:51,136 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39029, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-12-03 13:44:51,136 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:39029]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:51,137 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,137 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,137 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39029]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:51,138 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,138 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:51,140 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:51,140 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:51,140 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:51,142 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:51,142 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:51,142 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:51,144 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:51,144 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:51,144 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:51,144 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:51,144 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,194 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,212 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,243 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,246 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,246 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,246 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:51,246 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:51,246 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:51,246 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:51,246 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:51,246 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:51,246 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:51,246 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,246 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,312 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,312 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,344 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,344 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,446 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,446 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,446 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,526 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,526 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,527 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:51,527 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:51,527 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:51,527 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:51,527 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:51,527 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:51,527 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:51,527 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,544 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,595 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,597 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,597 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,597 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,612 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,644 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,644 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,644 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,644 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,644 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:51,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,677 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:51,677 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:51,677 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:51,677 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:51,677 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:51,677 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:51,796 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:51,796 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:51,796 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:51,796 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:51,796 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:51,796 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:51,796 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:51,797 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:51,797 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:51,797 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:51,797 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:51,797 INFO   ||  Processed session termination for sessionid: 0x16775631e570000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:51,810 INFO   ||  Session: 0x16775631e570000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:51,810 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:51,810 INFO   ||  EventThread shut down for session: 0x16775631e570000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:51,811 INFO   ||  Closed socket connection for client /127.0.0.1:37210 which had sessionid 0x16775631e570000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:51,913 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:51,995 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,045 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,063 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,063 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,163 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,163 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,164 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,413 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,514 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,514 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,564 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,586 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,587 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,587 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,587 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:52,593 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:52,593 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:52,594 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:52,594 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:52,594 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:52,594 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:52,595 INFO   ||  Stopped Kafka server 1 at localhost:39029   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:52,595 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:52,595 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,595 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,595 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:52,595 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,595 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:52,595 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,595 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:52,595 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowAsynchronousProductionAndAutomaticConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
2018-12-03 13:44:52,601 INFO   ||  binding to port localhost/127.0.0.1:41663   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:52,601 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,602 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,602 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 38963
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:41663
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:52,603 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:52,603 INFO   ||  Connecting to zookeeper on localhost:41663   [kafka.server.KafkaServer]
2018-12-03 13:44:52,603 INFO   ||  Initiating client connection, connectString=localhost:41663 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@353371a7   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:52,603 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:52,604 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:52,604 INFO   ||  Opening socket connection to server localhost/127.0.0.1:41663. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:52,604 INFO   ||  Socket connection established to localhost/127.0.0.1:41663, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:52,604 INFO   ||  Accepted socket connection from /127.0.0.1:34160   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:52,604 INFO   ||  Client attempting to establish new session at /127.0.0.1:34160   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,604 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:52,613 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,647 INFO   ||  Established session 0x16775632af90000 with negotiated timeout 6000 for client /127.0.0.1:34160   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:52,647 INFO   ||  Session establishment complete on server localhost/127.0.0.1:41663, sessionid = 0x16775632af90000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:52,648 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:52,663 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,697 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,738 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,797 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,821 INFO   ||  Cluster ID = 6r12oRs1TtKOdI0N_OnlGg   [kafka.server.KafkaServer]
2018-12-03 13:44:52,821 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:52,822 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,822 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,822 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:52,823 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:52,823 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:52,824 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-12-03 13:44:52,841 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:52,841 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:52,842 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:52,842 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:52,853 INFO   ||  Awaiting socket connections on localhost:38963.   [kafka.network.Acceptor]
2018-12-03 13:44:52,854 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:52,854 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,854 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,854 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,855 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:52,856 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:52,856 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,856 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:52,864 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,864 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:52,864 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:52,864 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:52,865 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:52,865 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:setData cxid:0x2a zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,865 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:52,865 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:52,865 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:52,880 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:52,888 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:52,889 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:52,889 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:52,890 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,891 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:52,892 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:52,892 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:52,892 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,893 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:52,893 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,893 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,896 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:52,897 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,905 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:52,905 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,38963,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:52,905 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:52,906 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:52,906 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:52,907 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:52,907 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:38963 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:52,939 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:52,939 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:52,939 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:52,939 INFO   ||  Started Kafka server 1 at localhost:38963 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:52,941 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topicA Error:KeeperErrorCode = NoNode for /config/topics/topicA   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,947 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,955 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,965 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:52,971 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:52,980 INFO   ||  [Controller id=1] New topics: [Set(topicA)], deleted topics: [Set()], new partition replica assignment [Map(topicA-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:52,980 INFO   ||  [Controller id=1] New topic creation callback for topicA-0   [kafka.controller.KafkaController]
2018-12-03 13:44:52,980 INFO   ||  [Controller id=1] New partition creation callback for topicA-0   [kafka.controller.KafkaController]
2018-12-03 13:44:52,980 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:52,981 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:52,981 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:52,981 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,988 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:38963]
	buffer.memory = 33554432
	client.id = manual
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:52,988 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38963]
	check.crcs = true
	client.id = f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:52,988 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:52,990 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:52,990 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:52,990 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:52,990 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:53,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:53,013 WARN   ||  [Producer clientId=manual] Error while fetching metadata with correlation id 1 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,013 WARN   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] Error while fetching metadata with correlation id 2 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,022 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:53,022 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions topicA-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:53,022 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:setData cxid:0x6e zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,023 INFO   ||  Loading producer state from offset 0 for partition topicA-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:53,023 INFO   ||  Completed load of log topicA-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:53,024 INFO   ||  Created log for partition [topicA,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:53,024 INFO   ||  [Partition topicA-0 broker=1] No checkpointed highwatermark is found for partition topicA-0   [kafka.cluster.Partition]
2018-12-03 13:44:53,024 INFO   ||  Replica loaded for partition topicA-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:53,024 INFO   ||  [Partition topicA-0 broker=1] topicA-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:53,030 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x6f zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,065 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,065 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,088 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:53,115 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,115 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,115 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:53,116 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: topicA-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:53,116 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:53,116 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:53,116 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:53,116 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:53,116 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:53,117 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:53,117 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x77 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,155 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632af90000 type:create cxid:0x78 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,156 INFO   ||  [Producer clientId=manual] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:53,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,188 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:53,189 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:53,190 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:53,190 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:53,190 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:53,191 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:53,191 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:53,191 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:53,191 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:53,191 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:53,215 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,215 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,216 INFO   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] Discovered group coordinator localhost:38963 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:53,216 INFO   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:53,217 INFO   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:53,217 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153 with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,218 INFO   ||  [GroupCoordinator 1]: Stabilized group f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153 generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,218 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153 for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,219 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:53,249 INFO   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:53,249 INFO   ||  [Consumer clientId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153, groupId=f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153] Setting newly assigned partitions [topicA-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:53,252 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153 with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,252 INFO   ||  [GroupCoordinator 1]: Group f216ba9c-0d8d-4fdf-a1a1-35a65ab1a153 with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
The consumer completed normally in   0.27166s total;   1 samples;  0.27166s avg;  0.27166s min;  0.27166s max
2018-12-03 13:44:53,256 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:53,256 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:53,257 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:53,257 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:53,258 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:53,258 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:53,258 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:53,259 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:53,259 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:53,259 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,265 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,314 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,314 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,314 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,315 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,456 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,456 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,456 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:53,456 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:53,456 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:53,456 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:53,456 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:53,456 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:53,456 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:53,456 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,456 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,464 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,464 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,464 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,466 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,515 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,618 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,618 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,618 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:53,618 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:53,618 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:53,618 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:53,618 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:53,618 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:53,618 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:53,618 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,636 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,636 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,636 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,654 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,654 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,654 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,654 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,654 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:53,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,683 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:53,683 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:53,683 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:53,683 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:53,683 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:53,683 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:53,784 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:53,784 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:53,784 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:53,784 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:53,784 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:53,784 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:53,784 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:53,784 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:53,784 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:53,785 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:53,785 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:53,785 INFO   ||  Processed session termination for sessionid: 0x16775632af90000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,796 INFO   ||  Session: 0x16775632af90000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:53,796 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,796 INFO   ||  EventThread shut down for session: 0x16775632af90000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:53,797 INFO   ||  Closed socket connection for client /127.0.0.1:34160 which had sessionid 0x16775632af90000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:53,815 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,822 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,823 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,823 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:53,823 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:53,829 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:53,829 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:53,829 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:53,829 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:53,830 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:53,830 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:53,830 INFO   ||  Stopped Kafka server 1 at localhost:38963   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:53,830 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:53,830 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,830 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,830 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:53,830 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,830 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:53,830 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,830 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:53,830 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowProducersAndConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithMultipleBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowAsynchronousProductionAndAutomaticConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
2018-12-03 13:44:53,833 INFO   ||  binding to port localhost/127.0.0.1:43185   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:53,833 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,834 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,834 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 44755
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:43185
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-12-03 13:44:53,835 INFO   ||  starting   [kafka.server.KafkaServer]
2018-12-03 13:44:53,835 INFO   ||  Connecting to zookeeper on localhost:43185   [kafka.server.KafkaServer]
2018-12-03 13:44:53,835 INFO   ||  Initiating client connection, connectString=localhost:43185 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5cc5ee25   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:53,835 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:53,835 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:53,836 INFO   ||  Opening socket connection to server localhost/127.0.0.1:43185. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:53,836 INFO   ||  Socket connection established to localhost/127.0.0.1:43185, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:53,836 INFO   ||  Accepted socket connection from /127.0.0.1:35406   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:53,836 INFO   ||  Client attempting to establish new session at /127.0.0.1:35406   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,836 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-12-03 13:44:53,849 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,877 INFO   ||  Established session 0x16775632fc90000 with negotiated timeout 6000 for client /127.0.0.1:35406   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:53,877 INFO   ||  Session establishment complete on server localhost/127.0.0.1:43185, sessionid = 0x16775632fc90000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:53,877 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-12-03 13:44:53,894 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,916 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,916 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,927 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:53,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:53,969 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:54,016 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,016 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,027 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,048 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,052 INFO   ||  Cluster ID = 9P2bdCR2Qdy1vpPpk3HrGg   [kafka.server.KafkaServer]
2018-12-03 13:44:54,052 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:54,053 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:54,053 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:54,053 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:54,053 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-12-03 13:44:54,054 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-12-03 13:44:54,054 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-12-03 13:44:54,066 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,075 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-12-03 13:44:54,076 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-12-03 13:44:54,076 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-12-03 13:44:54,077 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-12-03 13:44:54,086 INFO   ||  Awaiting socket connections on localhost:44755.   [kafka.network.Acceptor]
2018-12-03 13:44:54,087 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-12-03 13:44:54,087 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,087 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,088 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,089 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:54,089 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:54,090 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,090 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,090 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,090 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:54,090 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,090 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,090 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:54,094 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:54,094 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-12-03 13:44:54,094 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-12-03 13:44:54,102 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:54,102 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,111 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:54,111 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:54,111 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:54,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,118 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-12-03 13:44:54,121 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:54,121 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x46 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,121 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x47 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,135 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-12-03 13:44:54,135 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,44755,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-12-03 13:44:54,135 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-12-03 13:44:54,135 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:54,136 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-12-03 13:44:54,136 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,136 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,166 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,166 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,185 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,185 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,185 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-12-03 13:44:54,185 INFO   ||  Started Kafka server 1 at localhost:44755 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:54,185 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-12-03 13:44:54,186 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-12-03 13:44:54,186 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topicA Error:KeeperErrorCode = NoNode for /config/topics/topicA   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,187 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-12-03 13:44:54,187 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-12-03 13:44:54,187 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:44755 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-12-03 13:44:54,194 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,210 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:54,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,219 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:44755]
	buffer.memory = 33554432
	client.id = manual
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-12-03 13:44:54,219 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:44755]
	check.crcs = true
	client.id = 92b5ae59-844b-46f5-bd99-d2df6c4e7f38
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 92b5ae59-844b-46f5-bd99-d2df6c4e7f38
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-12-03 13:44:54,219 INFO   ||  [Controller id=1] New topics: [Set(topicA)], deleted topics: [Set()], new partition replica assignment [Map(topicA-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:54,219 INFO   ||  [Controller id=1] New topic creation callback for topicA-0   [kafka.controller.KafkaController]
2018-12-03 13:44:54,219 INFO   ||  [Controller id=1] New partition creation callback for topicA-0   [kafka.controller.KafkaController]
2018-12-03 13:44:54,220 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,220 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:54,220 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,220 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,220 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,220 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,220 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-12-03 13:44:54,220 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,227 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x61 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,244 WARN   ||  [Producer clientId=manual] Error while fetching metadata with correlation id 1 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,244 WARN   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] Error while fetching metadata with correlation id 2 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,260 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:54,261 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions topicA-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:54,261 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:setData cxid:0x6e zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,262 INFO   ||  Loading producer state from offset 0 for partition topicA-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:54,262 INFO   ||  Completed load of log topicA-0 with 1 log segments, log start offset 0 and log end offset 0 in 0 ms   [kafka.log.Log]
2018-12-03 13:44:54,263 INFO   ||  Created log for partition [topicA,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:54,263 INFO   ||  [Partition topicA-0 broker=1] No checkpointed highwatermark is found for partition topicA-0   [kafka.cluster.Partition]
2018-12-03 13:44:54,263 INFO   ||  Replica loaded for partition topicA-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:54,263 INFO   ||  [Partition topicA-0 broker=1] topicA-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:54,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,269 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x6f zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,317 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,327 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-12-03 13:44:54,346 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: topicA-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:54,353 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-12-03 13:44:54,354 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-12-03 13:44:54,354 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:54,354 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-12-03 13:44:54,354 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,354 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:54,354 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:54,354 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x77 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,387 INFO   ||  Got user-level KeeperException when processing sessionid:0x16775632fc90000 type:create cxid:0x78 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:54,388 INFO   ||  [Producer clientId=manual] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-12-03 13:44:54,427 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:54,427 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:54,429 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-12-03 13:44:54,429 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-12-03 13:44:54,429 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-12-03 13:44:54,429 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-12-03 13:44:54,429 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-12-03 13:44:54,429 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-12-03 13:44:54,429 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:54,430 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-12-03 13:44:54,446 INFO   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] Discovered group coordinator localhost:44755 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:54,446 INFO   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:54,446 INFO   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:54,447 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 92b5ae59-844b-46f5-bd99-d2df6c4e7f38 with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,448 INFO   ||  [GroupCoordinator 1]: Stabilized group 92b5ae59-844b-46f5-bd99-d2df6c4e7f38 generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,448 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group 92b5ae59-844b-46f5-bd99-d2df6c4e7f38 for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,449 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-12-03 13:44:54,466 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,480 INFO   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-12-03 13:44:54,480 INFO   ||  [Consumer clientId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38, groupId=92b5ae59-844b-46f5-bd99-d2df6c4e7f38] Setting newly assigned partitions [topicA-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-12-03 13:44:54,482 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 92b5ae59-844b-46f5-bd99-d2df6c4e7f38 with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,482 INFO   ||  [GroupCoordinator 1]: Group 92b5ae59-844b-46f5-bd99-d2df6c4e7f38 with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
The consumer completed normally in   0.26502s total;   1 samples;  0.26502s avg;  0.26502s min;  0.26502s max
2018-12-03 13:44:54,484 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-12-03 13:44:54,484 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-12-03 13:44:54,485 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-12-03 13:44:54,485 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-12-03 13:44:54,486 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:54,487 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-12-03 13:44:54,487 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:54,487 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-12-03 13:44:54,487 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-12-03 13:44:54,487 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,490 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,490 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,490 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:54,490 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-12-03 13:44:54,490 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-12-03 13:44:54,490 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:54,490 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:54,490 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-12-03 13:44:54,490 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-12-03 13:44:54,490 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,490 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,690 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,690 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,690 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,848 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,848 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,848 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-12-03 13:44:54,848 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-12-03 13:44:54,848 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:54,848 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:54,848 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-12-03 13:44:54,848 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:54,848 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-12-03 13:44:54,848 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,900 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,917 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:54,977 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,977 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:54,977 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,017 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,051 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,088 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,088 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,088 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,088 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,088 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-12-03 13:44:55,113 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-12-03 13:44:55,113 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-12-03 13:44:55,114 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-12-03 13:44:55,114 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-12-03 13:44:55,114 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-12-03 13:44:55,114 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-12-03 13:44:55,118 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,118 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,218 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,218 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,232 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-12-03 13:44:55,232 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:55,232 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:55,232 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-12-03 13:44:55,232 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-12-03 13:44:55,232 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-12-03 13:44:55,232 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-12-03 13:44:55,232 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-12-03 13:44:55,232 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-12-03 13:44:55,232 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-12-03 13:44:55,232 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-12-03 13:44:55,233 INFO   ||  Processed session termination for sessionid: 0x16775632fc90000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:55,244 INFO   ||  Session: 0x16775632fc90000 closed   [org.apache.zookeeper.ZooKeeper]
2018-12-03 13:44:55,244 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:55,244 INFO   ||  EventThread shut down for session: 0x16775632fc90000   [org.apache.zookeeper.ClientCnxn]
2018-12-03 13:44:55,244 INFO   ||  Closed socket connection for client /127.0.0.1:35406 which had sessionid 0x16775632fc90000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-12-03 13:44:55,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,268 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,318 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,319 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,469 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,769 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,869 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:55,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,019 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,020 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-12-03 13:44:56,053 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-12-03 13:44:56,059 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-12-03 13:44:56,060 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-12-03 13:44:56,060 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:56,060 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:56,060 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-12-03 13:44:56,061 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-12-03 13:44:56,061 INFO   ||  Stopped Kafka server 1 at localhost:44755   [io.debezium.kafka.KafkaServer]
2018-12-03 13:44:56,061 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-12-03 13:44:56,061 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:56,061 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-12-03 13:44:56,061 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:56,061 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:56,061 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:56,061 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-12-03 13:44:56,061 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-12-03 13:44:56,061 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowProducersAndConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
13:44:56.063 [pool-58-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (45402 ms)
<> Total tests run: 34
<> Ignored tests: 0
<> Failed tests: 0

13:44:56.064 [pool-2-thread-1] ERROR fr.inria.lille.repair.nopol.NoPol - Error ExecutionException java.util.concurrent.ExecutionException: java.lang.RuntimeException: failingTestCasesValidated: nothing to repair, no failing test cases
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #657
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:48 which is executed by 5 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:48
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #658
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:47 which is executed by 5 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:47
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #659
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:396 which is executed by 20 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:396
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #660
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:395 which is executed by 20 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:395
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #661
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:189 which is executed by 20 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:189
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #662
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:157 which is executed by 20 tests
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:157
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #663
13:44:56.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Debug:84 which is executed by 3 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Debug:84
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #664
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Debug:83 which is executed by 3 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Debug:83
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #665
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:318 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:318
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #666
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:315 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:315
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #667
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:310 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:310
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #668
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:309 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:309
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #669
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:308 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:308
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #670
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:307 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:307
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #671
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:293 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:293
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #672
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:292 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:292
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #673
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:283 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:283
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #674
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:282 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:282
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #675
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:278 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:278
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #676
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:232 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:232
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #677
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:209 which is executed by 22 tests
13:44:56.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:209
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #678
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:208 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:208
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #679
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:206 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:206
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #680
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:205 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:205
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #681
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:182 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:182
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #682
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:163 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:163
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #683
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:161 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:161
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #684
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:160 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:160
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #685
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:152 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:152
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #686
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:148 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:148
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #687
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:23 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:23
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #688
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:26 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:26
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #689
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:57 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:57
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #690
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:51 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:51
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #691
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:34 which is executed by 22 tests
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:34
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #692
13:44:56.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:33 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:33
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #693
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:29 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:29
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #694
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:28 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:28
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #695
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:98 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:98
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #696
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:92 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:92
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #697
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:85 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:85
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #698
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:83 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:83
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #699
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:80 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:80
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #700
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:79 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:79
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #701
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:77 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:77
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #702
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:57 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:57
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #703
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:56 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:56
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #704
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:53 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:53
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #705
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:52 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:52
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #706
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:44 which is executed by 22 tests
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:44
13:44:56.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #707
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:40 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:40
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #708
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:442 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:442
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #709
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:441 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:441
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #710
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:440 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:440
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #711
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:439 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:439
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #712
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:274 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:274
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #713
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:272 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:272
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #714
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:271 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:271
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #715
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:268 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:268
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #716
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:265 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:265
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #717
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:260 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:260
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #718
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:259 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:259
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #719
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:258 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:258
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #720
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:257 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:257
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #721
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:51 which is executed by 22 tests
13:44:56.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:51
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #722
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:49 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:49
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #723
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:48 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:48
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #724
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:47 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:47
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #725
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:46 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:46
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #726
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:45 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:45
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #727
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:44 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:44
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #728
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:37 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:37
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #729
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:36 which is executed by 22 tests
2018-12-03 13:44:56,069 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,069 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:36
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #730
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:43 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:43
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #731
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:42 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:42
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #732
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:71 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:71
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #733
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:62 which is executed by 22 tests
13:44:56.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:62
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #734
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:56 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:56
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #735
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:54 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:54
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #736
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:48 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:48
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #737
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:45 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:45
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #738
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:44 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:44
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #739
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:41 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:41
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #740
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:40 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:40
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #741
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:39 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:39
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #742
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:37 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:37
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #743
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:188 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:188
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #744
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:184 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:184
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #745
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:182 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:182
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #746
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:156 which is executed by 22 tests
13:44:56.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:156
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #747
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:220 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:220
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #748
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:219 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:219
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #749
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:213 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:213
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #750
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:212 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:212
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #751
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:211 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:211
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #752
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:206 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:206
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #753
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:205 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:205
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #754
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:200 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:200
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #755
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:189 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:189
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #756
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:188 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:188
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #757
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:104 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:104
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #758
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:103 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:103
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #759
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:99 which is executed by 22 tests
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:99
13:44:56.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #760
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:137 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:137
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #761
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:117 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:117
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #762
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:97 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:97
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #763
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:96 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:96
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #764
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:95 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:95
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #765
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:74 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:74
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #766
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:72 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:72
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #767
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:67 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:67
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #768
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:74 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:74
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #769
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:70 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:70
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #770
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:66 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:66
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #771
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:162 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:162
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #772
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:161 which is executed by 22 tests
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:161
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #773
13:44:56.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:51 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:51
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #774
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:49 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:49
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #775
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:48 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:48
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #776
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:47 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:47
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #777
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:46 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:46
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #778
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:122 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:122
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #779
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:120 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:120
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #780
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:119 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:119
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #781
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:118 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:118
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #782
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:117 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:117
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #783
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:116 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:116
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #784
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:189 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:189
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #785
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormatInfo:114 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormatInfo:114
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #786
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormatInfo:106 which is executed by 22 tests
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormatInfo:106
13:44:56.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #787
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:138 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:138
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #788
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:137 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:137
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #789
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:136 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:136
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #790
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:135 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:135
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #791
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:134 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:134
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #792
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:122 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:122
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #793
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:121 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:121
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #794
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LevelConverter:23 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LevelConverter:23
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #795
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LevelConverter:26 which is executed by 22 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LevelConverter:26
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #796
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:316 which is executed by 17 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:316
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #797
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.MessageConverter:23 which is executed by 23 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.MessageConverter:23
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #798
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.MessageConverter:26 which is executed by 23 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.MessageConverter:26
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #799
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:59 which is executed by 23 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:59
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #800
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:168 which is executed by 23 tests
13:44:56.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:168
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #801
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:165 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:165
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #802
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:102 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:102
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #803
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.LayoutBase:44 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.LayoutBase:44
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #804
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:781 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:781
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #805
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:215 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:215
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #806
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.LiteralConverter:25 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.LiteralConverter:25
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #807
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LoggerConverter:21 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LoggerConverter:21
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #808
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:23 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:23
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #809
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:63 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:63
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #810
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:62 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:62
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #811
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:18 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:18
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #812
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:53 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:53
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #813
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:50 which is executed by 23 tests
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:50
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #814
13:44:56.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:48 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:48
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #815
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:208 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:208
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #816
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:206 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:206
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #817
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:203 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:203
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #818
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:19 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:19
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #819
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:22 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:22
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #820
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Level:99 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Level:99
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #821
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:53 which is executed by 23 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:53
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #822
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:35 which is executed by 24 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:35
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #823
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:101 which is executed by 24 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:101
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #824
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:100 which is executed by 24 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:100
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #825
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:69 which is executed by 21 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:69
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #826
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDateTimeCodec:28 which is executed by 2 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDateTimeCodec:28
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #827
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDateTimeCodec:31 which is executed by 2 tests
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDateTimeCodec:31
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #828
13:44:56.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:61 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:61
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #829
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:1099 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:1099
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #830
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:925 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:925
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #831
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:924 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:924
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #832
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:922 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:922
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #833
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:921 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:921
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #834
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:920 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:920
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #835
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:620 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:620
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #836
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:619 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:619
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #837
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:618 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:618
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #838
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:612 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:612
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #839
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:608 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:608
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #840
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:604 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:604
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #841
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:600 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:600
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #842
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:596 which is executed by 2 tests
13:44:56.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:596
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #843
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:592 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:592
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #844
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:588 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:588
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #845
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:584 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:584
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #846
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:580 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:580
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #847
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:579 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:579
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #848
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:578 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:578
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #849
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:577 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:577
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #850
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:576 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:576
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #851
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:573 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:573
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #852
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:569 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:569
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #853
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:568 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:568
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #854
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:566 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:566
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #855
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:565 which is executed by 2 tests
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:565
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #856
13:44:56.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:564 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:564
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #857
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:498 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:498
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #858
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:495 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:495
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #859
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:494 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:494
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #860
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:491 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:491
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #861
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:488 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:488
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #862
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:484 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:484
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #863
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:483 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:483
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #864
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:478 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:478
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #865
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:476 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:476
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #866
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:475 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:475
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #867
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:474 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:474
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #868
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:473 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:473
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #869
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:369 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:369
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #870
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:365 which is executed by 2 tests
13:44:56.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:365
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #871
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:364 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:364
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #872
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:360 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:360
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #873
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:359 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:359
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #874
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:312 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:312
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #875
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:308 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:308
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #876
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:305 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:305
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #877
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:304 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:304
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #878
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:303 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:303
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #879
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:302 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:302
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #880
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:298 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:298
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #881
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:293 which is executed by 2 tests
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:293
13:44:56.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #882
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:292 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:292
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #883
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:288 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:288
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #884
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:284 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:284
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #885
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:283 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:283
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #886
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:282 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:282
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #887
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:280 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:280
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #888
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:275 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:275
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #889
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:270 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:270
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #890
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:259 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:259
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #891
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:256 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:256
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #892
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:252 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:252
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #893
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:251 which is executed by 2 tests
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:251
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #894
13:44:56.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:247 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:247
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #895
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:243 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:243
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #896
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:242 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:242
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #897
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:241 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:241
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #898
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:240 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:240
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #899
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:236 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:236
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #900
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:169 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:169
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #901
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:168 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:168
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #902
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:167 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:167
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #903
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:157 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:157
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #904
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:156 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:156
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #905
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:155 which is executed by 2 tests
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:155
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #906
13:44:56.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:150 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:150
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #907
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:149 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:149
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #908
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:148 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:148
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #909
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:146 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:146
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #910
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:145 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:145
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #911
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:143 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:143
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #912
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:142 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:142
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #913
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:140 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:140
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #914
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:139 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:139
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #915
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:136 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:136
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #916
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:135 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:135
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #917
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:134 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:134
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #918
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:133 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:133
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #919
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:128 which is executed by 2 tests
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:128
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #920
13:44:56.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:127 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:127
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #921
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:122 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:122
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #922
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:121 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:121
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #923
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:119 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:119
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #924
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:118 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:118
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #925
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:115 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:115
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #926
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:114 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:114
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #927
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:113 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:113
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #928
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:109 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:109
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #929
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:107 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:107
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #930
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:105 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:105
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #931
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:102 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:102
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #932
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonTypeCodecMap:56 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonTypeCodecMap:56
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #933
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader$Context:843 which is executed by 2 tests
13:44:56.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader$Context:843
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #934
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader$Context:834 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader$Context:834
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #935
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.Optional$Some:77 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.Optional$Some:77
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #936
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.Optional$Some:72 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.Optional$Some:72
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #937
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:66 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:66
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #938
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:63 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:63
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #939
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:59 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:59
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #940
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:58 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:58
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #941
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:57 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:57
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #942
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:51 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:51
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #943
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:48 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:48
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #944
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:43 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:43
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #945
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.assertions.Assertions:37 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.assertions.Assertions:37
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #946
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.assertions.Assertions:34 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.assertions.Assertions:34
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #947
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDoubleCodec:28 which is executed by 2 tests
13:44:56.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDoubleCodec:28
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #948
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDoubleCodec:31 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDoubleCodec:31
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #949
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.CodecCache:43 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.CodecCache:43
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #950
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.CodecCache:42 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.CodecCache:42
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #951
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.CodecCache:41 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.CodecCache:41
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #952
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.CodecCache:40 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.CodecCache:40
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #953
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.CodecCache:31 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.CodecCache:31
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #954
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonType:137 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonType:137
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #955
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:775 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:775
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #956
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:771 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:771
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #957
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:770 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:770
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #958
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:768 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:768
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #959
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:767 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:767
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #960
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:764 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:764
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #961
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:755 which is executed by 2 tests
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:755
13:44:56.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #962
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:751 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:751
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #963
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:743 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:743
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #964
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:742 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:742
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #965
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:733 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:733
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #966
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:725 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:725
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #967
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:724 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:724
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #968
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:720 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:720
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #969
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:696 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:696
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #970
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:691 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:691
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #971
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:688 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:688
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #972
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:684 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:684
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #973
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:682 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:682
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #974
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:681 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:681
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #975
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:587 which is executed by 2 tests
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:587
13:44:56.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #976
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:586 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:586
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #977
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:582 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:582
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #978
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:579 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:579
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #979
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:461 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:461
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #980
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:460 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:460
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #981
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:459 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:459
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #982
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:455 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:455
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #983
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:454 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:454
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #984
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:453 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:453
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #985
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:452 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:452
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #986
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:448 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:448
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #987
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:447 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:447
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #988
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:446 which is executed by 2 tests
13:44:56.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:446
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #989
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:445 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:445
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #990
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:369 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:369
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #991
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:368 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:368
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #992
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:367 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:367
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #993
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:363 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:363
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #994
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:362 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:362
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #995
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:360 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:360
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #996
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:356 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:356
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #997
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:353 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:353
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #998
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:349 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:349
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #999
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:346 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:346
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1000
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:342 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:342
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1001
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:341 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:341
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1002
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:339 which is executed by 2 tests
13:44:56.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:339
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1003
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:335 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:335
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1004
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:332 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:332
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1005
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:329 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:329
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1006
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:326 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:326
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1007
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:321 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:321
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1008
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:320 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:320
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1009
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:319 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:319
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1010
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:314 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:314
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1011
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:313 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:313
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1012
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:312 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:312
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1013
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:106 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:106
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1014
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:91 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:91
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1015
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:90 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:90
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1016
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:82 which is executed by 2 tests
13:44:56.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:82
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1017
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:81 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:81
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1018
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:72 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:72
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1019
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:66 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:66
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1020
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:65 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:65
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1021
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:48 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:48
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1022
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:530 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:530
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1023
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:527 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:527
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1024
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:524 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:524
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1025
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:523 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:523
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1026
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:521 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:521
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1027
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:520 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:520
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1028
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:473 which is executed by 2 tests
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:473
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1029
13:44:56.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:472 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:472
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1030
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:469 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:469
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1031
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:451 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:451
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1032
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:449 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:449
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1033
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:448 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:448
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1034
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:447 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:447
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1035
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:445 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:445
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1036
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:444 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:444
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1037
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:443 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:443
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1038
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:442 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:442
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1039
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:438 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:438
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1040
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:354 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:354
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1041
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:353 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:353
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1042
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:352 which is executed by 2 tests
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:352
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1043
13:44:56.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:351 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:351
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1044
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:349 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:349
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1045
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:348 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:348
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1046
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:338 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:338
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1047
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:332 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:332
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1048
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:331 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:331
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1049
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:330 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:330
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1050
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:322 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:322
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1051
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:321 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:321
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1052
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:320 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:320
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1053
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:319 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:319
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1054
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:306 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:306
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1055
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:305 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:305
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1056
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:303 which is executed by 2 tests
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:303
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1057
13:44:56.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:268 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:268
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1058
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:267 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:267
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1059
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:259 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:259
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1060
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:257 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:257
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1061
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:256 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:256
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1062
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:252 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:252
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1063
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:248 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:248
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1064
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:243 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:243
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1065
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:242 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:242
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1066
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:240 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:240
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1067
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:236 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:236
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1068
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:234 which is executed by 2 tests
13:44:56.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:234
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1069
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:103 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:103
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1070
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:102 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:102
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1071
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:98 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:98
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1072
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:95 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:95
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1073
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:93 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:93
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1074
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:87 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:87
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1075
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:85 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:85
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1076
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:83 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:83
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1077
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:81 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:81
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1078
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:79 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:79
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1079
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:75 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:75
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1080
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:73 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:73
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1081
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:72 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:72
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1082
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:71 which is executed by 2 tests
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:71
13:44:56.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1083
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:66 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:66
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1084
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:59 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:59
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1085
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:46 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:46
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1086
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:43 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:43
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1087
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:41 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:41
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1088
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:40 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:40
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1089
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:36 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:36
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1090
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.DecoderContext$Builder:47 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.DecoderContext$Builder:47
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1091
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:37 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:37
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1092
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:102 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:102
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1093
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:72 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:72
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1094
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:70 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:70
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1095
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:67 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:67
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1096
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:66 which is executed by 2 tests
13:44:56.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:66
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1097
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:65 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:65
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1098
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:63 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:63
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1099
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader$Context:1133 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader$Context:1133
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1100
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader$Context:1129 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader$Context:1129
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1101
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonStringCodec:28 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonStringCodec:28
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1102
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonStringCodec:31 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonStringCodec:31
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1103
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:41 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:41
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1104
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:101 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:101
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1105
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:89 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:89
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1106
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:87 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:87
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1107
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:85 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:85
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1108
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:84 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:84
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1109
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:83 which is executed by 2 tests
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:83
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1110
13:44:56.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:82 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:82
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1111
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:81 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:81
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1112
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:79 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:79
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1113
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonElement:55 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonElement:55
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1114
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonElement:46 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonElement:46
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1115
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:63 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:63
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1116
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:53 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:53
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1117
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:37 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ProvidersCodecRegistry:37
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1118
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonValueCodecProvider:84 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonValueCodecProvider:84
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1119
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonValueCodecProvider:74 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonValueCodecProvider:74
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1120
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonInt32Codec:28 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonInt32Codec:28
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1121
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonInt32Codec:31 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonInt32Codec:31
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1122
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.DecoderContext:32 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.DecoderContext:32
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1123
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:60 which is executed by 2 tests
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:60
13:44:56.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1124
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:53 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:53
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1125
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:51 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:51
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1126
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:50 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:50
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1127
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:49 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:49
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1128
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:44 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:44
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1129
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:40 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:40
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1130
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:38 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:38
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1131
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:30 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:30
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1132
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonDocument:727 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonDocument:727
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1133
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonDocument:723 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonDocument:723
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1134
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonDocument:720 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonDocument:720
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1135
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonDocument:62 which is executed by 2 tests
13:44:56.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonDocument:62
13:44:56.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1136
13:44:56.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.jdbc.TemporalPrecisionMode:41 which is executed by 2 tests
2018-12-03 13:44:56,102 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,102 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,270 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,319 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,319 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-173296864
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1137
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:54 which is executed by 23 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:54
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1138
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:44 which is executed by 23 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:44
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1139
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:43 which is executed by 23 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:43
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1140
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:41 which is executed by 23 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:41
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1141
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:44 which is executed by 3 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:44
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1142
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:32 which is executed by 3 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:32
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1143
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:31 which is executed by 3 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:31
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1144
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonTypeClassMap:103 which is executed by 3 tests
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonTypeClassMap:103
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1145
13:44:56.339 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonTypeClassMap:93 which is executed by 3 tests
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonTypeClassMap:93
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1146
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:250 which is executed by 96 tests
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:250
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1147
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:249 which is executed by 96 tests
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:249
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1148
13:44:56.340 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.data.VariableScaleDecimal:50 which is executed by 1 tests
2018-12-03 13:44:56,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,420 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-1999701148
13:44:56.438 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1149
13:44:56.438 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.data.VariableScaleDecimal:35 which is executed by 1 tests
2018-12-03 13:44:56,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-12-03 13:44:56,519 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,520 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-1999701148
13:44:56.535 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1150
13:44:56.535 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:75 which is executed by 1 tests
2018-12-03 13:44:56,571 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,571 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,620 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,621 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:44:56.632 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1151
13:44:56.633 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:74 which is executed by 1 tests
2018-12-03 13:44:56,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:44:56.729 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:44:56,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:44:56,820 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,870 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,871 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,871 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,920 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,921 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,971 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:56,988 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:44:57,054 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,070 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,071 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,104 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,121 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,121 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,372 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,372 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,421 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,422 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,472 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,572 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,621 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,672 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,672 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,672 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,922 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:57,973 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,023 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,072 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,072 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,086 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:44:58,116 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:44:58,122 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,123 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,123 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,123 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,130 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:44:58,157 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,172 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,173 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,223 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,273 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,323 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,346 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:44:58,347 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:44:58,347 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:44:58,423 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,446 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:44:58.447 [pool-186-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1706 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:44:58,473 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,574 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,624 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,674 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:44:58,723 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,724 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:44:58.767 [pool-189-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (78 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:44:58.768 [pool-190-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:44:58.769 [pool-191-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:44:58.770 [pool-192-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:44:58.771 [pool-193-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:44:58.771 [pool-194-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:44:58.773 [pool-185-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:44:58.773 [pool-185-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:44:58.773 [pool-185-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:74.
13:44:58.773 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1152
13:44:58.773 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:73 which is executed by 1 tests
2018-12-03 13:44:58,773 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:58,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:44:58.878 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:44:58,925 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:44:59,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,075 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,124 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,125 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,125 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,158 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,162 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:44:59,175 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,225 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,274 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,325 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,325 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,375 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,375 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,425 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,475 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,626 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,626 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:44:59,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,010 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,026 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,076 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,126 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,127 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,176 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,177 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,227 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,227 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,260 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:00,276 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,287 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:00,302 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:00,326 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,326 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,327 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,658 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,661 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:00,662 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:00,662 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:00,754 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:00.755 [pool-196-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1866 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:45:00,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,808 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,858 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:00,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:00.974 [pool-199-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (52 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:00.975 [pool-200-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:00.975 [pool-201-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:00.976 [pool-202-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:00.977 [pool-203-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:00.978 [pool-204-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:45:00.979 [pool-195-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:00.979 [pool-195-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:00.979 [pool-195-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:73.
13:45:00.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1153
13:45:00.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:72 which is executed by 1 tests
2018-12-03 13:45:01,059 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:45:01.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:01,109 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:45:01,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,209 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,209 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,334 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:01,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,561 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,610 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,610 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,610 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,860 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,910 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,960 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:01,960 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,060 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,060 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,110 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,160 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,432 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:02,459 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:02,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,474 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:02,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,561 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,611 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,611 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,612 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,658 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:02,658 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:02,659 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:02,661 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,661 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,661 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,711 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,746 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:02.747 [pool-206-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1662 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:45:02,761 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,812 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,912 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:02,912 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:02,962 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:02.968 [pool-209-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (48 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:02.969 [pool-210-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:02.970 [pool-211-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:02.971 [pool-212-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:02.971 [pool-213-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:02.973 [pool-214-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:45:02.974 [pool-205-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:02.974 [pool-205-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:02.974 [pool-205-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:72.
13:45:02.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1154
13:45:02.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:71 which is executed by 1 tests
2018-12-03 13:45:03,012 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:45:03.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1155
13:45:03.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:70 which is executed by 1 tests
2018-12-03 13:45:03,112 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:45:03.167 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:45:03,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,263 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,413 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,422 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:03,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,514 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,813 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,913 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,913 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,913 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:03,963 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,114 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,164 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,214 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,214 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,214 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,514 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,519 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:04,546 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:04,559 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:04,564 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,564 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,614 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,665 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,715 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,742 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:04,742 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:04,742 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:04,815 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,815 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,833 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:04.834 [pool-216-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1656 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:45:04,915 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,915 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:04,915 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:05.040 [pool-219-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (48 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:05.041 [pool-220-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:05.042 [pool-221-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:05.043 [pool-222-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:05.043 [pool-223-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:05.044 [pool-224-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:45:05.045 [pool-215-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:05.045 [pool-215-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:05.045 [pool-215-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:70.
13:45:05.045 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1156
13:45:05.045 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:69 which is executed by 1 tests
2018-12-03 13:45:05,066 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,115 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
13:45:05.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-12-03 13:45:05,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:45:05,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,266 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,266 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,401 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:05,416 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,416 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,416 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,716 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,716 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,816 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,866 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:05,917 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,017 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,067 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,067 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,067 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,117 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,217 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,417 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,499 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:06,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,518 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,518 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,518 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,525 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:06,538 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:06,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,722 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:06,723 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:06,723 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:06,768 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,768 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,818 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,818 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,818 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,835 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:06.835 [pool-226-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1680 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:45:06,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:06,969 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:07,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:07.044 [pool-229-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (48 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:07.045 [pool-230-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:07.046 [pool-231-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:07.047 [pool-232-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:07.048 [pool-233-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:07.048 [pool-234-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:45:07.050 [pool-225-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:07.050 [pool-225-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:07.050 [pool-225-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:69.
13:45:07.050 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1157
13:45:07.050 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:68 which is executed by 1 tests
-936045199
13:45:07.145 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:07,169 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-12-03 13:45:07,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,269 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,269 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,269 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,400 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:07,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,469 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,520 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,670 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,670 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,820 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,820 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,870 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,970 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:07,970 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,020 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,070 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,120 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,220 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,271 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,321 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,421 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,471 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,471 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,498 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:08,521 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,525 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-12-03 13:45:08,538 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:08,571 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,621 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,722 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:08,722 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-12-03 13:45:08,722 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-12-03 13:45:08,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,815 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:08.815 [pool-236-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1659 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-12-03 13:45:08,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:08,972 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-12-03 13:45:09,022 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:09.023 [pool-239-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (51 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:09.024 [pool-240-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:09.025 [pool-241-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
13:45:09.026 [pool-242-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
13:45:09.026 [pool-243-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
13:45:09.027 [pool-244-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

13:45:09.028 [pool-235-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:09.028 [pool-235-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
13:45:09.028 [pool-235-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:68.
13:45:09.028 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1158
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:188 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:188
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1159
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:184 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:184
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1160
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:117 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:117
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1161
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:116 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:116
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1162
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:112 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:112
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1163
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:109 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:109
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1164
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:187 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:187
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1165
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:186 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:186
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1166
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:185 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:185
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1167
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:184 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:184
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1168
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:111 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:111
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1169
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:109 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:109
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1170
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:108 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:108
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1171
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:107 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:107
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1172
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:106 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:106
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1173
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:105 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:105
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1174
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:103 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:103
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1175
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:102 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:102
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1176
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:98 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:98
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1177
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:83 which is executed by 1 tests
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:83
13:45:09.029 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1178
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:82 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:82
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1179
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:74 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:74
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1180
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:73 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:73
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1181
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:69 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:69
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1182
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:65 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:65
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1183
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:64 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:64
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1184
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:63 which is executed by 1 tests
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:63
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1185
13:45:09.030 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.relational.ddl.DataType:28 which is executed by 1 tests
2018-12-03 13:45:09,072 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,122 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,172 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,172 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
1853714981
13:45:09.178 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1186
13:45:09.178 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.ParsingException:50 which is executed by 1 tests
2018-12-03 13:45:09,222 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2127061920
13:45:09.272 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1187
13:45:09.272 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.TokenStream:2699 which is executed by 1 tests
2018-12-03 13:45:09,322 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,322 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,372 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,373 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
175865530
2018-12-03 13:45:09,423 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,423 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:09.426 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1188
13:45:09.426 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.TokenStream:737 which is executed by 1 tests
2018-12-03 13:45:09,473 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
175865530
13:45:09.566 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-12-03 13:45:09,573 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:09,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-12-03 13:45:09,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,723 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,723 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2018-12-03 13:45:09,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,974 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:09,974 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:09.974 [pool-246-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (398 ms)
<> Total tests run: 86
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

2018-12-03 13:45:10,074 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,174 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,174 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,224 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,274 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,375 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,424 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,474 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,525 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,825 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,875 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,875 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,925 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,926 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:10,975 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,126 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,176 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13:45:11.245 [pool-247-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (83 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 0

io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
13:45:11.248 [pool-248-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

13:45:11.253 [pool-249-thread-1] INFO  f.i.l.r.n.s.ConstraintModelBuilder$1PassingListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 0

io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
13:45:11.255 [pool-250-thread-1] INFO  f.i.l.r.n.s.ConstraintModelBuilder$1PassingListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

2018-12-03 13:45:11,276 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,276 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,326 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,377 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,426 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,426 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,427 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,476 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:11.553 [pool-245-thread-1] DEBUG f.i.l.c.s.ConstraintBasedSynthesis - Operators:
2018-12-03 13:45:11,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,626 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,626 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:11.673 [pool-245-thread-1] DEBUG f.i.l.c.s.ConstraintBasedSynthesis - Successful code synthesis: msg.length()==0
13:45:11.674 [pool-245-thread-1] INFO  f.i.l.repair.nopol.patch.TestPatch - Applying patch: io.debezium.text.TokenStream:737: PRECONDITION msg.length()==0
2018-12-03 13:45:11,676 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,676 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,777 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,777 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,827 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,827 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:11,977 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,077 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,078 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,127 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,278 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,328 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,328 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,378 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,378 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,428 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,428 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,428 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,478 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,528 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,529 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,578 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,628 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:12.699 [pool-245-thread-1] INFO  f.i.l.repair.nopol.patch.TestPatch - Running test suite to check the patch "msg.length()==0" is working
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/RepairThemAll/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-12-03 13:45:12,729 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,729 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,729 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-12-03 13:45:12,781 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,781 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:12.795 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - ----INFORMATION----
2018-12-03 13:45:12,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,881 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,881 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb classes : 243
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb methods : 2200
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb Statements Analyzed : 16
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb Statements with Angelic Value Found : 1
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb inputs in SMT : 2
13:45:12.918 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb SMT level: 1
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb SMT components: [0] []
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb variables in SMT : 40
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - NoPol Execution time : 90088ms
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - 
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - ----PATCH FOUND----
13:45:12.919 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - msg.length()==0
2018-12-03 13:45:12,931 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:12.935 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb test that executes the patch: 1
13:45:12.935 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - io.debezium.text.TokenStream:737: PRECONDITION
2018-12-03 13:45:12,981 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-12-03 13:45:12,981 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
13:45:12.984 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - --- a/debezium-core/src/main/java/io/debezium/text/TokenStream.java
+++ b/debezium-core/src/main/java/io/debezium/text/TokenStream.java
@@ -736,3 +736,5 @@
                     + "': " + fragment;
-            throw new ParsingException(pos, msg);
+            if (msg.length()==0) {
+                throw new ParsingException(pos, msg);
+            }
         }

2018-12-03 13:45:13,031 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
PATCH
209.47user 7.74system 1:31.93elapsed 236%CPU (0avgtext+0avgdata 3972652maxresident)k
98592inputs+23024outputs (63major+1244768minor)pagefaults 0swaps


Node: uvb-43.sophia.grid5000.fr



Date: Mon Dec  3 13:45:13 EST 2018

