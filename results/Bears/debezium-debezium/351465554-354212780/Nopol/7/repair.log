cd /tmp/Nopol_Bears_debezium-debezium_351465554-354212780;
export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8;
TZ="America/New_York"; export TZ;
export PATH="/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/:$PATH";
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/";
time java -Xmx4048m -cp /home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar fr.inria.lille.repair.Main \
	--mode repair \
	--type pre_then_cond \
	--oracle angelic \
	--synthesis smt \
	--flocal gzoltar \
	--json \
	--solver z3 \
	--solver-path /home/tdurieux/defects4j4repair/script/../libs/z3/build/z3 \
	--complianceLevel 8 \
	--source debezium-connector-mongodb/src/main:support/checkstyle/src/main:debezium-core/src/main:debezium-assembly-descriptors/src/main:debezium-connector-postgres/src/main:debezium-connector-mysql/src/main:debezium-embedded/src/main \
	--classpath "debezium-connector-mongodb/target/classes:support/checkstyle/target/classes:debezium-core/target/classes:debezium-assembly-descriptors/target/classes:debezium-connector-postgres/target/classes:debezium-connector-mysql/target/classes:debezium-embedded/target/classes:debezium-connector-mongodb/target/test-classes:debezium-core/target/test-classes:debezium-connector-postgres/target/test-classes:debezium-connector-mysql/target/test-classes:debezium-embedded/target/test-classes:/home/tdurieux/.m2/repository/com/puppycrawl/tools/checkstyle/6.7/checkstyle-6.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/tdurieux/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/home/tdurieux/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/tdurieux/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.3/commons-beanutils-core-1.8.3.jar:/home/tdurieux/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/home/tdurieux/.m2/repository/commons-cli/commons-cli/1.3/commons-cli-1.3.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/18.0/guava-18.0.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/postgresql/postgresql/42.0.0/postgresql-42.0.0.jar:/home/tdurieux/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/com/github/shyiko/mysql-binlog-connector-java/0.13.0/mysql-binlog-connector-java-0.13.0.jar:/home/tdurieux/.m2/repository/mil/nga/wkb/1.0.2/wkb-1.0.2.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka_2.11/1.0.1/kafka_2.11-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/home/tdurieux/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/tdurieux/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/org/apache/curator/curator-test/2.11.0/curator-test-2.11.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/16.0.1/guava-16.0.1.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver/3.4.2/mongodb-driver-3.4.2.jar:/home/tdurieux/.m2/repository/org/mongodb/bson/3.4.2/bson-3.4.2.jar:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver-core/3.4.2/mongodb-driver-core-3.4.2.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar";
	echo "\n\nNode: `hostname`\n";
	echo "\n\nDate: `date`\n";
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
18:36:56.654 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Source files: [debezium-connector-mongodb/src/main, support/checkstyle/src/main, debezium-core/src/main, debezium-assembly-descriptors/src/main, debezium-connector-postgres/src/main, debezium-connector-mysql/src/main, debezium-embedded/src/main]
18:36:56.666 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Classpath: [file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mongodb/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/support/checkstyle/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-core/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-assembly-descriptors/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-postgres/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mysql/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-embedded/target/classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mongodb/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-core/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-postgres/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-connector-mysql/target/test-classes/, file:/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/debezium-embedded/target/test-classes/, file:/home/tdurieux/.m2/repository/com/puppycrawl/tools/checkstyle/6.7/checkstyle-6.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar, file:/home/tdurieux/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar, file:/home/tdurieux/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar, file:/home/tdurieux/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.3/commons-beanutils-core-1.8.3.jar, file:/home/tdurieux/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar, file:/home/tdurieux/.m2/repository/commons-cli/commons-cli/1.3/commons-cli-1.3.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/18.0/guava-18.0.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/postgresql/postgresql/42.0.0/postgresql-42.0.0.jar, file:/home/tdurieux/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/github/shyiko/mysql-binlog-connector-java/0.13.0/mysql-binlog-connector-java-0.13.0.jar, file:/home/tdurieux/.m2/repository/mil/nga/wkb/1.0.2/wkb-1.0.2.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka_2.11/1.0.1/kafka_2.11-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar, file:/home/tdurieux/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar, file:/home/tdurieux/.m2/repository/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/org/apache/curator/curator-test/2.11.0/curator-test-2.11.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/16.0.1/guava-16.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.1/jackson-core-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-api/1.0.1/connect-api-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-clients/1.0.1/kafka-clients-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar, file:/home/tdurieux/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-transforms/1.0.1/connect-transforms-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar, file:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver/3.4.2/mongodb-driver-3.4.2.jar, file:/home/tdurieux/.m2/repository/org/mongodb/bson/3.4.2/bson-3.4.2.jar, file:/home/tdurieux/.m2/repository/org/mongodb/mongodb-driver-core/3.4.2/mongodb-driver-core-3.4.2.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-core/0.8.0-SNAPSHOT/debezium-core-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT-tests.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-runtime/1.0.1/connect-runtime-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-tools/1.0.1/kafka-tools-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/kafka-log4j-appender/1.0.1/kafka-log4j-appender-1.0.1.jar, file:/home/tdurieux/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.1/jackson-jaxrs-json-provider-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.1/jackson-jaxrs-base-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.1/jackson-module-jaxb-annotations-2.9.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.25.1/jersey-container-servlet-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar, file:/home/tdurieux/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar, file:/home/tdurieux/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar, file:/home/tdurieux/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, file:/home/tdurieux/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-server/9.2.22.v20170606/jetty-server-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-http/9.2.22.v20170606/jetty-http-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-io/9.2.22.v20170606/jetty-io-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlet/9.2.22.v20170606/jetty-servlet-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-security/9.2.22.v20170606/jetty-security-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-servlets/9.2.22.v20170606/jetty-servlets-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-continuation/9.2.22.v20170606/jetty-continuation-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/eclipse/jetty/jetty-util/9.2.22.v20170606/jetty-util-9.2.22.v20170606.jar, file:/home/tdurieux/.m2/repository/org/reflections/reflections/0.9.11/reflections-0.9.11.jar, file:/home/tdurieux/.m2/repository/com/google/guava/guava/20.0/guava-20.0.jar, file:/home/tdurieux/.m2/repository/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar, file:/home/tdurieux/.m2/repository/org/apache/maven/maven-artifact/3.5.0/maven-artifact-3.5.0.jar, file:/home/tdurieux/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-json/1.0.1/connect-json-1.0.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.1/jackson-databind-2.9.1.jar, file:/home/tdurieux/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.0/jackson-annotations-2.9.0.jar, file:/home/tdurieux/.m2/repository/org/apache/kafka/connect-file/1.0.1/connect-file-1.0.1.jar, file:/home/tdurieux/.m2/repository/io/debezium/debezium-embedded/0.8.0-SNAPSHOT/debezium-embedded-0.8.0-SNAPSHOT.jar, file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar, file:/home/tdurieux/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar, file:/home/tdurieux/.m2/repository/junit/junit/4.12/junit-4.12.jar, file:/home/tdurieux/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-assert/1.4/fest-assert-1.4.jar, file:/home/tdurieux/.m2/repository/org/easytesting/fest-util/1.1.6/fest-util-1.1.6.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-connect-avro-converter/4.0.0/kafka-connect-avro-converter-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, file:/home/tdurieux/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar, file:/home/tdurieux/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar, file:/home/tdurieux/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar, file:/home/tdurieux/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar, file:/home/tdurieux/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar, file:/home/tdurieux/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar, file:/home/tdurieux/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar, file:/home/tdurieux/.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar, file:/home/tdurieux/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar]
18:36:56.668 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Statement type: PRE_THEN_COND
18:36:56.668 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Args: [io.debezium.config.ConfigurationTest, io.debezium.connector.mongodb.CollectionIdTest, io.debezium.connector.mongodb.ConnectionIT, io.debezium.connector.mongodb.FiltersTest, io.debezium.connector.mongodb.ModuleTest, io.debezium.connector.mongodb.MongoClientsIT, io.debezium.connector.mongodb.MongoDbConnectorIT, io.debezium.connector.mongodb.MongoDbConnectorTest, io.debezium.connector.mongodb.MongoUtilTest, io.debezium.connector.mongodb.RecordMakersTest, io.debezium.connector.mongodb.ReplicaSetsTest, io.debezium.connector.mongodb.ReplicatorIT, io.debezium.connector.mongodb.SourceInfoTest, io.debezium.connector.mongodb.TopicSelectorTest, io.debezium.connector.mongodb.transforms.MongoDataConverterTest, io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelopeTest, io.debezium.connector.mysql.BinlogReaderBufferIT, io.debezium.connector.mysql.BinlogReaderIT, io.debezium.connector.mysql.ChainedReaderTest, io.debezium.connector.mysql.ConnectionIT, io.debezium.connector.mysql.FiltersTest, io.debezium.connector.mysql.GtidSetTest, io.debezium.connector.mysql.MetadataIT, io.debezium.connector.mysql.MySqlConnectorIT, io.debezium.connector.mysql.MySqlConnectorJsonIT, io.debezium.connector.mysql.MySqlConnectorRegressionIT, io.debezium.connector.mysql.MySqlConnectorTest, io.debezium.connector.mysql.MySqlDdlParserTest, io.debezium.connector.mysql.MySqlFixedLengthBinaryColumnIT, io.debezium.connector.mysql.MySqlGeometryIT, io.debezium.connector.mysql.MySqlGeometryTest, io.debezium.connector.mysql.MySqlSchemaTest, io.debezium.connector.mysql.MySqlTableMaintenanceStatementsIT, io.debezium.connector.mysql.MySqlTaskContextIT, io.debezium.connector.mysql.MySqlTaskContextTest, io.debezium.connector.mysql.MySqlUnsignedIntegerConverterTest, io.debezium.connector.mysql.MySqlUnsignedIntegerIT, io.debezium.connector.mysql.MySqlValueConvertersTest, io.debezium.connector.mysql.ReadBinLogIT, io.debezium.connector.mysql.SnapshotReaderIT, io.debezium.connector.mysql.SourceInfoTest, io.debezium.connector.mysql.TableConvertersTest, io.debezium.connector.postgresql.CustomTypeEncodingTest, io.debezium.connector.postgresql.PostgresConnectorIT, io.debezium.connector.postgresql.PostgresConnectorTaskIT, io.debezium.connector.postgresql.PostgresSchemaIT, io.debezium.connector.postgresql.RecordsSnapshotProducerIT, io.debezium.connector.postgresql.RecordsStreamProducerIT, io.debezium.connector.postgresql.SnapshotWithOverridesProducerIT, io.debezium.connector.postgresql.SourceInfoTest, io.debezium.connector.postgresql.connection.PostgresConnectionIT, io.debezium.connector.postgresql.connection.ReplicationConnectionIT, io.debezium.connector.postgresql.connection.wal2json.ISODateTimeFormatTest, io.debezium.connector.simple.SimpleSourceConnectorOutputTest, io.debezium.data.EnvelopeTest, io.debezium.document.ArraySerdesTest, io.debezium.document.DocumentSerdesTest, io.debezium.document.DocumentTest, io.debezium.document.JacksonArrayReadingAndWritingTest, io.debezium.document.JacksonWriterTest, io.debezium.document.PathsTest, io.debezium.embedded.EmbeddedEngineTest, io.debezium.embedded.OffsetCommitPolicyTest, io.debezium.function.BufferedBlockingConsumerTest, io.debezium.function.PredicatesTest, io.debezium.kafka.KafkaClusterTest, io.debezium.kafka.ZookeeperServerTest, io.debezium.relational.ColumnEditorTest, io.debezium.relational.SelectorsTest, io.debezium.relational.TableEditorTest, io.debezium.relational.TableIdParserTest, io.debezium.relational.TableSchemaBuilderTest, io.debezium.relational.TableTest, io.debezium.relational.ddl.DataTypeGrammarParserTest, io.debezium.relational.ddl.DataTypeParserTest, io.debezium.relational.ddl.DdlChangesTest, io.debezium.relational.ddl.DdlParserSql2003Test, io.debezium.relational.history.FileDatabaseHistoryTest, io.debezium.relational.history.HistoryRecordTest, io.debezium.relational.history.KafkaDatabaseHistoryTest, io.debezium.relational.history.MemoryDatabaseHistoryTest, io.debezium.relational.mapping.ColumnMappersTest, io.debezium.relational.mapping.MaskStringsTest, io.debezium.relational.mapping.TruncateStringsTest, io.debezium.text.PositionTest, io.debezium.text.TokenStreamTest, io.debezium.text.XmlCharactersTest, io.debezium.time.ConversionsTest, io.debezium.time.TemporalsTest, io.debezium.transforms.ByLogicalTableRouterTest, io.debezium.transforms.UnwrapFromEnvelopeTest, io.debezium.util.ElapsedTimeStrategyTest, io.debezium.util.HashCodeTest, io.debezium.util.SchemaNameAdjusterTest, io.debezium.util.StringsTest, io.debezium.util.TestingTest]
18:36:56.671 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Config: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=true, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=PRE_THEN_COND, synthesis=SMT, oracle=ANGELIC, solver=Z3, solverPath='/home/tdurieux/defects4j4repair/script/../libs/z3/build/z3', projectSources=[debezium-connector-mongodb/src/main, support/checkstyle/src/main, debezium-core/src/main, debezium-assembly-descriptors/src/main, debezium-connector-postgres/src/main, debezium-connector-mysql/src/main, debezium-embedded/src/main], projectClasspath='[Ljava.net.URL;@45c8e616', projectTests=[io.debezium.config.ConfigurationTest, io.debezium.connector.mongodb.CollectionIdTest, io.debezium.connector.mongodb.ConnectionIT, io.debezium.connector.mongodb.FiltersTest, io.debezium.connector.mongodb.ModuleTest, io.debezium.connector.mongodb.MongoClientsIT, io.debezium.connector.mongodb.MongoDbConnectorIT, io.debezium.connector.mongodb.MongoDbConnectorTest, io.debezium.connector.mongodb.MongoUtilTest, io.debezium.connector.mongodb.RecordMakersTest, io.debezium.connector.mongodb.ReplicaSetsTest, io.debezium.connector.mongodb.ReplicatorIT, io.debezium.connector.mongodb.SourceInfoTest, io.debezium.connector.mongodb.TopicSelectorTest, io.debezium.connector.mongodb.transforms.MongoDataConverterTest, io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelopeTest, io.debezium.connector.mysql.BinlogReaderBufferIT, io.debezium.connector.mysql.BinlogReaderIT, io.debezium.connector.mysql.ChainedReaderTest, io.debezium.connector.mysql.ConnectionIT, io.debezium.connector.mysql.FiltersTest, io.debezium.connector.mysql.GtidSetTest, io.debezium.connector.mysql.MetadataIT, io.debezium.connector.mysql.MySqlConnectorIT, io.debezium.connector.mysql.MySqlConnectorJsonIT, io.debezium.connector.mysql.MySqlConnectorRegressionIT, io.debezium.connector.mysql.MySqlConnectorTest, io.debezium.connector.mysql.MySqlDdlParserTest, io.debezium.connector.mysql.MySqlFixedLengthBinaryColumnIT, io.debezium.connector.mysql.MySqlGeometryIT, io.debezium.connector.mysql.MySqlGeometryTest, io.debezium.connector.mysql.MySqlSchemaTest, io.debezium.connector.mysql.MySqlTableMaintenanceStatementsIT, io.debezium.connector.mysql.MySqlTaskContextIT, io.debezium.connector.mysql.MySqlTaskContextTest, io.debezium.connector.mysql.MySqlUnsignedIntegerConverterTest, io.debezium.connector.mysql.MySqlUnsignedIntegerIT, io.debezium.connector.mysql.MySqlValueConvertersTest, io.debezium.connector.mysql.ReadBinLogIT, io.debezium.connector.mysql.SnapshotReaderIT, io.debezium.connector.mysql.SourceInfoTest, io.debezium.connector.mysql.TableConvertersTest, io.debezium.connector.postgresql.CustomTypeEncodingTest, io.debezium.connector.postgresql.PostgresConnectorIT, io.debezium.connector.postgresql.PostgresConnectorTaskIT, io.debezium.connector.postgresql.PostgresSchemaIT, io.debezium.connector.postgresql.RecordsSnapshotProducerIT, io.debezium.connector.postgresql.RecordsStreamProducerIT, io.debezium.connector.postgresql.SnapshotWithOverridesProducerIT, io.debezium.connector.postgresql.SourceInfoTest, io.debezium.connector.postgresql.connection.PostgresConnectionIT, io.debezium.connector.postgresql.connection.ReplicationConnectionIT, io.debezium.connector.postgresql.connection.wal2json.ISODateTimeFormatTest, io.debezium.connector.simple.SimpleSourceConnectorOutputTest, io.debezium.data.EnvelopeTest, io.debezium.document.ArraySerdesTest, io.debezium.document.DocumentSerdesTest, io.debezium.document.DocumentTest, io.debezium.document.JacksonArrayReadingAndWritingTest, io.debezium.document.JacksonWriterTest, io.debezium.document.PathsTest, io.debezium.embedded.EmbeddedEngineTest, io.debezium.embedded.OffsetCommitPolicyTest, io.debezium.function.BufferedBlockingConsumerTest, io.debezium.function.PredicatesTest, io.debezium.kafka.KafkaClusterTest, io.debezium.kafka.ZookeeperServerTest, io.debezium.relational.ColumnEditorTest, io.debezium.relational.SelectorsTest, io.debezium.relational.TableEditorTest, io.debezium.relational.TableIdParserTest, io.debezium.relational.TableSchemaBuilderTest, io.debezium.relational.TableTest, io.debezium.relational.ddl.DataTypeGrammarParserTest, io.debezium.relational.ddl.DataTypeParserTest, io.debezium.relational.ddl.DdlChangesTest, io.debezium.relational.ddl.DdlParserSql2003Test, io.debezium.relational.history.FileDatabaseHistoryTest, io.debezium.relational.history.HistoryRecordTest, io.debezium.relational.history.KafkaDatabaseHistoryTest, io.debezium.relational.history.MemoryDatabaseHistoryTest, io.debezium.relational.mapping.ColumnMappersTest, io.debezium.relational.mapping.MaskStringsTest, io.debezium.relational.mapping.TruncateStringsTest, io.debezium.text.PositionTest, io.debezium.text.TokenStreamTest, io.debezium.text.XmlCharactersTest, io.debezium.time.ConversionsTest, io.debezium.time.TemporalsTest, io.debezium.transforms.ByLogicalTableRouterTest, io.debezium.transforms.UnwrapFromEnvelopeTest, io.debezium.util.ElapsedTimeStrategyTest, io.debezium.util.HashCodeTest, io.debezium.util.SchemaNameAdjusterTest, io.debezium.util.StringsTest, io.debezium.util.TestingTest], complianceLevel=8, outputFolder=., json=true}
18:36:56.671 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Available processors (cores): 8
18:36:56.679 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Free memory: 184 MB
18:36:56.679 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Maximum memory: 3 GB
18:36:56.680 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Total memory available to JVM: 241 MB
18:36:56.680 [main] INFO  fr.inria.lille.repair.nopol.NoPol - Java version: 1.8.0_181
18:36:56.681 [main] INFO  fr.inria.lille.repair.nopol.NoPol - JAVA_HOME: /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/
18:36:56.681 [main] INFO  fr.inria.lille.repair.nopol.NoPol - PATH: /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/bin:/usr/games
18:37:17.651 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1
18:37:17.652 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:201 which is executed by 67 tests
18:37:17.652 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:201
18:37:17.652 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #2
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:450 which is executed by 67 tests
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:450
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #3
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:449 which is executed by 67 tests
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:449
18:37:17.653 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #4
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:407 which is executed by 67 tests
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:407
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #5
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:406 which is executed by 73 tests
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:406
18:37:17.654 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #6
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:405 which is executed by 73 tests
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:405
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #7
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:402 which is executed by 73 tests
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:402
18:37:17.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #8
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:260 which is executed by 73 tests
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:260
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #9
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:259 which is executed by 73 tests
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:259
18:37:17.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #10
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:55 which is executed by 49 tests
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:55
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #11
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:617 which is executed by 49 tests
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:617
18:37:17.657 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #12
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:615 which is executed by 49 tests
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:615
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #13
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:614 which is executed by 49 tests
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:614
18:37:17.658 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #14
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:607 which is executed by 49 tests
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:607
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #15
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:600 which is executed by 49 tests
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:600
18:37:17.659 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #16
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:593 which is executed by 49 tests
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:593
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #17
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:591 which is executed by 49 tests
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:591
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #18
18:37:17.660 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:590 which is executed by 49 tests
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:590
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #19
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:589 which is executed by 49 tests
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:589
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #20
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:588 which is executed by 49 tests
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:588
18:37:17.661 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #21
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:587 which is executed by 49 tests
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:587
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #22
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:580 which is executed by 49 tests
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:580
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #23
18:37:17.662 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:570 which is executed by 49 tests
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:570
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #24
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:569 which is executed by 49 tests
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:569
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #25
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:568 which is executed by 49 tests
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:568
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #26
18:37:17.663 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:567 which is executed by 49 tests
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:567
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #27
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:560 which is executed by 49 tests
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:560
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #28
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:559 which is executed by 49 tests
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:559
18:37:17.664 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #29
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:558 which is executed by 49 tests
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:558
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #30
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:557 which is executed by 49 tests
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:557
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #31
18:37:17.665 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:556 which is executed by 49 tests
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:556
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #32
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:554 which is executed by 49 tests
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:554
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #33
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:553 which is executed by 49 tests
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:553
18:37:17.666 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #34
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:549 which is executed by 49 tests
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:549
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #35
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:548 which is executed by 49 tests
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:548
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #36
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:547 which is executed by 49 tests
18:37:17.667 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:547
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #37
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:546 which is executed by 49 tests
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:546
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #38
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:543 which is executed by 49 tests
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:543
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #39
18:37:17.668 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:539 which is executed by 49 tests
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:539
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #40
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:538 which is executed by 49 tests
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:538
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #41
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:537 which is executed by 49 tests
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:537
18:37:17.669 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #42
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:535 which is executed by 49 tests
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:535
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #43
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:534 which is executed by 49 tests
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:534
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #44
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:533 which is executed by 49 tests
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:533
18:37:17.670 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #45
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:532 which is executed by 49 tests
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:532
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #46
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:531 which is executed by 49 tests
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:531
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #47
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:529 which is executed by 49 tests
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:529
18:37:17.671 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #48
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:526 which is executed by 49 tests
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:526
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #49
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:525 which is executed by 49 tests
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:525
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #50
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:524 which is executed by 49 tests
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:524
18:37:17.672 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #51
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:523 which is executed by 49 tests
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:523
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #52
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:521 which is executed by 49 tests
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:521
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #53
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:518 which is executed by 49 tests
18:37:17.673 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:518
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #54
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:513 which is executed by 49 tests
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:513
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #55
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:512 which is executed by 49 tests
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:512
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #56
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:510 which is executed by 49 tests
18:37:17.674 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:510
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #57
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:509 which is executed by 49 tests
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:509
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #58
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:507 which is executed by 49 tests
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:507
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #59
18:37:17.675 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:431 which is executed by 49 tests
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:431
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #60
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:287 which is executed by 49 tests
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:287
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #61
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:286 which is executed by 49 tests
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:286
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #62
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:285 which is executed by 49 tests
18:37:17.676 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:285
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #63
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:259 which is executed by 49 tests
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:259
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #64
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:256 which is executed by 49 tests
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:256
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #65
18:37:17.677 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:255 which is executed by 49 tests
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:255
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #66
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:247 which is executed by 49 tests
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:247
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #67
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:246 which is executed by 49 tests
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:246
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #68
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:245 which is executed by 49 tests
18:37:17.678 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:245
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #69
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:235 which is executed by 49 tests
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:235
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #70
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:233 which is executed by 49 tests
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:233
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #71
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:227 which is executed by 49 tests
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:227
18:37:17.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #72
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:224 which is executed by 49 tests
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:224
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #73
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:223 which is executed by 49 tests
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:223
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #74
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:217 which is executed by 49 tests
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:217
18:37:17.680 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #75
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:216 which is executed by 49 tests
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:216
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #76
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:215 which is executed by 49 tests
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:215
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #77
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:214 which is executed by 49 tests
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:214
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #78
18:37:17.681 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:213 which is executed by 49 tests
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:213
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #79
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:212 which is executed by 49 tests
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:212
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #80
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:209 which is executed by 49 tests
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:209
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #81
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:205 which is executed by 49 tests
18:37:17.682 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:205
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #82
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:146 which is executed by 49 tests
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:146
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #83
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:138 which is executed by 49 tests
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:138
18:37:17.683 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #84
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:134 which is executed by 49 tests
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:134
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #85
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:133 which is executed by 49 tests
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:133
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #86
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:132 which is executed by 49 tests
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:132
18:37:17.684 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #87
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:126 which is executed by 49 tests
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:126
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #88
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:121 which is executed by 49 tests
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:121
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #89
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:117 which is executed by 49 tests
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:117
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #90
18:37:17.685 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:116 which is executed by 49 tests
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:116
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #91
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:106 which is executed by 49 tests
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:106
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #92
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:102 which is executed by 49 tests
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:102
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #93
18:37:17.686 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:99 which is executed by 49 tests
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:99
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #94
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:95 which is executed by 49 tests
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:95
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #95
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:93 which is executed by 49 tests
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:93
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #96
18:37:17.687 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:86 which is executed by 49 tests
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:86
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #97
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:85 which is executed by 49 tests
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:85
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #98
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:79 which is executed by 49 tests
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:79
18:37:17.688 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #99
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:38 which is executed by 49 tests
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:38
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #100
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:37 which is executed by 49 tests
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:37
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #101
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:35 which is executed by 49 tests
18:37:17.689 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:35
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #102
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:34 which is executed by 49 tests
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:34
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #103
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:33 which is executed by 49 tests
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:33
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #104
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:32 which is executed by 49 tests
18:37:17.690 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:32
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #105
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:31 which is executed by 49 tests
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:31
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #106
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:265 which is executed by 49 tests
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:265
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #107
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:261 which is executed by 49 tests
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:261
18:37:17.691 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #108
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:260 which is executed by 49 tests
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:260
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #109
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:256 which is executed by 49 tests
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:256
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #110
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:144 which is executed by 49 tests
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:144
18:37:17.692 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #111
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:142 which is executed by 49 tests
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:142
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #112
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:136 which is executed by 49 tests
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:136
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #113
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:135 which is executed by 49 tests
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:135
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #114
18:37:17.693 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:134 which is executed by 49 tests
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:134
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #115
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:133 which is executed by 49 tests
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:133
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #116
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:132 which is executed by 49 tests
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:132
18:37:17.694 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #117
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:131 which is executed by 49 tests
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:131
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #118
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:129 which is executed by 49 tests
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:129
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #119
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:127 which is executed by 49 tests
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:127
18:37:17.695 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #120
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:122 which is executed by 49 tests
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:122
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #121
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:121 which is executed by 49 tests
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:121
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #122
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:116 which is executed by 49 tests
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:116
18:37:17.696 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #123
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:107 which is executed by 49 tests
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:107
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #124
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:93 which is executed by 49 tests
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:93
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #125
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:91 which is executed by 49 tests
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:91
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #126
18:37:17.697 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:90 which is executed by 49 tests
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.v3.ConnectionFactoryImpl:90
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #127
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostChooserFactory:20 which is executed by 49 tests
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostChooserFactory:20
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #128
18:37:17.698 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostChooserFactory:19 which is executed by 49 tests
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostChooserFactory:19
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #129
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.SingleHostChooser:26 which is executed by 49 tests
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.SingleHostChooser:26
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #130
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.HostRequirement:11 which is executed by 50 tests
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.HostRequirement:11
18:37:17.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #131
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:525 which is executed by 49 tests
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:525
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #132
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:523 which is executed by 49 tests
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:523
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #133
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:499 which is executed by 49 tests
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:499
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #134
18:37:17.700 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:490 which is executed by 49 tests
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:490
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #135
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:488 which is executed by 49 tests
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:488
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #136
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:485 which is executed by 49 tests
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:485
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #137
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.PGProperty:475 which is executed by 49 tests
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.PGProperty:475
18:37:17.701 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #138
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.PSQLState:16 which is executed by 49 tests
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.PSQLState:16
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #139
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:40 which is executed by 49 tests
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:40
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #140
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:29 which is executed by 49 tests
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:29
18:37:17.702 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #141
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:25 which is executed by 49 tests
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:25
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #142
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:21 which is executed by 49 tests
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:21
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #143
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:34 which is executed by 49 tests
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:34
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #144
18:37:17.703 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:33 which is executed by 49 tests
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:33
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #145
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.SocketFactoryFactory:32 which is executed by 49 tests
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.SocketFactoryFactory:32
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #146
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:62 which is executed by 49 tests
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:62
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #147
18:37:17.704 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:59 which is executed by 49 tests
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:59
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #148
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:58 which is executed by 49 tests
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:58
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #149
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:52 which is executed by 49 tests
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:52
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #150
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:39 which is executed by 49 tests
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:39
18:37:17.705 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #151
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.GT:24 which is executed by 49 tests
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.GT:24
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #152
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1017 which is executed by 49 tests
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1017
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #153
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1016 which is executed by 49 tests
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1016
18:37:17.706 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #154
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1015 which is executed by 49 tests
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1015
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #155
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1008 which is executed by 49 tests
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1008
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #156
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1007 which is executed by 49 tests
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1007
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #157
18:37:17.707 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1006 which is executed by 49 tests
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1006
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #158
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.jdbc.PgConnection:1001 which is executed by 49 tests
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.jdbc.PgConnection:1001
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #159
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver$1:86 which is executed by 49 tests
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver$1:86
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #160
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver$1:88 which is executed by 49 tests
18:37:17.708 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver$1:88
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #161
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:49 which is executed by 49 tests
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:49
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #162
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:48 which is executed by 49 tests
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:48
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #163
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:47 which is executed by 49 tests
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:47
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #164
18:37:17.709 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.core.ConnectionFactory:45 which is executed by 49 tests
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.core.ConnectionFactory:45
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #165
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:47 which is executed by 48 tests
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:47
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #166
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:44 which is executed by 48 tests
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:44
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #167
18:37:17.710 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:41 which is executed by 48 tests
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.hostchooser.GlobalHostStatusTracker:41
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #168
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:35 which is executed by 48 tests
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:35
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #169
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.util.HostSpec:34 which is executed by 48 tests
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.util.HostSpec:34
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #170
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:281 which is executed by 187 tests
18:37:17.711 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:281
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #171
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:111 which is executed by 189 tests
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:111
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #172
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:107 which is executed by 189 tests
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:107
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #173
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:103 which is executed by 189 tests
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:103
18:37:17.712 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #174
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.impl.StaticLoggerBinder:68 which is executed by 189 tests
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.impl.StaticLoggerBinder:68
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #175
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:299 which is executed by 189 tests
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:299
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #176
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:297 which is executed by 189 tests
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:297
18:37:17.713 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #177
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:293 which is executed by 189 tests
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:293
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #178
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:270 which is executed by 189 tests
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:270
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #179
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.LoggerFactory:269 which is executed by 189 tests
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.LoggerFactory:269
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #180
18:37:17.714 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.ContextSelectorStaticBinder:104 which is executed by 189 tests
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.ContextSelectorStaticBinder:104
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #181
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:34 which is executed by 189 tests
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:34
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #182
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:30 which is executed by 189 tests
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.selector.DefaultContextSelector:30
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #183
18:37:17.715 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:45 which is executed by 189 tests
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:45
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #184
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:124 which is executed by 189 tests
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:124
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #185
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:122 which is executed by 189 tests
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:122
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #186
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:118 which is executed by 189 tests
18:37:17.716 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:118
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #187
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:117 which is executed by 189 tests
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:117
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #188
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:113 which is executed by 189 tests
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:113
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #189
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:107 which is executed by 189 tests
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:107
18:37:17.717 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #190
18:37:17.718 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.AbstractSourceInfo:26 which is executed by 26 tests
1884225868
18:37:18.119 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #191
18:37:18.120 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:125 which is executed by 161 tests
18:37:18.120 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:125
18:37:18.120 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #192
18:37:18.120 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mongodb.Module:22 which is executed by 25 tests
-1984916851
18:37:18.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #193
18:37:18.305 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:302 which is executed by 21 tests
380667182
18:37:18.581 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #194
18:37:18.582 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:269 which is executed by 21 tests
380667182
18:37:18.841 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #195
18:37:18.842 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:372 which is executed by 21 tests
380667182
18:37:19.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #196
18:37:19.147 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:337 which is executed by 21 tests
380667182
18:37:19.479 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #197
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:42 which is executed by 22 tests
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:42
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #198
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:39 which is executed by 22 tests
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:39
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #199
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:26 which is executed by 20 tests
18:37:19.480 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:26
18:37:19.481 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #200
18:37:19.481 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:25 which is executed by 20 tests
18:37:19.481 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:25
18:37:19.481 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #201
18:37:19.481 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:330 which is executed by 20 tests
380667182
18:37:19.731 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #202
18:37:19.731 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:329 which is executed by 20 tests
380667182
18:37:19.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,693 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,701 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,702 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,702 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,702 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,751 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,751 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,751 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,751 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,752 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,784 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,784 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,784 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,784 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,784 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,815 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,816 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,816 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,816 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,816 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,850 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,851 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,851 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,851 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,851 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,882 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,882 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,883 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,883 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,883 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,915 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,915 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,915 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,915 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,915 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,949 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,950 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,950 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,950 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,950 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:20,979 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,979 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,979 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,980 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:20,980 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:21,011 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,011 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,011 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,011 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,012 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:21,038 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,039 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,039 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,039 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,039 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,060 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,060 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,060 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,060 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,061 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,080 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,080 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,080 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,080 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,081 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,102 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,102 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,103 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,103 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,103 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,125 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,125 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,125 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,125 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,125 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,146 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,146 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,146 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,146 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,146 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,161 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,162 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,162 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,162 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,162 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,175 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,175 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,175 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,175 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,175 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,187 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,187 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,188 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,188 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,188 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,200 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,200 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,200 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,200 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:21,200 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:21.203 [pool-4-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1180 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:23.792 [pool-5-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (321 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:23.798 [pool-6-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:23.813 [pool-7-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:23.818 [pool-8-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:23.819 [pool-3-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:23.819 [pool-3-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:23.819 [pool-3-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:329.
18:37:23.820 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #203
18:37:23.820 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:328 which is executed by 20 tests
380667182
18:37:24.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #204
18:37:24.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:315 which is executed by 20 tests
380667182
18:37:24.309 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,775 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,779 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,779 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,779 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,779 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,821 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,821 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,821 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,821 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,821 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,848 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,848 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,848 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,848 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,848 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,869 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,870 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,870 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,870 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,870 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,890 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,890 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,890 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,890 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,890 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,908 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,908 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,908 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,908 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,909 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,926 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,927 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,927 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,927 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,927 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,946 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,946 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,947 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,947 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,947 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,964 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,965 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,965 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,965 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,965 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,982 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,982 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,982 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,982 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,982 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:24,999 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,999 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,999 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,999 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:24,999 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,011 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,011 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,011 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,011 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,011 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,024 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,024 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,024 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,025 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,025 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,036 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,036 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,036 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,037 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,037 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,051 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,051 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,051 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,052 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,052 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,070 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,070 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,070 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,070 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,070 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,084 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,084 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,085 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,085 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,085 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,099 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,099 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,099 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,099 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,100 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,115 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,115 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,115 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,115 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,115 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,127 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,128 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,128 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,128 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:25,128 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:25.129 [pool-10-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (804 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:26.452 [pool-11-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (311 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:26.458 [pool-12-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (5 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:26.463 [pool-13-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:26.468 [pool-14-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:26.469 [pool-9-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:26.469 [pool-9-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:26.469 [pool-9-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:315.
18:37:26.469 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #205
18:37:26.470 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:314 which is executed by 20 tests
380667182
18:37:26.699 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,163 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,167 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,167 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,167 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,167 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,195 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,195 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,195 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,195 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,195 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,216 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,216 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,216 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,216 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,216 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,236 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,236 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,236 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,236 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,236 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,254 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,255 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,255 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,255 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,255 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,273 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,273 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,273 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,273 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,273 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,291 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,291 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,291 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,291 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,291 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,311 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,312 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,312 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,312 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,312 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,329 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,329 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,330 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,330 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,330 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,347 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,347 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,347 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,347 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,347 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:27,364 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,364 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,364 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,364 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,364 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,376 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,376 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,376 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,376 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,376 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,388 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,388 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,388 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,389 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,389 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,400 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,400 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,400 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,400 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,400 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,412 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,412 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,412 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,412 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,412 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,426 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,426 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,427 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,427 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,427 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,438 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,438 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,438 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,438 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,438 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,450 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,450 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,450 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,450 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,450 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,463 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,463 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,463 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,463 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,463 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,473 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,474 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,474 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,474 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:27,474 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:27.475 [pool-16-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (760 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:28.616 [pool-17-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (305 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:28.623 [pool-18-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:28.628 [pool-19-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:28.632 [pool-20-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:28.633 [pool-15-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:28.634 [pool-15-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:28.634 [pool-15-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:314.
18:37:28.634 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #206
18:37:28.634 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:313 which is executed by 20 tests
380667182
18:37:28.863 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,335 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,338 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,339 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,339 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,339 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,368 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,369 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,369 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,369 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,369 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,391 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,392 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,392 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,392 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,392 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,415 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,415 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,415 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,415 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,415 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,437 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,437 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,437 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,437 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,437 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,458 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,458 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,458 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,458 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,458 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,479 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,479 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,479 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,479 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,479 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,501 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,501 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,501 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,502 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,502 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,521 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,521 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,522 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,522 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,522 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,540 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,540 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,541 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,541 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,541 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:29,556 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,556 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,557 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,557 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,557 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,569 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,569 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,570 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,570 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,570 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,583 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,583 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,583 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,583 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,583 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,596 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,596 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,596 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,596 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,597 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,610 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,610 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,610 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,610 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,610 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,626 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,626 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,626 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,626 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,626 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,637 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,637 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,638 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,638 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,638 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,649 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,649 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,649 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,649 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,650 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,662 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,662 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,662 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,662 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,662 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,674 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,674 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,674 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,674 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:29,674 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:29.675 [pool-22-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (795 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:31.099 [pool-23-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (608 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:31.105 [pool-24-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:31.111 [pool-25-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:31.115 [pool-26-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:31.116 [pool-21-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:31.117 [pool-21-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:31.117 [pool-21-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:313.
18:37:31.117 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #207
18:37:31.118 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:312 which is executed by 20 tests
380667182
18:37:31.357 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,807 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,811 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,811 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,811 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,812 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,840 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,840 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,841 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,841 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,841 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,859 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,859 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,860 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,860 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,860 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,878 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,878 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,879 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,879 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,879 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,897 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,897 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,897 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,897 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,897 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,914 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,914 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,914 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,914 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,914 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,931 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,931 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,932 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,932 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,932 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,951 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,951 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,951 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,951 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,951 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,968 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,968 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,968 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,968 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,968 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:31,984 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,984 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,984 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,984 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:31,984 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:32,000 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,000 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,000 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,000 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,000 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,011 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,011 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,012 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,012 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,012 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,023 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,023 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,024 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,024 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,024 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,034 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,034 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,034 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,035 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,035 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,045 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,046 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,046 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,046 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,046 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,059 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,059 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,059 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,059 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,060 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,070 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,070 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,070 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,070 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,070 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,080 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,080 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,080 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,080 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,080 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,090 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,090 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,090 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,090 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,090 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,099 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,099 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,099 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,099 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:32,099 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:32.100 [pool-28-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (730 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:33.080 [pool-29-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (295 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:33.088 [pool-30-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:33.094 [pool-31-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:33.099 [pool-32-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:33.102 [pool-27-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:33.102 [pool-27-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:33.102 [pool-27-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotLockingMode:312.
18:37:33.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #208
18:37:33.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:385 which is executed by 20 tests
380667182
18:37:33.327 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,769 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,772 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,772 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,772 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,772 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,799 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,799 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,799 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,799 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,799 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,821 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,821 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,821 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,821 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,821 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,842 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,842 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,842 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,843 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,843 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,862 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,862 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,862 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,862 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,863 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,888 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,888 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,889 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,889 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,889 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,914 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,914 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,914 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,914 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,914 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,932 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,932 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,932 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,932 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,932 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,948 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,948 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,948 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,948 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,948 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,963 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,963 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,963 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,963 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,963 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:33,978 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,978 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,978 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,978 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,978 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,989 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,989 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,989 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,989 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:33,989 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,001 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,001 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,001 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,001 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,001 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,012 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,012 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,012 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,012 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,012 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,022 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,022 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,022 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,022 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,022 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,035 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,036 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,036 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,036 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,036 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,046 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,046 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,046 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,046 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,046 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,055 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,055 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,056 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,056 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,056 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,073 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,073 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,074 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,074 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,074 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,083 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,083 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,083 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,083 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:34,083 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:34.084 [pool-34-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (743 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:35.167 [pool-35-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (303 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:35.172 [pool-36-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:35.178 [pool-37-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:35.182 [pool-38-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:35.183 [pool-33-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:35.183 [pool-33-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:35.183 [pool-33-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:385.
18:37:35.184 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #209
18:37:35.184 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:384 which is executed by 20 tests
380667182
18:37:35.413 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,886 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,889 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,890 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,890 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,890 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,914 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,914 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,914 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,914 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,914 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,932 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,932 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,932 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,932 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,932 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,949 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,949 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,949 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,949 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,949 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,966 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,966 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,966 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,966 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,966 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,983 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,983 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,983 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,983 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,983 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:35,999 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:35,999 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,000 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,000 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,000 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:36,018 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,018 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,018 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,018 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,018 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:36,035 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,035 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,035 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,035 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,035 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:36,051 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,052 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,052 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,052 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,052 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:36,067 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,067 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,067 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,067 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,067 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,078 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,078 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,078 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,079 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,079 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,090 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,090 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,090 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,090 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,090 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,101 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,101 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,101 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,101 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,101 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,111 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,111 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,111 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,111 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,111 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,125 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,125 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,125 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,125 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,125 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,135 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,135 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,135 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,135 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,135 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,145 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,145 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,145 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,145 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,145 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,155 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,155 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,155 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,155 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,155 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,164 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,165 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,165 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,165 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:36,165 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:36.165 [pool-40-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (725 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:37.113 [pool-41-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (295 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:37.119 [pool-42-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:37.123 [pool-43-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:37.128 [pool-44-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:37.135 [pool-39-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:37.135 [pool-39-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:37.135 [pool-39-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:384.
18:37:37.135 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #210
18:37:37.135 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:383 which is executed by 20 tests
380667182
18:37:37.354 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,817 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,820 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,821 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,821 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,821 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,849 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,849 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,849 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,849 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,849 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,870 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,870 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,871 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,871 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,871 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,890 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,890 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,891 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,891 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,891 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,908 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,908 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,908 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,908 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,908 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,928 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,929 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,929 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,929 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,929 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,946 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,947 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,947 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,947 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,947 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,965 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,965 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,965 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,965 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,965 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,981 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,981 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,981 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,981 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,981 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:37,996 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,996 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,996 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,996 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:37,996 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:38,011 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,011 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,011 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,011 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,011 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,021 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,021 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,022 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,022 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,022 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,033 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,033 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,033 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,033 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,033 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,043 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,043 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,044 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,044 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,044 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,053 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,054 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,054 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,054 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,054 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,067 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,067 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,067 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,067 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,067 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,077 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,077 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,077 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,077 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,077 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,087 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,087 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,087 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,087 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,087 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,096 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,096 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,097 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,097 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,097 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,105 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,106 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,106 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,106 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:38,106 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:38.106 [pool-46-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (740 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:39.082 [pool-47-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (353 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:39.087 [pool-48-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:39.091 [pool-49-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:39.094 [pool-50-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:39.095 [pool-45-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:39.095 [pool-45-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:39.095 [pool-45-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:383.
18:37:39.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #211
18:37:39.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:382 which is executed by 20 tests
380667182
18:37:39.312 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,750 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,754 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,754 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,754 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,754 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,779 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,779 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,779 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,779 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,779 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,798 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,798 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,798 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,798 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,799 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,816 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,816 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,817 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,817 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,817 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,833 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,833 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,833 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,833 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,833 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,850 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,850 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,850 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,850 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,850 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,866 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,866 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,866 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,866 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,867 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,885 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,885 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,885 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,885 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,885 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,901 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,901 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,901 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,901 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,901 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,917 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,917 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,917 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,917 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,917 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018-11-12 18:37:39,932 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,932 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,932 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,932 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,933 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,943 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,943 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,943 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,943 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,944 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,954 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,954 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,954 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,954 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,955 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,965 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,965 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,965 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,965 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,965 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,975 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,975 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,976 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,976 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,976 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,989 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,989 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,989 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,989 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:39,989 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,000 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,000 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,000 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,000 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,000 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,011 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,011 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,011 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,011 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,011 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,022 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,022 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,022 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,022 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,022 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,032 INFO   ||  Attempting to generate a filtered GTID set   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,032 INFO   ||  GTID set from previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,032 INFO   ||  GTID set after applying GTID source includes/excludes to previous recorded offset: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,032 INFO   ||  GTID set available on server: 036d85a9-64e5-11e6-9b48-42010af0000c:1-20,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
2018-11-12 18:37:40,032 INFO   ||  Final merged GTID set to use when connecting to MySQL: 036d85a9-64e5-11e6-9b48-42010af0000c:1-2,123e4567-e89b-12d3-a456-426655440000:1-41,7145bf69-d1ca-11e5-a588-0242ac110004:1-3200   [io.debezium.connector.mysql.MySqlTaskContext]
18:37:40.032 [pool-52-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (709 ms)
<> Total tests run: 202
<> Ignored tests: 0
<> Failed tests: 22
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:40.941 [pool-53-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (296 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:40.945 [pool-54-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:40.950 [pool-55-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCloseJdbcConnectionOnShutdown(MySqlTaskContextIT.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18:37:40.953 [pool-56-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (2 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT)
[Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
org.apache.kafka.connect.errors.ConnectException: Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.    at io.debezium.connector.mysql.MySqlJdbcContext.readMySqlSystemVariables(MySqlJdbcContext.java:251)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:77)
    at io.debezium.connector.mysql.MySqlTaskContext.<init>(MySqlTaskContext.java:53)
    at io.debezium.connector.mysql.MySqlTaskContextIT.shouldCreateTaskFromConfiguration(MySqlTaskContextIT.java:24)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

18:37:40.958 [pool-51-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:40.958 [pool-51-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldCloseJdbcConnectionOnShutdown(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server., shouldCreateTaskFromConfiguration(io.debezium.connector.mysql.MySqlTaskContextIT): Error reading MySQL variables: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.]
18:37:40.958 [pool-51-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SecureConnectionMode:382.
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #212
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.ConditionalFail:27 which is executed by 19 tests
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.ConditionalFail:27
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #213
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:883 which is executed by 18 tests
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:883
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #214
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:823 which is executed by 18 tests
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:823
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #215
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:811 which is executed by 18 tests
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:811
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #216
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:808 which is executed by 18 tests
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:808
18:37:40.959 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #217
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:806 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:806
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #218
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:805 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:805
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #219
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:804 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:804
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #220
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:803 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:803
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #221
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:802 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:802
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #222
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:800 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:800
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #223
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:799 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:799
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #224
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:758 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:758
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #225
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:755 which is executed by 18 tests
18:37:40.960 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:755
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #226
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:754 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:754
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #227
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:751 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:751
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #228
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:750 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:750
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #229
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:748 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:748
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #230
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:731 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:731
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #231
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:711 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:711
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #232
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:709 which is executed by 18 tests
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:709
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #233
18:37:40.961 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:707 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:707
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #234
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:706 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:706
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #235
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:705 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:705
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #236
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:699 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:699
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #237
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:698 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:698
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #238
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:694 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:694
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #239
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:693 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:693
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #240
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:688 which is executed by 18 tests
18:37:40.962 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:688
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #241
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:687 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:687
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #242
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:685 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:685
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #243
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:683 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:683
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #244
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:682 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:682
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #245
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:680 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:680
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #246
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:679 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:679
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #247
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:677 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:677
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #248
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:671 which is executed by 18 tests
18:37:40.963 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:671
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #249
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:670 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:670
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #250
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:668 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:668
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #251
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:667 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:667
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #252
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:665 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:665
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #253
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:663 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:663
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #254
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:661 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:661
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #255
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:625 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:625
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #256
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:623 which is executed by 18 tests
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:623
18:37:40.964 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #257
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:614 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:614
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #258
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:612 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:612
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #259
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:606 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:606
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #260
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:602 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:602
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #261
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:600 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:600
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #262
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:579 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:579
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #263
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:479 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:479
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #264
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:334 which is executed by 18 tests
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:334
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #265
18:37:40.965 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:331 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:331
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #266
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:328 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:328
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #267
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:323 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:323
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #268
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:319 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:319
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #269
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:317 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:317
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #270
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:313 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:313
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #271
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:311 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:311
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #272
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:307 which is executed by 18 tests
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:307
18:37:40.966 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #273
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:224 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:224
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #274
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:216 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:216
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #275
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:215 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:215
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #276
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:213 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:213
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #277
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:211 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:211
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #278
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:210 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:210
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #279
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:209 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:209
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #280
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:208 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:208
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #281
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:206 which is executed by 18 tests
18:37:40.967 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:206
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #282
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:204 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:204
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #283
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:197 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:197
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #284
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.NonRegisteringDriver:195 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.NonRegisteringDriver:195
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #285
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:403 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:403
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #286
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:402 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:402
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #287
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:400 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:400
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #288
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:389 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:389
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #289
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:373 which is executed by 18 tests
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$IntegerConnectionProperty:373
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #290
18:37:40.968 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:58 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:58
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #291
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.CharsetMapping:687 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.CharsetMapping:687
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #292
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.postgresql.Driver:202 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.postgresql.Driver:202
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #293
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:94 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:94
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #294
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:87 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:87
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #295
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:83 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:83
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #296
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:81 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:81
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #297
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:77 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:77
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #298
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Messages:72 which is executed by 18 tests
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Messages:72
18:37:40.969 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #299
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:567 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:567
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #300
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:565 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:565
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #301
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:553 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:553
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #302
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:550 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:550
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #303
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:548 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:548
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #304
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:546 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:546
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #305
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:436 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:436
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #306
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:435 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:435
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #307
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:433 which is executed by 18 tests
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:433
18:37:40.970 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #308
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:432 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:432
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #309
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:425 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:425
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #310
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:151 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:151
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #311
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Util:131 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Util:131
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #312
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:525 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:525
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #313
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:524 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:524
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #314
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:505 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:505
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #315
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:503 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:503
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #316
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:502 which is executed by 18 tests
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$MemorySizeConnectionProperty:502
18:37:40.971 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #317
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2311 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2311
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #318
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2309 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2309
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #319
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2308 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2308
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #320
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2307 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2307
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #321
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2304 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2304
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #322
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2302 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2302
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #323
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2295 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2295
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #324
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:2286 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:2286
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #325
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1972 which is executed by 18 tests
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1972
18:37:40.972 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #326
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1971 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1971
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #327
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1970 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1970
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #328
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1968 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1968
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #329
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1964 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1964
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #330
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1487 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1487
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #331
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1483 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1483
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #332
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1482 which is executed by 18 tests
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1482
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #333
18:37:40.973 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1481 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1481
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #334
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1479 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1479
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #335
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1475 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1475
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #336
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1457 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1457
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #337
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1412 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1412
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #338
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1397 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1397
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #339
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1365 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1365
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #340
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1362 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1362
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #341
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1358 which is executed by 18 tests
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1358
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #342
18:37:40.974 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1356 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1356
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #343
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1355 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1355
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #344
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1344 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1344
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #345
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1342 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1342
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #346
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1340 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1340
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #347
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1339 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1339
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #348
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1335 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1335
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #349
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1331 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1331
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #350
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1292 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1292
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #351
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1291 which is executed by 18 tests
18:37:40.975 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1291
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #352
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1273 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1273
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #353
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1261 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1261
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #354
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1260 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1260
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #355
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1235 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1235
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #356
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1227 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1227
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #357
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1180 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1180
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #358
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1174 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1174
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #359
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1172 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1172
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #360
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1171 which is executed by 18 tests
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1171
18:37:40.976 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #361
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1169 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1169
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #362
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1168 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1168
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #363
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1167 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1167
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #364
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1166 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1166
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #365
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1164 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1164
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #366
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1163 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1163
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #367
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1162 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1162
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #368
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1158 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1158
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #369
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1156 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1156
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #370
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1152 which is executed by 18 tests
18:37:40.977 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1152
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #371
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1129 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1129
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #372
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1125 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1125
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #373
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1124 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1124
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #374
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1122 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1122
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #375
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1118 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1118
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #376
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1116 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1116
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #377
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1115 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1115
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #378
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1109 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1109
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #379
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1107 which is executed by 18 tests
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1107
18:37:40.978 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #380
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1106 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1106
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #381
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1100 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1100
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #382
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1096 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1096
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #383
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1094 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1094
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #384
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1093 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1093
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #385
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1092 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1092
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #386
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:1088 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:1088
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #387
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:135 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:135
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #388
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:126 which is executed by 18 tests
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:126
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #389
18:37:40.979 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StringUtils:124 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StringUtils:124
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #390
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1146 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1146
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #391
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1139 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1139
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #392
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1127 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1127
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #393
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1121 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1121
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #394
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1117 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1117
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #395
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1116 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1116
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #396
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1115 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1115
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #397
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1113 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1113
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #398
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1111 which is executed by 18 tests
18:37:40.980 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1111
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #399
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1101 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1101
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #400
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1078 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1078
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #401
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1067 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1067
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #402
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1052 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1052
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #403
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1050 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1050
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #404
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1048 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1048
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #405
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1046 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1046
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #406
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1044 which is executed by 18 tests
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1044
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #407
18:37:40.981 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1043 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1043
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #408
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1040 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1040
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #409
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1039 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1039
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #410
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1037 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1037
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #411
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1035 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1035
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #412
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1026 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1026
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #413
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1023 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1023
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #414
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1020 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1020
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #415
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1018 which is executed by 18 tests
18:37:40.982 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1018
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #416
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1016 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1016
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #417
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1015 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1015
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #418
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1013 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1013
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #419
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:1012 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:1012
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #420
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:998 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:998
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #421
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:995 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:995
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #422
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:989 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:989
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #423
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:984 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:984
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #424
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.SQLError:982 which is executed by 18 tests
18:37:40.983 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.SQLError:982
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #425
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4899 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4899
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #426
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4658 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4658
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #427
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4519 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4519
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #428
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4455 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4455
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #429
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4447 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:4447
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #430
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3790 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3790
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #431
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3628 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3628
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #432
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3556 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:3556
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #433
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2569 which is executed by 18 tests
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2569
18:37:40.984 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #434
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2565 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2565
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #435
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2563 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2563
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #436
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2562 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2562
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #437
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2561 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2561
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #438
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2560 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2560
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #439
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2559 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2559
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #440
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2558 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2558
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #441
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2557 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2557
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #442
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2556 which is executed by 18 tests
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2556
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #443
18:37:40.985 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2555 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2555
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #444
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2554 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2554
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #445
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2553 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2553
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #446
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2552 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2552
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #447
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2544 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2544
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #448
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2539 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2539
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #449
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2535 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2535
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #450
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2534 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2534
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #451
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2531 which is executed by 18 tests
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2531
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #452
18:37:40.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2529 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2529
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #453
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2521 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2521
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #454
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2518 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2518
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #455
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2514 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2514
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #456
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2509 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2509
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #457
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2507 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2507
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #458
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2504 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2504
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #459
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2500 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2500
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #460
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2498 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2498
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #461
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2495 which is executed by 18 tests
18:37:40.987 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2495
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #462
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2494 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2494
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #463
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2492 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2492
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #464
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2490 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2490
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #465
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2489 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2489
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #466
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2488 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2488
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #467
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2487 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2487
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #468
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2486 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2486
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #469
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2485 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2485
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #470
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2483 which is executed by 18 tests
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2483
18:37:40.988 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #471
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2479 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2479
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #472
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2477 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2477
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #473
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2475 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2475
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #474
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2417 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2417
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #475
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2246 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2246
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #476
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2210 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2210
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #477
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2201 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2201
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #478
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2062 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2062
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #479
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2026 which is executed by 18 tests
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:2026
18:37:40.989 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #480
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1990 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1990
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #481
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1972 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1972
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #482
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1945 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1945
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #483
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1936 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1936
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #484
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1918 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1918
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #485
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1900 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1900
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #486
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1882 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1882
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #487
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1846 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1846
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #488
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1806 which is executed by 18 tests
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1806
18:37:40.990 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #489
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1627 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1627
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #490
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1618 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1618
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #491
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1609 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1609
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #492
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1537 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1537
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #493
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1386 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1386
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #494
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1383 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1383
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #495
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1379 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1379
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #496
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1378 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1378
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #497
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1376 which is executed by 18 tests
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1376
18:37:40.991 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #498
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1374 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1374
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #499
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1371 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1371
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #500
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1370 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1370
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #501
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1368 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1368
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #502
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1364 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl:1364
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #503
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:4975 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:4975
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #504
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:3325 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:3325
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #505
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.MysqlIO:3320 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.MysqlIO:3320
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #506
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:486 which is executed by 18 tests
18:37:40.992 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:486
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #507
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:485 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:485
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #508
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:483 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:483
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #509
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:472 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$LongConnectionProperty:472
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #510
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:71 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:71
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #511
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:69 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:69
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #512
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:67 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:67
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #513
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:64 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:64
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #514
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:61 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:61
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #515
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:56 which is executed by 18 tests
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:56
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #516
18:37:40.993 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.log.LogFactory:52 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.log.LogFactory:52
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #517
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:583 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:583
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #518
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:582 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:582
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #519
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:580 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:580
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #520
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:578 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:578
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #521
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:577 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:577
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #522
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:575 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:575
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #523
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:574 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:574
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #524
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:558 which is executed by 18 tests
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$StringConnectionProperty:558
18:37:40.994 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #525
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:266 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:266
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #526
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:263 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:263
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #527
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:253 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:253
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #528
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:245 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:245
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #529
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:222 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:222
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #530
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:221 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:221
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #531
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:217 which is executed by 18 tests
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:217
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #532
18:37:40.995 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:216 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:216
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #533
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:215 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:215
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #534
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:214 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:214
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #535
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:211 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:211
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #536
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:207 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:207
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #537
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:205 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:205
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #538
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:203 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:203
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #539
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:201 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:201
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #540
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:199 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:199
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #541
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:195 which is executed by 18 tests
18:37:40.996 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:195
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #542
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:190 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:190
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #543
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:188 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:188
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #544
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:187 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:187
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #545
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:184 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:184
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #546
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:181 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:181
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #547
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:179 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:179
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #548
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:177 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:177
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #549
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:175 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:175
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #550
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:171 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:171
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #551
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:170 which is executed by 18 tests
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:170
18:37:40.997 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #552
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:169 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:169
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #553
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:167 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:167
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #554
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:165 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:165
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #555
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:164 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:164
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #556
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:157 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:157
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #557
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:154 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:154
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #558
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:152 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:152
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #559
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:148 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:148
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #560
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:146 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:146
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #561
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:142 which is executed by 18 tests
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:142
18:37:40.998 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #562
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:140 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:140
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #563
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:137 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:137
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #564
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:136 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:136
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #565
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:134 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:134
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #566
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:132 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:132
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #567
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.StandardSocketFactory:120 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.StandardSocketFactory:120
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #568
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:98 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:98
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #569
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:97 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:97
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #570
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:70 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:70
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #571
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:69 which is executed by 18 tests
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:69
18:37:40.999 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #572
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:67 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.fabric.jdbc.FabricMySQLDriver:67
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #573
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Buffer:464 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Buffer:464
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #574
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.Buffer:463 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.Buffer:463
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #575
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:99 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:99
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #576
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:98 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:98
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #577
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:96 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:96
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #578
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:94 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:94
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #579
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:93 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:93
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #580
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:91 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:91
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #581
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:90 which is executed by 18 tests
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:90
18:37:41.000 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #582
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:74 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:74
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #583
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:70 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty:70
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #584
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:3124 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:3124
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #585
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:3119 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:3119
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #586
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:786 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:786
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #587
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:780 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:780
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #588
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.DatabaseMetaData:772 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.DatabaseMetaData:772
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #589
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:283 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:283
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #590
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:282 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:282
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #591
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:281 which is executed by 18 tests
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:281
18:37:41.001 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #592
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:278 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:278
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #593
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:277 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:277
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #594
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:273 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:273
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #595
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:271 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:271
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #596
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:217 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:217
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #597
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:216 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:216
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #598
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:215 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:215
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #599
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:214 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:214
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #600
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:200 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:200
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #601
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:192 which is executed by 18 tests
18:37:41.002 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:192
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #602
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:166 which is executed by 18 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.ConnectionPropertiesImpl$ConnectionProperty:166
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #603
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:37 which is executed by 29 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:37
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #604
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.AnnotationBasedTestRule:36 which is executed by 29 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.AnnotationBasedTestRule:36
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #605
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:123 which is executed by 47 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:123
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #606
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:44 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:44
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #607
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:42 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:42
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #608
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:41 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:41
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #609
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:40 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LoggerNameUtil:40
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #610
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:370 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:370
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #611
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:369 which is executed by 32 tests
18:37:41.003 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:369
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #612
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:368 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:368
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #613
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:367 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:367
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #614
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:363 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:363
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #615
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:356 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:356
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #616
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:355 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:355
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #617
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:145 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:145
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #618
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:141 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:141
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #619
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:140 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:140
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #620
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:138 which is executed by 32 tests
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:138
18:37:41.004 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #621
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:137 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:137
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #622
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:136 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:136
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #623
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:135 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:135
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #624
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:132 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:132
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #625
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:157 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:157
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #626
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:156 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:156
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #627
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:152 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:152
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #628
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:150 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:150
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #629
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:149 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:149
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #630
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:148 which is executed by 32 tests
18:37:41.005 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:148
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #631
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:147 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:147
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #632
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:145 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:145
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #633
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:144 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:144
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #634
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:143 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:143
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #635
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:142 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:142
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #636
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:141 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:141
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #637
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:140 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:140
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #638
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:139 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:139
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #639
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:136 which is executed by 32 tests
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:136
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #640
18:37:41.006 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:134 which is executed by 32 tests
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:134
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #641
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:133 which is executed by 32 tests
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:133
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #642
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:132 which is executed by 32 tests
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:132
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #643
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:52 which is executed by 8 tests
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:52
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #644
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:51 which is executed by 8 tests
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:51
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #645
18:37:41.007 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.connector.mysql.MySqlConnectorConfig$SnapshotMode:226 which is executed by 5 tests
380667182
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #646
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:54 which is executed by 30 tests
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:54
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #647
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:44 which is executed by 30 tests
18:37:41.224 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:44
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #648
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:43 which is executed by 30 tests
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:43
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #649
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:41 which is executed by 30 tests
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation com.mysql.jdbc.AbandonedConnectionCleanupThread:41
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #650
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:503 which is executed by 13 tests
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:503
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #651
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:502 which is executed by 13 tests
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:502
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #652
18:37:41.225 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:404 which is executed by 12 tests
-2099881367
18:37:41.458 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #653
18:37:41.458 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:414 which is executed by 10 tests
-2099881367
18:37:41.655 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #654
18:37:41.656 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.IoUtil$1:413 which is executed by 10 tests
-2099881367
18:37:41.849 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:37:42,940 INFO   ||  binding to port localhost/127.0.0.1:43445   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:42,954 INFO   ||  Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:host.name=granduc-16.luxembourg.grid5000.fr   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.version=1.8.0_181   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.vendor=Oracle Corporation   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.class.path=/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.io.tmpdir=/tmp   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:java.compiler=<NA>   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:os.name=Linux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:os.arch=amd64   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,954 INFO   ||  Server environment:os.version=4.9.0-8-amd64   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,955 INFO   ||  Server environment:user.name=tdurieux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,955 INFO   ||  Server environment:user.home=/home/tdurieux   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,955 INFO   ||  Server environment:user.dir=/tmp/Nopol_Bears_debezium-debezium_351465554-354212780   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:42,974 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:43,001 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:43,025 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 42903
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:43445
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:43,160 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:43,164 INFO   ||  Connecting to zookeeper on localhost:43445   [kafka.server.KafkaServer]
2018-11-12 18:37:43,197 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:43,204 INFO   ||  Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:host.name=granduc-16.luxembourg.grid5000.fr   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.version=1.8.0_181   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.vendor=Oracle Corporation   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.class.path=/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin//../lib/tools.jar   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.io.tmpdir=/tmp   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:java.compiler=<NA>   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:os.name=Linux   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,204 INFO   ||  Client environment:os.arch=amd64   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,205 INFO   ||  Client environment:os.version=4.9.0-8-amd64   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,205 INFO   ||  Client environment:user.name=tdurieux   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,205 INFO   ||  Client environment:user.home=/home/tdurieux   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,205 INFO   ||  Client environment:user.dir=/tmp/Nopol_Bears_debezium-debezium_351465554-354212780   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,211 INFO   ||  Initiating client connection, connectString=localhost:43445 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@22f4e2d9   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:43,234 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:43,239 INFO   ||  Opening socket connection to server localhost/127.0.0.1:43445. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:43,277 INFO   ||  Socket connection established to localhost/127.0.0.1:43445, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:43,277 INFO   ||  Accepted socket connection from /127.0.0.1:42532   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:43,291 INFO   ||  Client attempting to establish new session at /127.0.0.1:42532   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:43,295 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:43,348 INFO   ||  Established session 0x1670a49b9460000 with negotiated timeout 6000 for client /127.0.0.1:42532   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:43,349 INFO   ||  Session establishment complete on server localhost/127.0.0.1:43445, sessionid = 0x1670a49b9460000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:43,351 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:43,451 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:43,475 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:43,504 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:43,820 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:43,840 INFO   ||  Cluster ID = jae5fWNTRzKXIxJRtJ451Q   [kafka.server.KafkaServer]
2018-11-12 18:37:43,846 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:43,915 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:43,916 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:43,919 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:43,996 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:44,012 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:44,029 INFO   ||  Logs loading complete in 17 ms.   [kafka.log.LogManager]
2018-11-12 18:37:44,149 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:44,152 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:44,158 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:44,161 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:44,532 INFO   ||  Awaiting socket connections on localhost:42903.   [kafka.network.Acceptor]
2018-11-12 18:37:44,538 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:44,578 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,580 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,582 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,597 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:44,654 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:44,661 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,671 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,672 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:44,677 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:44,694 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:44,696 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:44,696 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:44,706 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:setData cxid:0x2a zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:44,709 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:44,711 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:44,715 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:44,717 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 5 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:44,795 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:44,830 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,831 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,843 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,845 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:44,847 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,848 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:44,849 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,849 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:44,850 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:44,866 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,867 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,900 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:44,906 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:44,908 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:44,912 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,913 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,914 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,915 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:44,916 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:44,920 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:44,926 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:44,930 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:44,936 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:44,937 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:44,938 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:44,948 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:44,949 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,42903,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:44,952 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:44,975 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:37:44,984 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:44,984 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:44,987 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:44,988 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:44,989 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:37:44,990 INFO   ||  Started Kafka server 1 at localhost:42903 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:45,016 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:42903 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:45,071 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,081 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,093 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:45,108 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:45,111 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:45,115 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:45,116 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:45,120 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:42903]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:45,121 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:45,129 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:45,137 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,139 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,152 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,152 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,182 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:45,183 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:45,192 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:45,244 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:45,256 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 36 ms   [kafka.log.Log]
2018-11-12 18:37:45,259 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:45,260 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:45,264 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:45,266 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:45,290 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 3 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:45,452 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:45,494 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:45,520 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:42903, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:45,520 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:42903, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:45,521 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:42903]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:45,527 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,527 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,532 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:42903]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,546 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,546 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,547 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,547 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,686 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:45,687 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:42903]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,689 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:45,690 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,690 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,691 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:45,699 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:45,808 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:45,810 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:45,847 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:42903]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:45,884 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,884 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:45,912 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,924 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,936 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:45,957 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:45,962 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:45,962 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:45,963 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:45,963 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:45,963 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:45,964 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:45,965 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,972 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49b9460000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:45,997 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:46,000 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:46,004 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:46,005 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms   [kafka.log.Log]
2018-11-12 18:37:46,006 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:46,007 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:46,007 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:46,007 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:46,009 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:46,019 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 7 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:46,099 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:42903 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,101 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,102 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,130 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,148 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,161 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,174 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:46,208 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,211 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,296 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,298 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,360 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:46,364 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:42903, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:46,364 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:42903, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:46,364 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:42903]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:46,368 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:46,368 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:46,376 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:42903 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,377 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,377 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,381 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,383 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,384 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,387 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,387 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,399 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,399 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,406 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:42903]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:46,409 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:46,409 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:46,419 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:42903 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,420 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,420 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,423 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,425 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,427 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,430 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:46,430 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:46,439 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1542065865407, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-11-12 18:37:46,441 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,442 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,447 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:37:46,448 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:37:46,462 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:37:46,472 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:37:46,477 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:46,494 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:46,495 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:46,500 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:46,505 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:37:46,507 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,538 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,543 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,639 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,644 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,664 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,664 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,668 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:46,669 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:46,670 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:37:46,670 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:46,671 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:46,671 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:46,673 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:46,674 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:46,674 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,840 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,845 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:46,873 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,873 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:46,873 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,025 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,025 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,026 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:47,027 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:37:47,027 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:47,028 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:47,029 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:47,031 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:47,031 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,028 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:47,139 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,139 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,139 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,180 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,180 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,180 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,184 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,184 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:47,204 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:37:47,205 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:37:47,207 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:37:47,207 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:37:47,208 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:37:47,208 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:37:47,247 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:47,292 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:47,335 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:37:47,336 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:47,336 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:47,336 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:47,341 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:47,342 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:47,343 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:37:47,343 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:37:47,343 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:37:47,346 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:37:47,347 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:47,348 INFO   ||  Processed session termination for sessionid: 0x1670a49b9460000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:47,357 INFO   ||  Session: 0x1670a49b9460000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:47,358 INFO   ||  EventThread shut down for session: 0x1670a49b9460000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:47,362 INFO   ||  Closed socket connection for client /127.0.0.1:42532 which had sessionid 0x1670a49b9460000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:37:47,363 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,916 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,919 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,919 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:47,921 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:37:47,968 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:37:47,974 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:37:47,978 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:48,010 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:48,013 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:48,013 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:48,013 INFO   ||  Stopped Kafka server 1 at localhost:42903   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:48,014 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:48,015 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,015 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,015 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:48,015 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,015 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:48,015 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,015 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:48,016 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:37:48,021 INFO   ||  binding to port localhost/127.0.0.1:36685   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:48,021 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,023 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,024 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40311
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:36685
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:48,026 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:48,026 INFO   ||  Connecting to zookeeper on localhost:36685   [kafka.server.KafkaServer]
2018-11-12 18:37:48,026 INFO   ||  Initiating client connection, connectString=localhost:36685 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@15e96293   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:48,027 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:48,027 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:48,028 INFO   ||  Opening socket connection to server localhost/127.0.0.1:36685. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:48,029 INFO   ||  Socket connection established to localhost/127.0.0.1:36685, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:48,029 INFO   ||  Accepted socket connection from /127.0.0.1:37666   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:48,029 INFO   ||  Client attempting to establish new session at /127.0.0.1:37666   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,029 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:48,045 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:48,059 INFO   ||  Established session 0x1670a49ccf60000 with negotiated timeout 6000 for client /127.0.0.1:37666   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:48,059 INFO   ||  Session establishment complete on server localhost/127.0.0.1:36685, sessionid = 0x1670a49ccf60000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:48,060 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:48,074 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,098 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,101 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:48,127 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,169 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:48,187 INFO   ||  Cluster ID = A_7F4BB0TLaBXhQ_X-xybw   [kafka.server.KafkaServer]
2018-11-12 18:37:48,187 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:48,189 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:48,196 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:48,196 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:48,197 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:48,198 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:48,199 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-11-12 18:37:48,291 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:48,291 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:48,292 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:48,293 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:48,352 INFO   ||  Awaiting socket connections on localhost:40311.   [kafka.network.Acceptor]
2018-11-12 18:37:48,989 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:48,990 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:48,991 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:48,993 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:48,999 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:48,999 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:49,003 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:49,007 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:49,007 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,007 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,008 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:49,007 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:49,007 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:49,007 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:49,011 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:49,019 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:49,025 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:49,026 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:49,027 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:49,028 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,029 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:49,030 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:49,030 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:49,038 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:49,050 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:49,050 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x47 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,051 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x48 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,061 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:49,061 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40311,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:49,062 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:49,062 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:49,063 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,063 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,063 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:49,064 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,064 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:49,065 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,065 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,073 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:49,075 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:37:49,078 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:37:49,079 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:49,081 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40311 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:49,084 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,084 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,084 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:49,084 INFO   ||  Started Kafka server 1 at localhost:40311 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:49,089 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,098 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,109 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:49,117 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:49,118 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:49,119 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:49,119 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,119 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:49,120 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40311, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,120 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40311, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,121 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40311]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:49,121 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,122 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,125 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,126 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,126 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40311]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,127 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,128 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,129 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,129 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,129 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:49,141 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:49,152 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:49,153 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:49,156 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:49,157 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:37:49,159 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:49,159 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:49,159 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:49,159 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:49,246 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,247 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40311]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:49,249 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,249 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:49,250 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,250 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,250 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:49,374 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,378 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40311]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:49,379 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:49,381 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,382 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,397 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,403 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,415 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:49,433 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:49,436 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:49,436 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:49,436 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:49,436 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,437 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:49,438 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:49,438 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,445 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49ccf60000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:49,470 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:49,471 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:49,475 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:49,475 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:37:49,476 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:49,476 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:49,476 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:49,476 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:49,477 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:49,478 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:49,493 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40311 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,494 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:49,494 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,503 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,505 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,507 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,510 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:49,532 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,533 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:49,852 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:49,895 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,896 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,911 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:49,944 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:49,947 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40311, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,947 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40311, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:49,948 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40311]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:49,951 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,951 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,959 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40311 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,959 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:49,959 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,963 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,965 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,967 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,970 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,970 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:49,981 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,981 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:49,985 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40311]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:49,989 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,989 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:49,998 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40311 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:49,999 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:49,999 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,003 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,005 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,007 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,010 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,010 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:50,022 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,022 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,027 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40311]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:50,030 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:50,030 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:50,039 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40311 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,040 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:50,040 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,043 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,044 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,046 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,049 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,050 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:50,061 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,061 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,065 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40311]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:50,068 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:50,068 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:50,076 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40311 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,076 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:50,076 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,079 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,080 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,082 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,084 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:50,085 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:50,096 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,096 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,100 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:37:50,100 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:37:50,104 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:37:50,105 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:37:50,107 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:50,113 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:50,113 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:50,116 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:50,119 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:37:50,120 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,208 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,208 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,208 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:50,208 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:50,208 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:37:50,208 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:50,208 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:50,208 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:50,208 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:50,208 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,208 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,212 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,212 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,212 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,280 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,280 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,280 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:50,280 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:37:50,280 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:50,281 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:50,281 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:50,281 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:50,281 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:50,281 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,296 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,467 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,467 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,467 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,562 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,600 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,600 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,600 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,800 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,800 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:50,824 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:37:50,824 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:37:50,825 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:37:50,825 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:37:50,825 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:37:50,825 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:37:50,911 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:37:50,911 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:50,911 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:50,911 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:50,912 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:50,912 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:50,912 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:37:50,912 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:37:50,912 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:37:50,914 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:37:50,914 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:50,915 INFO   ||  Processed session termination for sessionid: 0x1670a49ccf60000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:50,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:50,925 INFO   ||  Session: 0x1670a49ccf60000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:50,925 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:50,925 INFO   ||  EventThread shut down for session: 0x1670a49ccf60000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:50,925 INFO   ||  Closed socket connection for client /127.0.0.1:37666 which had sessionid 0x1670a49ccf60000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:37:50,964 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,006 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,190 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,190 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,190 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,196 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,196 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,196 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,197 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,197 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,197 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:37:51,228 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:37:51,230 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:37:51,231 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:51,231 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:51,231 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:51,231 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:51,232 INFO   ||  Stopped Kafka server 1 at localhost:40311   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:51,233 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:51,233 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,233 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,233 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:51,233 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,233 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,234 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:51,234 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:51,234 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:37:51,237 INFO   ||  binding to port localhost/127.0.0.1:34351   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:51,238 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,247 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,248 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40313
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:34351
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:51,249 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:51,249 INFO   ||  Connecting to zookeeper on localhost:34351   [kafka.server.KafkaServer]
2018-11-12 18:37:51,250 INFO   ||  Initiating client connection, connectString=localhost:34351 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5690d7a4   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:51,250 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:51,251 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:51,252 INFO   ||  Opening socket connection to server localhost/127.0.0.1:34351. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:51,252 INFO   ||  Socket connection established to localhost/127.0.0.1:34351, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:51,252 INFO   ||  Accepted socket connection from /127.0.0.1:41996   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:51,252 INFO   ||  Client attempting to establish new session at /127.0.0.1:41996   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,253 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:51,284 INFO   ||  Established session 0x1670a49d9870000 with negotiated timeout 6000 for client /127.0.0.1:41996   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:51,285 INFO   ||  Session establishment complete on server localhost/127.0.0.1:34351, sessionid = 0x1670a49d9870000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:51,285 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:51,299 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,323 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,353 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,396 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,412 INFO   ||  Cluster ID = EM_Ta5_MQc23luyAHNjcbQ   [kafka.server.KafkaServer]
2018-11-12 18:37:51,413 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:51,415 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,417 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,417 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:51,419 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:51,420 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:51,420 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:37:51,488 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:51,488 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:51,489 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:51,492 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:51,499 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:51,531 INFO   ||  Awaiting socket connections on localhost:40313.   [kafka.network.Acceptor]
2018-11-12 18:37:51,532 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:51,533 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,534 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,534 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,538 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:51,538 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:51,539 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,540 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:51,542 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,542 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:51,550 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:51,551 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:51,551 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:51,558 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:51,558 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:51,558 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:51,567 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,568 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:51,576 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:51,576 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:51,580 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:51,580 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:51,588 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:51,589 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x3f zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,589 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x40 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,607 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:51,607 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40313,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:51,607 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:51,613 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:51,614 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:51,615 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,615 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,615 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:51,615 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:51,615 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:51,616 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40313 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:51,617 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,617 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,617 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,617 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:51,617 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:51,617 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:51,617 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:delete cxid:0x4e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,632 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,632 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,633 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:51,633 INFO   ||  Started Kafka server 1 at localhost:40313 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:51,640 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:51,643 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:setData cxid:0x52 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,647 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x53 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,658 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:51,664 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40313]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:51,667 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:51,667 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:51,668 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,668 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,669 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:51,669 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:51,669 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:51,670 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:51,671 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x5b zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,677 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x5c zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:51,677 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,701 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:51,703 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:51,707 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:51,708 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:37:51,709 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:51,710 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:51,710 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:51,710 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:51,786 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:51,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,824 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:51,827 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40313, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:51,828 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40313, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:51,828 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40313]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:51,831 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,831 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,831 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40313]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,833 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,833 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,833 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:51,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:51,948 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:51,949 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40313]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:51,950 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,950 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:51,950 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:51,954 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:52,066 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,069 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:52,069 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40313]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:52,073 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,073 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,086 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:52,096 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:52,108 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:52,125 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:52,128 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:52,128 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:52,128 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:52,128 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:52,128 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:52,129 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:52,130 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:52,137 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49d9870000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:52,162 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:52,163 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:52,166 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:52,167 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:37:52,167 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:52,168 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:52,168 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:52,168 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:52,168 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:52,170 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:52,181 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40313 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,181 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,181 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,184 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,186 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,187 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,189 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:52,211 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,213 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,214 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,221 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,222 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065871810, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,224 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,224 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,226 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065871817, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,227 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065871820, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,230 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,230 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,255 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:52,258 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40313, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,259 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40313, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,259 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40313]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:52,261 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,261 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,268 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40313 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,268 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,269 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,271 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,272 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,273 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,276 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,276 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,282 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,283 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065871810, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,283 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,284 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,284 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065871817, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,284 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065871820, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,289 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,289 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,293 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40313]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:52,296 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,296 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,302 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40313 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,303 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,303 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,306 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,308 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,309 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,312 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,314 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,321 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,321 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065871810, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,322 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,323 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,323 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065871817, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,323 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065871820, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,324 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:37:52,328 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,328 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,332 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40313]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:52,335 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,335 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,342 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40313 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,343 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,343 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,346 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,348 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,350 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,352 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,353 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,359 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,360 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065871810, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,360 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,361 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,361 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065871817, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,361 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065871820, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,362 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:37:52,365 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,366 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,370 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40313]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:52,372 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,372 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:52,380 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40313 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,380 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,381 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,383 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,385 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,387 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,389 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:52,389 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:52,396 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,397 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065871810, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,397 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,398 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:52,398 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065871817, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,398 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065871820, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:52,399 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:37:52,407 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,408 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,411 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:37:52,411 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:37:52,416 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:37:52,417 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:37:52,419 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:52,423 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:52,423 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:52,424 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:52,425 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:37:52,425 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,472 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,539 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,539 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,540 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:52,540 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:52,540 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:37:52,540 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:52,540 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:52,540 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:52,540 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:52,540 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,540 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,543 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,543 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,543 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,573 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,584 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,584 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,585 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:52,585 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:37:52,585 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:52,585 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:52,585 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:52,585 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:52,585 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:52,585 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,624 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,666 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,666 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,666 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,723 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,734 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,734 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,734 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,735 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,735 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:52,762 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:37:52,762 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:37:52,762 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:37:52,762 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:37:52,762 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:37:52,762 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:37:52,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:52,920 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:37:52,920 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:52,920 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:52,920 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:52,921 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:52,921 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:52,921 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:37:52,921 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:37:52,921 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:37:52,924 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:37:52,924 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:52,925 INFO   ||  Processed session termination for sessionid: 0x1670a49d9870000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:52,934 INFO   ||  Session: 0x1670a49d9870000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:52,935 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:52,935 INFO   ||  EventThread shut down for session: 0x1670a49d9870000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:52,935 INFO   ||  Closed socket connection for client /127.0.0.1:41996 which had sessionid 0x1670a49d9870000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:37:53,225 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,265 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,415 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,415 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,415 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,417 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,417 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,417 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,418 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,418 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,418 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:37:53,449 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:37:53,451 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:37:53,452 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:53,452 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:53,452 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:53,452 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:53,453 INFO   ||  Stopped Kafka server 1 at localhost:40313   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:53,454 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:53,454 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,454 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,454 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:53,455 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,455 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:53,455 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,455 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:53,455 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:37:53,459 INFO   ||  binding to port localhost/127.0.0.1:46209   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:53,459 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,464 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,465 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 38107
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46209
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:53,466 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:53,466 INFO   ||  Connecting to zookeeper on localhost:46209   [kafka.server.KafkaServer]
2018-11-12 18:37:53,467 INFO   ||  Initiating client connection, connectString=localhost:46209 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@399ba5c0   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:53,467 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:53,468 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:53,476 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46209. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:53,476 INFO   ||  Socket connection established to localhost/127.0.0.1:46209, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:53,476 INFO   ||  Accepted socket connection from /127.0.0.1:45996   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:53,477 INFO   ||  Client attempting to establish new session at /127.0.0.1:45996   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,477 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:53,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:53,509 INFO   ||  Established session 0x1670a49e2350000 with negotiated timeout 6000 for client /127.0.0.1:45996   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:53,509 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46209, sessionid = 0x1670a49e2350000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:53,509 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:53,519 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,543 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,573 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,579 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,612 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,615 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,632 INFO   ||  Cluster ID = ZSAgLI7XRI2tPCLmZ8rEWA   [kafka.server.KafkaServer]
2018-11-12 18:37:53,633 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:53,634 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,638 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,640 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:53,640 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:53,641 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:53,641 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:37:53,705 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:53,705 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:53,706 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:53,707 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:53,734 INFO   ||  Awaiting socket connections on localhost:38107.   [kafka.network.Acceptor]
2018-11-12 18:37:53,735 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:53,736 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,737 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,738 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,743 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:53,743 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:53,744 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,744 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:53,744 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,744 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:53,744 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:53,744 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:53,747 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:53,783 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:53,788 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:53,790 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:53,791 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:53,791 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:53,791 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:53,791 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:53,792 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,800 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,810 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:53,811 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,811 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,811 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:53,811 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:53,811 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:53,812 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:53,812 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:53,812 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,813 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:delete cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,813 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,824 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:53,824 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:53,825 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,38107,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:53,825 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:53,827 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:37:53,828 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:53,829 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:37:53,830 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:53,831 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:38107 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:53,850 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,850 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,851 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:53,851 INFO   ||  Started Kafka server 1 at localhost:38107 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:53,851 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38107, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:53,851 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38107, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:53,852 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:38107]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:53,854 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,854 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,855 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38107]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,856 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,856 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,856 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:53,970 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:53,971 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38107]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:53,972 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,972 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:53,972 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:53,973 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:setData cxid:0x53 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,980 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x54 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:53,992 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:54,000 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:54,000 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:54,000 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:54,000 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:54,001 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:54,001 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:54,002 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x5c zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,004 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x5d zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,028 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:54,030 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:54,032 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:54,033 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:37:54,034 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:54,035 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:54,035 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:54,035 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:54,089 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:54,092 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:54,092 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,095 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,095 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,107 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,112 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,128 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,140 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:54,148 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:54,150 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:54,150 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:54,150 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:54,150 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:54,151 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:54,151 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:54,152 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,154 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49e2350000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:54,178 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:54,178 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,180 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:54,182 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:54,183 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:37:54,184 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:54,184 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:54,184 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:54,184 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:54,185 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:54,186 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:54,203 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38107 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,204 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,204 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,207 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,208 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,210 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,211 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:54,251 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,254 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,432 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,606 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,606 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,621 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:54,704 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:54,706 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38107, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:54,706 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38107, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:54,707 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,709 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,709 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,716 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38107 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,716 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,716 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,719 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,720 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,722 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,724 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,724 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,734 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,734 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,738 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,740 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,740 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,747 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38107 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,748 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,748 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,750 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,751 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,753 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,755 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,755 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,764 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,764 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,768 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,770 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,770 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,776 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38107 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,777 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,777 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,779 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,780 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,782 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,784 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,785 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,794 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,794 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,799 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,801 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,801 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,807 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38107 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,808 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,808 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,810 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,813 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,814 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,816 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:54,817 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:54,830 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,830 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,834 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,836 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,836 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,847 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38107, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:54,847 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38107, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:54,848 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:38107]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:54,850 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,850 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,850 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38107]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:54,852 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,852 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:54,857 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:54,858 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:37:54,858 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:37:54,861 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:37:54,862 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:37:54,863 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:54,867 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:54,867 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:54,868 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:54,870 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:37:54,870 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:54,944 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:54,944 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:54,945 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:54,945 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:54,945 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:37:54,945 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:54,945 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:54,945 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:54,945 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:54,945 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:54,946 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:54,966 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,967 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:54,981 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,117 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,144 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,144 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,144 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,181 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,213 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,213 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,213 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:55,213 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:37:55,213 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:55,213 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:55,213 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:55,213 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:55,213 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:55,213 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,227 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,227 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,227 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,268 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,318 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,337 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,337 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,337 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,339 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,339 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,364 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:37:55,364 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:37:55,364 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:37:55,364 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:37:55,364 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:37:55,364 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:37:55,424 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,468 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:37:55,468 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:55,468 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:55,468 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:55,469 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:55,469 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:55,469 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:37:55,469 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:37:55,469 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:37:55,471 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:37:55,471 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:55,472 INFO   ||  Processed session termination for sessionid: 0x1670a49e2350000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,485 INFO   ||  Session: 0x1670a49e2350000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:55,485 INFO   ||  Closed socket connection for client /127.0.0.1:45996 which had sessionid 0x1670a49e2350000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:37:55,485 INFO   ||  EventThread shut down for session: 0x1670a49e2350000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:55,485 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,486 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,620 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,635 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,635 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,635 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,638 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,638 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,638 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,640 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,640 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,640 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:37:55,669 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:37:55,670 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,671 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:37:55,672 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:55,672 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:55,672 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:55,672 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:55,673 INFO   ||  Stopped Kafka server 1 at localhost:38107   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:55,673 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:55,674 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,674 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,674 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:55,674 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,674 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:55,674 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,674 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:55,674 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:37:55,678 INFO   ||  binding to port localhost/127.0.0.1:44029   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:55,679 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,684 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,685 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 36175
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:44029
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:55,686 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:55,687 INFO   ||  Connecting to zookeeper on localhost:44029   [kafka.server.KafkaServer]
2018-11-12 18:37:55,687 INFO   ||  Initiating client connection, connectString=localhost:44029 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@25c78bf   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:55,687 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:55,689 INFO   ||  Opening socket connection to server localhost/127.0.0.1:44029. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:55,689 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:55,689 INFO   ||  Socket connection established to localhost/127.0.0.1:44029, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:55,690 INFO   ||  Accepted socket connection from /127.0.0.1:47296   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:55,690 INFO   ||  Client attempting to establish new session at /127.0.0.1:47296   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,690 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:55,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:55,722 INFO   ||  Established session 0x1670a49eadf0000 with negotiated timeout 6000 for client /127.0.0.1:47296   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:55,722 INFO   ||  Session establishment complete on server localhost/127.0.0.1:44029, sessionid = 0x1670a49eadf0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:55,722 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:55,733 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,756 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,787 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,829 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,846 INFO   ||  Cluster ID = V7p2JoZRTJGlJS53D_VAsQ   [kafka.server.KafkaServer]
2018-11-12 18:37:55,846 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:55,848 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,848 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,849 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:55,850 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:55,850 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:55,851 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-11-12 18:37:55,928 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:55,928 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:55,929 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:55,929 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:55,950 INFO   ||  Awaiting socket connections on localhost:36175.   [kafka.network.Acceptor]
2018-11-12 18:37:55,952 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:55,953 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,953 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,954 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,956 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:55,958 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:55,960 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,960 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,960 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:55,960 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:55,961 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:55,961 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:55,961 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:55,966 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:55,967 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:55,967 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:55,972 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:55,972 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:55,979 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:55,979 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:55,980 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:56,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:56,035 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,038 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:56,049 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:56,050 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,051 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,051 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:56,051 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:56,051 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,051 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:56,051 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x49 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,052 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,056 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,056 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,056 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,056 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:56,056 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:56,056 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,056 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:delete cxid:0x4d zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,062 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:56,062 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,36175,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:56,062 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:56,068 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:56,070 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:37:56,073 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:37:56,074 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:56,077 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:36175 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:56,084 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,089 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,089 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,089 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:56,089 INFO   ||  Started Kafka server 1 at localhost:36175 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:56,092 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,098 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,110 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:56,116 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:36175]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:56,117 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:56,118 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:56,118 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:56,118 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,119 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,119 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,119 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:56,125 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,126 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,128 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,130 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,134 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,152 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:56,153 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:56,156 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:56,156 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:37:56,158 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:56,158 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:56,158 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:56,158 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:56,237 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:56,259 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:56,262 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36175, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,262 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36175, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,262 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:36175]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:56,264 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,264 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,264 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36175]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,265 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,265 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,266 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,266 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,266 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:56,379 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,379 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36175]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:56,381 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,381 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,381 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:56,384 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:56,422 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,489 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,492 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,493 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:56,494 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36175]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:56,498 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,498 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,507 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,517 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,527 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,529 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:56,552 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:56,554 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:56,554 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:56,554 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:56,554 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,555 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:56,555 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:56,556 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,565 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49eadf0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:56,579 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,589 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:56,591 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:56,593 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:56,594 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:37:56,595 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:56,595 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:56,595 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:56,595 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:56,596 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:56,597 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:56,602 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36175 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,603 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,603 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,606 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,608 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,609 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,611 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:56,638 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,639 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,652 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,653 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,671 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:37:56,673 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36175, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,673 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36175, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:56,673 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36175]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:56,675 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,675 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36175 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,684 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,686 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,687 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,690 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,691 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,700 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,701 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,705 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36175]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:56,707 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,707 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:56,713 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36175 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,713 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,713 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,716 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,717 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,718 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,719 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:56,720 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:56,726 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1542065876235, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-11-12 18:37:56,727 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,728 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,730 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:37:56,730 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:37:56,733 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:37:56,734 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:37:56,735 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:56,737 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:37:56,738 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:56,738 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:37:56,739 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:37:56,739 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,760 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,760 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,760 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:56,761 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:56,761 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:37:56,761 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:56,761 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:56,761 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:56,761 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:56,761 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:56,761 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,787 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,788 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,888 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,939 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:56,960 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,960 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,960 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:56,987 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,089 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,116 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,116 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,117 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:57,117 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:37:57,117 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:57,117 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:57,117 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:57,117 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:57,117 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:57,117 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,138 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,139 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,186 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,186 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,186 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,338 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,353 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,353 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,354 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,355 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,355 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:57,404 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:37:57,404 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:37:57,404 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:37:57,404 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:37:57,404 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:37:57,404 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:37:57,490 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,514 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:37:57,514 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:57,514 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:57,514 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:57,515 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:57,515 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:57,515 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:37:57,515 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:37:57,515 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:37:57,519 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:37:57,519 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:57,519 INFO   ||  Processed session termination for sessionid: 0x1670a49eadf0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:57,530 INFO   ||  Closed socket connection for client /127.0.0.1:47296 which had sessionid 0x1670a49eadf0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:37:57,530 INFO   ||  Session: 0x1670a49eadf0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:57,530 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,530 INFO   ||  EventThread shut down for session: 0x1670a49eadf0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:57,591 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,643 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,682 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,729 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,832 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:57,848 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,848 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,848 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:57,849 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:37:57,867 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:37:57,869 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:37:57,869 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:57,869 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:57,870 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:37:57,870 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:37:57,870 INFO   ||  Stopped Kafka server 1 at localhost:36175   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:57,870 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:57,871 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,871 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,871 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:57,871 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:57,871 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:57,871 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:57,871 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:37:57,872 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:37:57,875 INFO   ||  binding to port localhost/127.0.0.1:36645   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:57,876 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,877 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,878 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40885
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:36645
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:37:57,879 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:37:57,879 INFO   ||  Connecting to zookeeper on localhost:36645   [kafka.server.KafkaServer]
2018-11-12 18:37:57,880 INFO   ||  Initiating client connection, connectString=localhost:36645 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@35f1cea6   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:37:57,880 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:37:57,880 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:57,881 INFO   ||  Opening socket connection to server localhost/127.0.0.1:36645. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:57,881 INFO   ||  Socket connection established to localhost/127.0.0.1:36645, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:57,881 INFO   ||  Accepted socket connection from /127.0.0.1:58786   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:37:57,881 INFO   ||  Client attempting to establish new session at /127.0.0.1:58786   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,882 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:37:57,912 INFO   ||  Established session 0x1670a49f3740000 with negotiated timeout 6000 for client /127.0.0.1:58786   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:37:57,912 INFO   ||  Session establishment complete on server localhost/127.0.0.1:36645, sessionid = 0x1670a49f3740000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:37:57,913 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:37:57,923 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:57,947 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:57,977 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:58,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:37:58,020 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:58,036 INFO   ||  Cluster ID = 3T80f0wIT7i8guhtHv4R7w   [kafka.server.KafkaServer]
2018-11-12 18:37:58,037 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:58,041 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:58,041 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:58,041 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:58,041 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:37:58,042 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:37:58,043 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:37:58,043 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:37:59,031 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,031 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,031 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,033 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:37:59,033 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,034 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:37:59,034 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:37:59,035 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:37:59,055 INFO   ||  Awaiting socket connections on localhost:40885.   [kafka.network.Acceptor]
2018-11-12 18:37:59,056 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:37:59,058 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,060 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,061 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,062 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:37:59,064 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:37:59,067 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,068 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,068 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:37:59,069 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:59,069 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,069 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,069 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:59,079 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:59,079 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:37:59,079 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:37:59,080 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,085 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:37:59,092 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:59,092 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:37:59,092 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:37:59,097 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:37:59,102 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:59,102 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,103 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x47 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,115 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:37:59,115 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40885,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:37:59,116 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,116 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:37:59,117 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,117 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,117 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:59,117 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,117 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:37:59,118 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,118 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,121 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:37:59,123 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:37:59,125 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:37:59,126 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:37:59,131 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40885 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:37:59,132 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,147 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,147 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,147 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:37:59,147 INFO   ||  Started Kafka server 1 at localhost:40885 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:37:59,150 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,199 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,211 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:59,217 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40885, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:59,217 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40885, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:59,218 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40885]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:37:59,218 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:59,219 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:59,219 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:37:59,219 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,219 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:59,220 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,220 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,220 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,221 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40885]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,221 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,222 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,222 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,222 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:59,223 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,247 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:59,248 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:59,251 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:59,251 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:37:59,252 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:59,253 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:37:59,253 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:59,253 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:59,331 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:59,332 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40885]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,333 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:59,334 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:37:59,334 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,334 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,334 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:37:59,441 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:37:59,443 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40885]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:37:59,443 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:37:59,445 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,445 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:37:59,453 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,456 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,468 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:37:59,489 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:37:59,491 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:37:59,491 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:59,492 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:37:59,492 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,492 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:59,492 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:37:59,493 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,504 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a49f3740000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:37:59,528 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:37:59,529 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:37:59,532 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:37:59,532 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:37:59,533 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:37:59,533 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:37:59,533 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:37:59,533 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:37:59,534 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:59,535 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:37:59,549 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40885 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:59,550 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:59,550 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:59,553 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,554 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,555 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,557 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:59,582 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:37:59,582 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:37:59,934 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,934 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:37:59,952 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,952 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:37:59,965 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:37:59,985 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,000 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:00,002 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40885, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:00,002 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40885, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:00,002 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40885]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:00,004 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,004 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,010 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40885 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,011 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,011 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,013 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,014 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,015 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,017 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,017 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,027 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,027 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,030 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40885]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:00,031 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,031 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,035 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,037 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40885 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,038 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,038 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,040 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,041 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,042 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,044 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,044 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,051 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,051 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,054 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40885]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:00,055 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,055 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,061 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40885 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,062 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,062 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,064 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,065 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,066 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,069 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,076 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,076 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,079 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40885]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:00,080 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,080 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:00,087 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40885 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,087 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,087 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,094 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,095 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,096 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,098 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:00,098 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:00,105 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,105 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,108 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:00,108 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:00,111 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:00,112 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:00,113 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:00,115 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:00,115 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:00,116 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:00,117 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:00,117 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,135 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,136 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,166 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,166 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,186 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,236 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,236 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,236 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,267 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,267 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,267 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:00,268 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:00,268 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:00,268 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:00,268 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:00,268 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:00,268 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:00,268 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,268 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,268 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,268 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,268 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,295 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,295 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,295 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:00,295 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:00,295 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:00,295 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:00,295 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:00,295 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:00,295 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:00,295 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,317 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,467 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,467 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,467 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,659 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,659 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,659 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,661 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,661 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:00,683 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:00,683 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:00,684 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:00,684 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:00,684 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:00,684 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:00,782 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:00,782 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:00,782 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:00,782 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:00,783 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:00,783 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:00,783 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:00,783 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:00,783 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:00,784 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:00,784 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:00,785 INFO   ||  Processed session termination for sessionid: 0x1670a49f3740000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:00,799 INFO   ||  Session: 0x1670a49f3740000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:00,799 INFO   ||  Closed socket connection for client /127.0.0.1:58786 which had sessionid 0x1670a49f3740000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:00,799 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:00,799 INFO   ||  EventThread shut down for session: 0x1670a49f3740000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:00,869 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,968 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,987 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,987 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:00,988 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,037 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,037 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,038 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,042 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:01,042 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:01,042 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:01,088 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,287 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,670 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,939 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:01,990 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,042 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:02,042 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:02,042 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:02,140 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,140 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,140 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,189 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,190 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,239 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,541 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,592 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,622 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,773 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:02,992 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,042 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:03,043 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,042 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:03,043 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:03,058 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:03,060 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:03,060 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:03,061 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:03,061 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:03,061 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:03,061 INFO   ||  Stopped Kafka server 1 at localhost:40885   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:03,062 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:03,063 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,063 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,063 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:03,063 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,063 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,086 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:03,086 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:03,086 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:03,090 INFO   ||  binding to port localhost/127.0.0.1:46323   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:03,090 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,093 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,099 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,100 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 39373
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46323
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:03,101 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:03,101 INFO   ||  Connecting to zookeeper on localhost:46323   [kafka.server.KafkaServer]
2018-11-12 18:38:03,101 INFO   ||  Initiating client connection, connectString=localhost:46323 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@52e373c3   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:03,101 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:03,102 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:03,102 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46323. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:03,102 INFO   ||  Socket connection established to localhost/127.0.0.1:46323, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:03,103 INFO   ||  Accepted socket connection from /127.0.0.1:44032   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:03,103 INFO   ||  Client attempting to establish new session at /127.0.0.1:44032   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,103 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:03,133 INFO   ||  Established session 0x1670a4a07d40000 with negotiated timeout 6000 for client /127.0.0.1:44032   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:03,133 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46323, sessionid = 0x1670a4a07d40000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:03,133 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:03,143 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,144 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,168 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,243 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,252 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,293 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,294 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,311 INFO   ||  Cluster ID = 9Pn5acRgSoOulWNyrpFVag   [kafka.server.KafkaServer]
2018-11-12 18:38:03,311 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:03,313 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:03,314 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:03,314 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:03,315 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:03,316 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:03,316 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:03,343 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,344 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,382 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:03,383 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:03,384 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:03,387 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:03,400 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,405 INFO   ||  Awaiting socket connections on localhost:39373.   [kafka.network.Acceptor]
2018-11-12 18:38:03,406 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:03,407 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,408 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,411 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,418 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:03,419 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:03,419 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,420 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,420 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:03,421 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:03,421 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:03,421 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:03,423 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:03,425 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:03,431 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:03,432 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:03,432 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:03,433 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,433 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:03,433 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:03,434 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:03,443 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:03,449 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:03,450 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x45 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,450 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,461 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:03,461 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,39373,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:03,462 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:03,465 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,466 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,467 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:03,467 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,467 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:39373 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:03,468 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:03,468 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,469 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,480 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,480 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,480 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:03,480 INFO   ||  Started Kafka server 1 at localhost:39373 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:03,491 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:03,493 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:03,494 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,497 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:03,509 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:03,515 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:39373]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:03,517 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:03,517 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:03,517 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:03,517 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,517 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,517 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,518 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:03,519 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,520 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,522 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,527 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,544 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,551 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:03,552 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:03,555 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:03,555 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:03,557 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:03,557 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:03,557 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:03,557 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:03,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,626 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,627 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:03,653 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:03,655 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39373, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:03,656 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39373, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:03,656 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:39373]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:03,658 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,658 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,658 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39373]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,659 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,659 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,659 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:03,768 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:03,768 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:39373]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:03,769 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:03,769 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,770 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,770 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:03,877 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:03,879 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:03,879 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39373]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:03,881 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,881 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:03,889 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,899 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,910 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:03,931 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:03,933 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:03,934 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:03,934 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:03,934 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,934 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:03,935 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:03,935 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,947 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a07d40000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:03,971 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:03,972 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:03,974 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:03,975 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:03,975 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:03,976 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:03,976 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:03,976 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:03,976 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:03,978 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:03,986 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:03,986 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:03,987 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:03,995 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,996 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:03,997 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:03,998 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:03,999 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,001 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:04,024 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,029 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,034 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,035 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065883645, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,036 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,036 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,036 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065883650, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,037 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065883651, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,038 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,038 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,058 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:04,060 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:39373, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,060 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:39373, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,061 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39373]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:04,062 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,062 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,067 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,070 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,071 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,073 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,074 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,075 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,081 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,082 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065883645, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,083 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,083 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,083 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065883650, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,084 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065883651, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,086 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,086 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,089 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39373]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:04,091 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,091 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,095 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,096 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,096 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,098 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,099 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,100 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,102 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,103 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,107 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,108 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065883645, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,108 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,109 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,109 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065883650, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,109 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065883651, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,110 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:04,112 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,112 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,115 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39373]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:04,117 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,117 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,122 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,122 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,122 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,125 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,126 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,127 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,129 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,129 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,135 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,135 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065883645, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,136 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,136 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,136 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065883650, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,137 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065883651, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,137 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:04,140 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,140 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,142 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:39373]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:04,144 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,144 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:04,145 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,147 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,149 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:39373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,150 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,150 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,152 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,153 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,154 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,155 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:04,155 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:04,160 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,160 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065883645, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,161 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,161 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:04,161 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065883650, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,162 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065883651, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:04,164 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:04,166 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,166 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,169 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:04,169 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:04,172 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:04,172 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:04,173 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:04,176 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:04,176 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:04,176 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:04,177 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:04,177 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,220 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,220 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,220 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:04,220 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:04,220 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:04,220 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:04,220 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:04,220 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:04,221 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:04,221 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,221 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,246 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,246 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,275 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,276 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,295 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,398 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,420 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,420 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,420 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,454 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,479 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,553 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,553 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,553 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:04,553 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:04,553 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:04,553 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:04,553 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:04,553 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:04,553 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:04,553 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,566 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,566 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,566 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,608 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,608 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,608 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,611 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,611 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:04,640 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:04,640 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:04,640 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:04,640 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:04,640 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:04,640 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:04,697 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,744 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:04,744 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:04,744 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:04,744 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:04,744 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:04,744 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:04,745 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:04,745 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:04,745 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:04,745 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:04,745 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:04,746 INFO   ||  Processed session termination for sessionid: 0x1670a4a07d40000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:04,761 INFO   ||  Session: 0x1670a4a07d40000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:04,762 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:04,762 INFO   ||  EventThread shut down for session: 0x1670a4a07d40000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:04,762 INFO   ||  Closed socket connection for client /127.0.0.1:44032 which had sessionid 0x1670a4a07d40000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:04,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:04,847 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,028 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,047 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,049 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,078 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,199 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,247 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,248 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,249 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,313 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,313 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,314 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,315 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,315 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,315 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:05,331 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,450 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,456 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,880 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,901 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:05,982 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,030 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,049 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,049 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,201 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,201 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,249 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,252 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,299 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,315 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:06,315 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:06,315 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:06,329 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:06,330 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:06,330 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:06,330 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:06,331 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:06,331 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:06,331 INFO   ||  Stopped Kafka server 1 at localhost:39373   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:06,331 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:06,331 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,331 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,331 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:06,331 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,331 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:06,331 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,331 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:06,332 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:06,334 INFO   ||  binding to port localhost/127.0.0.1:34759   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:06,335 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,336 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,337 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35473
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:34759
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:06,338 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:06,338 INFO   ||  Connecting to zookeeper on localhost:34759   [kafka.server.KafkaServer]
2018-11-12 18:38:06,339 INFO   ||  Initiating client connection, connectString=localhost:34759 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@62875e1f   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:06,339 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:06,339 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:06,340 INFO   ||  Opening socket connection to server localhost/127.0.0.1:34759. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:06,340 INFO   ||  Socket connection established to localhost/127.0.0.1:34759, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:06,340 INFO   ||  Accepted socket connection from /127.0.0.1:59098   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:06,340 INFO   ||  Client attempting to establish new session at /127.0.0.1:59098   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,341 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:06,402 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,417 INFO   ||  Established session 0x1670a4a147f0000 with negotiated timeout 6000 for client /127.0.0.1:59098   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:06,418 INFO   ||  Session establishment complete on server localhost/127.0.0.1:34759, sessionid = 0x1670a4a147f0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:06,418 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:06,431 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,454 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,483 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,484 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:06,527 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,544 INFO   ||  Cluster ID = Yw4BE4M3QnmGRaRYt-Y30w   [kafka.server.KafkaServer]
2018-11-12 18:38:06,544 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:06,546 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:06,546 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:06,546 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:06,547 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:06,548 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:06,548 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:06,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,613 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:06,613 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:06,614 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:06,615 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:06,633 INFO   ||  Awaiting socket connections on localhost:35473.   [kafka.network.Acceptor]
2018-11-12 18:38:06,634 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:06,634 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,634 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,635 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,636 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:06,637 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:06,638 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,638 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,638 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:06,639 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:06,639 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:06,640 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:06,640 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:06,646 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:06,647 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:06,647 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:06,652 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:06,653 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,654 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:06,654 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:06,658 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:06,664 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:06,668 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:06,669 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x44 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,669 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x45 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,676 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:06,676 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,35473,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:06,676 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:06,680 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:06,680 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:06,681 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,681 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,681 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:06,681 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,681 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:35473 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:06,683 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:06,683 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,683 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,694 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:06,696 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:06,705 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,705 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,705 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:06,705 INFO   ||  Started Kafka server 1 at localhost:35473 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:06,706 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35473, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:06,706 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35473, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:06,706 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:35473]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:06,708 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,708 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,708 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35473]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,709 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,709 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,709 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:06,816 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:06,816 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35473]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,817 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:06,818 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,818 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,818 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:06,818 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:setData cxid:0x54 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,826 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x55 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,851 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,886 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:06,893 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:06,893 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:06,893 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:06,893 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,894 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:06,894 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,895 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x5d zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,898 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x5e zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,922 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:06,923 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:06,925 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:06,926 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:06,926 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:06,927 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:06,927 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:06,928 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:06,928 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:06,928 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:06,928 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:06,929 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,929 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:06,937 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:setData cxid:0x68 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,951 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,952 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x69 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,976 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:06,984 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:06,987 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:06,989 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:06,989 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:06,990 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:06,990 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,990 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:06,991 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:06,991 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x71 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:06,994 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a147f0000 type:create cxid:0x72 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:07,003 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,018 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:07,019 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:07,021 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:07,022 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:07,022 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:07,022 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:07,022 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:07,022 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:07,023 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:07,024 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:07,032 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,034 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,034 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,034 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,055 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,057 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,058 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,059 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:07,080 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,080 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,104 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,254 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,302 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,354 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,435 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,436 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,446 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:07,472 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:07,474 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35473, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:07,474 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35473, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:07,474 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,476 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,476 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,481 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,482 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,482 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,484 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,486 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,487 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,488 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,489 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,494 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,494 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,497 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,499 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,499 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,502 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,504 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,504 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,504 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,506 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,507 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,508 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,509 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,509 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,515 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,515 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,518 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,519 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,519 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,526 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,527 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,528 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,529 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,530 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,535 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,536 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,538 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,540 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,540 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35473 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,547 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,548 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,549 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,551 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:07,551 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:07,557 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,557 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,560 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,562 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,562 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,571 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35473, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:07,571 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35473, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:07,571 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:35473]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:07,573 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,573 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,574 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35473]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:07,576 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,576 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:07,581 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:07,582 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:07,582 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:07,585 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:07,586 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:07,587 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:07,591 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:07,591 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:07,591 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:07,593 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:07,593 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,605 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,639 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,639 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,639 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:07,640 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:07,640 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:07,640 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:07,640 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:07,640 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:07,640 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:07,640 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,640 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,640 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,689 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,691 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,702 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,790 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,839 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,839 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,839 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,841 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,884 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,903 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,936 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:07,947 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,947 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,948 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:07,948 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:07,948 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:07,948 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:07,948 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:07,948 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:07,948 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:07,948 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,986 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,986 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:07,987 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,035 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,035 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,035 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,039 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,039 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,041 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,042 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,063 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:08,063 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:08,063 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:08,063 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:08,063 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:08,063 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:08,154 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,155 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:08,155 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:08,155 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:08,155 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:08,156 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:08,156 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:08,156 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:08,156 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:08,156 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:08,156 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,157 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:08,157 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:08,158 INFO   ||  Processed session termination for sessionid: 0x1670a4a147f0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,168 INFO   ||  Closed socket connection for client /127.0.0.1:59098 which had sessionid 0x1670a4a147f0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:08,169 INFO   ||  Session: 0x1670a4a147f0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:08,169 INFO   ||  EventThread shut down for session: 0x1670a4a147f0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:08,169 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,185 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,206 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,258 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,308 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,392 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,404 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,493 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,546 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,547 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,547 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,547 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:08,560 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:08,561 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:08,561 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:08,562 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:08,562 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:08,562 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:08,562 INFO   ||  Stopped Kafka server 1 at localhost:35473   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:08,563 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:08,563 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,563 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,563 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:08,563 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,563 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:08,563 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,563 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:08,563 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:08,564 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,567 INFO   ||  binding to port localhost/127.0.0.1:40969   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:08,568 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,569 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,570 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 41585
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:40969
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:08,571 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:08,571 INFO   ||  Connecting to zookeeper on localhost:40969   [kafka.server.KafkaServer]
2018-11-12 18:38:08,572 INFO   ||  Initiating client connection, connectString=localhost:40969 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@719c40d4   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:08,572 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:08,572 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:08,573 INFO   ||  Opening socket connection to server localhost/127.0.0.1:40969. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:08,573 INFO   ||  Socket connection established to localhost/127.0.0.1:40969, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:08,573 INFO   ||  Accepted socket connection from /127.0.0.1:48114   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:08,573 INFO   ||  Client attempting to establish new session at /127.0.0.1:48114   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,573 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:08,600 INFO   ||  Established session 0x1670a4a1d380000 with negotiated timeout 6000 for client /127.0.0.1:48114   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:08,600 INFO   ||  Session establishment complete on server localhost/127.0.0.1:40969, sessionid = 0x1670a4a1d380000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:08,601 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:08,615 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,638 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,716 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,758 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,759 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,775 INFO   ||  Cluster ID = CodfrWepQAG-_EiZUwQLTQ   [kafka.server.KafkaServer]
2018-11-12 18:38:08,776 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:08,777 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,778 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,778 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:08,780 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:08,780 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:08,781 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-11-12 18:38:08,794 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,856 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,856 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,857 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:08,857 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:08,858 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:08,858 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:08,880 INFO   ||  Awaiting socket connections on localhost:41585.   [kafka.network.Acceptor]
2018-11-12 18:38:08,881 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:08,882 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,882 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,882 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,884 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:08,885 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:08,886 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,886 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,886 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:08,887 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:08,887 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:08,887 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:08,887 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:08,890 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:08,890 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:08,890 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:08,895 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:08,896 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,902 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:08,902 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:08,903 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:08,907 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:08,912 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:08,913 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x44 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,913 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x45 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,925 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:08,926 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,41585,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:08,926 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:08,930 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:08,930 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:08,931 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,931 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,931 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:08,931 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:08,931 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:08,932 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,932 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,932 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:41585 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:08,932 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,932 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:08,932 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:08,933 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:08,933 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,938 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,948 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:08,948 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:08,948 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:08,948 INFO   ||  Started Kafka server 1 at localhost:41585 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:08,955 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:08,957 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:08,958 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,961 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,973 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:08,979 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41585]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:08,981 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:08,981 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:08,981 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:08,981 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:08,981 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:08,981 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:08,981 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:08,983 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:08,984 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:08,985 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,987 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:08,991 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:09,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:09,016 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:09,016 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:09,019 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:09,019 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:09,021 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:09,021 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:09,021 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:09,021 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:09,090 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:09,112 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:09,115 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41585, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,115 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41585, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,115 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:41585]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:09,118 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,118 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,118 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41585]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,119 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,119 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,119 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:09,194 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,195 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,208 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,228 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,228 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41585]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:09,229 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,229 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,229 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:09,236 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:09,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,259 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,335 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,337 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:09,337 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41585]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:09,339 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,339 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,349 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:09,357 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:09,360 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,369 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:09,387 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,388 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:09,389 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:09,389 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:09,390 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:09,390 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:09,390 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:09,391 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:09,391 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:09,399 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a1d380000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:09,423 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:09,424 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:09,426 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:09,427 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:09,427 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:09,428 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:09,428 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:09,428 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:09,428 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:09,429 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:09,445 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41585 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,446 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,446 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,448 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,450 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,451 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,452 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:09,457 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,480 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,480 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,486 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,486 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,504 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:09,506 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41585, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,506 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41585, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:09,506 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41585]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:09,509 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,509 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,514 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41585 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,514 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,514 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,520 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,521 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,522 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,524 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,530 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,530 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,533 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41585]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:09,535 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,535 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:09,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41585 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,540 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,543 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,544 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,545 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,546 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:09,547 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:09,551 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1542065889088, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-11-12 18:38:09,552 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,552 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,555 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:09,555 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:09,558 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:09,559 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:09,560 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:09,562 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:09,562 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:09,563 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:09,564 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:09,564 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,613 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,686 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,686 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,686 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:09,686 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:09,686 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:09,686 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:09,687 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:09,687 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:09,687 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:09,687 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,687 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,711 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,808 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,812 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,886 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,886 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,887 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,938 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:09,943 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,943 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,943 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:09,943 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:09,944 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:09,944 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:09,944 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:09,944 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:09,944 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:09,944 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:09,946 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,008 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,027 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,027 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,027 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,046 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,063 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,082 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,082 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,082 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,083 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,083 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:10,107 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:10,108 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:10,108 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:10,108 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:10,108 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:10,108 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:10,140 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,146 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,212 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:10,212 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:10,212 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:10,212 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:10,212 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:10,212 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:10,212 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:10,212 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:10,213 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:10,213 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:10,213 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:10,215 INFO   ||  Processed session termination for sessionid: 0x1670a4a1d380000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,226 INFO   ||  Session: 0x1670a4a1d380000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:10,226 INFO   ||  EventThread shut down for session: 0x1670a4a1d380000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:10,226 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,227 INFO   ||  Closed socket connection for client /127.0.0.1:48114 which had sessionid 0x1670a4a1d380000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:10,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,489 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,521 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,714 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,777 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,778 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,778 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,778 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,778 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,778 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,779 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,779 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:10,779 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:10,796 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:10,797 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:10,797 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:10,798 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:10,798 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:10,798 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:10,798 INFO   ||  Stopped Kafka server 1 at localhost:41585   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:10,799 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:10,799 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,799 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,799 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:10,799 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,799 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:10,799 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,799 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:10,799 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:10,802 INFO   ||  binding to port localhost/127.0.0.1:37453   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:10,803 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,806 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,807 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 46003
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:37453
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:10,808 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:10,808 INFO   ||  Connecting to zookeeper on localhost:37453   [kafka.server.KafkaServer]
2018-11-12 18:38:10,808 INFO   ||  Initiating client connection, connectString=localhost:37453 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4bc20bfb   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:10,808 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:10,809 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:10,809 INFO   ||  Opening socket connection to server localhost/127.0.0.1:37453. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:10,810 INFO   ||  Socket connection established to localhost/127.0.0.1:37453, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:10,810 INFO   ||  Accepted socket connection from /127.0.0.1:38260   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:10,810 INFO   ||  Client attempting to establish new session at /127.0.0.1:38260   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,810 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:10,837 INFO   ||  Established session 0x1670a4a25f30000 with negotiated timeout 6000 for client /127.0.0.1:38260   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:10,837 INFO   ||  Session establishment complete on server localhost/127.0.0.1:37453, sessionid = 0x1670a4a25f30000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:10,837 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:10,847 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,870 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,897 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:10,900 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,990 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:10,998 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:11,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:11,008 INFO   ||  Cluster ID = sLnNy6bXRQugV4pDfA4V-Q   [kafka.server.KafkaServer]
2018-11-12 18:38:11,008 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:11,009 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:11,010 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:11,011 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:11,011 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:11,012 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:11,012 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:11,013 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-11-12 18:38:12,240 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,240 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,302 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,302 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,302 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,306 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:12,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:12,307 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:12,308 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:12,309 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:12,325 INFO   ||  Awaiting socket connections on localhost:46003.   [kafka.network.Acceptor]
2018-11-12 18:38:12,326 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:12,326 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,327 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,327 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,329 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:12,330 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:12,330 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,331 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,331 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:12,331 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:12,331 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:12,332 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:12,332 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:12,338 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:12,338 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:12,338 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:12,344 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,344 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:12,356 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:12,356 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:12,356 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:12,357 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:12,363 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,364 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:12,365 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,365 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:12,365 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:12,365 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,365 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,365 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,365 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:12,365 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,366 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:12,366 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,366 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,366 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:delete cxid:0x4d zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,374 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:12,374 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:12,374 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,46003,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:12,375 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:12,379 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:12,381 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:38:12,386 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:12,387 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:46003 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:12,395 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,395 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,396 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:12,396 INFO   ||  Started Kafka server 1 at localhost:46003 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:12,399 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,410 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,422 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:12,428 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:46003, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:12,428 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:46003, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:12,429 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:46003]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:12,429 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:12,429 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:12,430 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:12,430 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,430 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:12,431 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,431 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,431 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,432 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,432 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:46003]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,433 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,433 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,433 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:12,435 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,458 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:12,459 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:12,462 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:12,462 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:12,463 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:12,464 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:12,464 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:12,464 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:12,539 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:12,540 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:46003]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:12,541 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:12,541 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,541 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,541 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:12,647 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:12,648 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46003]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:12,648 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:12,650 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,650 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:12,657 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,661 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,673 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:12,697 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:12,698 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:12,698 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:12,698 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:12,698 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,699 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:12,699 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:12,700 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,709 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a25f30000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:12,734 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:12,734 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:12,737 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:12,737 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:12,738 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:12,738 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:12,738 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:12,738 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:12,738 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:12,739 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:12,754 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:46003 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:12,755 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:12,755 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:12,756 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:12,758 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:12,759 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:12,760 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:12,806 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:12,806 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,156 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,156 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,165 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:13,196 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:13,198 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:46003, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:13,198 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:46003, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:13,199 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46003]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:13,200 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,200 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,205 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:46003 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,205 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,205 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,207 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,207 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,208 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,209 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,209 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,209 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,210 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,211 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,216 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,216 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,219 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46003]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:13,221 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,221 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,226 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:46003 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,227 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,227 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,229 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,229 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,231 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,232 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,232 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,239 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,239 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,242 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46003]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:13,244 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,244 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,249 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:46003 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,249 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,250 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,251 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,252 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,252 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,254 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,254 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,255 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,259 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,262 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,262 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,265 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46003]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:13,267 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,267 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:13,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:46003 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,274 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,274 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,275 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,277 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:13,277 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:13,282 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,283 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,285 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:13,285 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:13,288 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:13,289 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:13,290 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:13,292 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:13,292 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:13,293 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:13,294 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:13,294 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,331 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,331 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,331 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:13,331 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:13,331 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:13,331 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:13,331 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:13,331 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:13,331 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:13,331 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,332 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,359 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,391 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,393 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,407 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,410 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,457 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,460 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,460 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,505 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,509 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,531 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,531 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,531 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,542 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,544 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,674 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,674 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,675 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:13,675 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:13,675 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:13,675 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:13,675 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:13,675 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:13,675 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:13,675 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,707 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,707 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,707 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,727 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,727 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,727 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,727 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,727 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:13,752 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:13,752 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:13,753 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:13,753 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:13,753 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:13,753 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:13,793 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,795 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:13,862 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:13,862 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:13,863 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:13,863 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:13,863 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:13,863 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:13,863 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:13,863 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:13,863 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:13,864 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:13,864 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:13,865 INFO   ||  Processed session termination for sessionid: 0x1670a4a25f30000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:13,878 INFO   ||  Session: 0x1670a4a25f30000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:13,878 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:13,878 INFO   ||  EventThread shut down for session: 0x1670a4a25f30000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:13,882 INFO   ||  Closed socket connection for client /127.0.0.1:38260 which had sessionid 0x1670a4a25f30000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:13,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,059 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,106 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,161 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,196 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,239 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:14,239 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:14,239 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:14,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,294 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,408 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,512 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,561 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,562 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:14,912 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,060 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,095 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,148 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,239 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:15,239 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:15,239 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:15,258 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,360 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,512 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,558 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,761 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:15,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,012 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,147 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,164 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,200 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,239 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:16,239 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:16,239 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:16,252 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:16,253 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:16,253 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:16,253 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:16,254 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:16,254 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:16,254 INFO   ||  Stopped Kafka server 1 at localhost:46003   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:16,255 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:16,255 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,255 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,255 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:16,255 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,255 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:16,255 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,255 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:16,255 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:16,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,265 INFO   ||  binding to port localhost/127.0.0.1:35203   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:16,265 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,267 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,268 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40817
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:35203
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:16,269 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:16,269 INFO   ||  Connecting to zookeeper on localhost:35203   [kafka.server.KafkaServer]
2018-11-12 18:38:16,269 INFO   ||  Initiating client connection, connectString=localhost:35203 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@18918d2e   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:16,269 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:16,270 INFO   ||  Opening socket connection to server localhost/127.0.0.1:35203. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:16,270 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:16,270 INFO   ||  Socket connection established to localhost/127.0.0.1:35203, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:16,270 INFO   ||  Accepted socket connection from /127.0.0.1:45604   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:16,271 INFO   ||  Client attempting to establish new session at /127.0.0.1:45604   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,271 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:16,304 INFO   ||  Established session 0x1670a4a3b4a0000 with negotiated timeout 6000 for client /127.0.0.1:45604   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:16,304 INFO   ||  Session establishment complete on server localhost/127.0.0.1:35203, sessionid = 0x1670a4a3b4a0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:16,304 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:16,313 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,385 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,413 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,415 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,457 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,474 INFO   ||  Cluster ID = gx0d8mVbRZ6dQzZvGvPQQQ   [kafka.server.KafkaServer]
2018-11-12 18:38:16,475 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:16,476 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:16,477 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:16,477 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:16,478 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:16,478 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:16,479 INFO   ||  Logs loading complete in 1 ms.   [kafka.log.LogManager]
2018-11-12 18:38:16,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:16,515 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,542 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:16,543 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:16,543 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:16,544 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:16,560 INFO   ||  Awaiting socket connections on localhost:40817.   [kafka.network.Acceptor]
2018-11-12 18:38:16,561 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:16,561 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,562 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,562 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,564 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:16,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,565 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,566 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:16,566 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,566 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,567 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:16,567 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:16,568 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:16,568 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:16,568 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:16,571 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:16,571 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:16,571 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:16,572 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,576 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:16,583 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:16,584 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:16,586 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:16,588 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:16,593 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:16,594 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x41 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,594 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x42 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,606 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:16,607 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40817,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:16,608 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:16,610 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,611 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:16,612 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40817 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:16,612 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:16,613 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,613 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:delete cxid:0x4e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,624 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:16,639 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,639 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,639 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:16,639 INFO   ||  Started Kafka server 1 at localhost:40817 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:16,641 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:setData cxid:0x52 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,648 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x53 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,660 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:16,665 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,667 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40817]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:16,667 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:16,668 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:16,668 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:16,668 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:16,668 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:16,669 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:16,669 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,669 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,669 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x5b zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,671 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,672 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x5c zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:16,696 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:16,697 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:16,700 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:16,700 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:16,701 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:16,702 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:16,702 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:16,702 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:16,713 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,715 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,715 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,775 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:16,845 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:16,847 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40817, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:16,847 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40817, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:16,848 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:40817]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:16,849 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,849 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,850 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40817]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,850 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,850 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,851 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,851 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,851 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:16,956 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:16,956 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:40817]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,957 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:16,957 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,957 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,957 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,957 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,957 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,958 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,958 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:16,958 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,958 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:16,958 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:16,963 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:16,964 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,062 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,063 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:17,063 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40817]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:17,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,065 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,065 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,071 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:17,080 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:17,092 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:17,114 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:17,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,116 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:17,116 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:17,116 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:17,116 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:17,117 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:17,117 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:17,118 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:17,128 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a3b4a0000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:17,152 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:17,153 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:17,155 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:17,156 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:17,156 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:17,156 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:17,156 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:17,157 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:17,157 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:17,158 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:17,168 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40817 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,169 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,169 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,170 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,171 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,172 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,174 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:17,194 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,194 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,198 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,198 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065896839, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,199 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,199 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,199 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065896842, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,199 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065896843, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,200 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,200 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,215 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:17,217 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,217 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:40817, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,217 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:40817, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,217 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40817]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:17,219 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,219 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,224 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40817 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,225 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,225 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,226 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,227 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,228 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,229 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,230 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,234 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,234 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065896839, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,235 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,235 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,235 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065896842, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,235 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065896843, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,237 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,238 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,241 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40817]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:17,242 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,242 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,247 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40817 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,248 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,248 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,249 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,250 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,251 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,253 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,253 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,257 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,257 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065896839, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,258 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,258 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,258 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065896842, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,258 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065896843, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,259 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:17,263 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,263 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,265 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40817]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:17,267 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,267 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,271 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40817 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,272 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,273 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,274 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,275 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,276 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,276 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,279 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,280 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065896839, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,280 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,280 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,281 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065896842, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,281 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065896843, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,281 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:17,284 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,284 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,287 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40817]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:17,288 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,288 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:17,292 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:40817 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,293 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,293 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,294 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,295 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,295 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,297 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:17,297 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:17,299 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,302 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,302 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065896839, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,303 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,303 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:17,303 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065896842, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,303 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065896843, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:17,304 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:17,306 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,306 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,309 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:17,309 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:17,312 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:17,313 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:17,313 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:17,314 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,316 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:17,316 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:17,317 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:17,317 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:17,318 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,366 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,366 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,366 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:17,366 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:17,366 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:17,367 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:17,367 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:17,367 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:17,367 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:17,367 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,367 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,417 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,465 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,567 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,567 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,567 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,667 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,695 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,695 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,695 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:17,695 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:17,695 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:17,695 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:17,695 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:17,695 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:17,695 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:17,695 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,726 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,726 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,726 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,762 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,762 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,762 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,763 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,763 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:17,766 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,766 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,792 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:17,792 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:17,792 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:17,792 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:17,792 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:17,792 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:17,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:17,891 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:17,891 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:17,891 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:17,891 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:17,891 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:17,892 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:17,892 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:17,892 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:17,892 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:17,892 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:17,892 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:17,893 INFO   ||  Processed session termination for sessionid: 0x1670a4a3b4a0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:17,907 INFO   ||  Session: 0x1670a4a3b4a0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:17,907 INFO   ||  EventThread shut down for session: 0x1670a4a3b4a0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:17,907 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:17,907 INFO   ||  Closed socket connection for client /127.0.0.1:45604 which had sessionid 0x1670a4a3b4a0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:17,966 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,116 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,118 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,217 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,318 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,318 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,451 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,455 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,466 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,476 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,476 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,476 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,477 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:18,489 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:18,490 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:18,490 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:18,490 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:18,491 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:18,491 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:18,491 INFO   ||  Stopped Kafka server 1 at localhost:40817   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:18,491 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:18,491 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,491 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,491 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:18,491 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,491 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:18,491 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,491 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:18,492 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:18,495 INFO   ||  binding to port localhost/127.0.0.1:42225   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:18,495 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,496 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,497 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 38905
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:42225
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:18,498 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:18,498 INFO   ||  Connecting to zookeeper on localhost:42225   [kafka.server.KafkaServer]
2018-11-12 18:38:18,499 INFO   ||  Initiating client connection, connectString=localhost:42225 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@21f12db6   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:18,499 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:18,499 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:18,500 INFO   ||  Opening socket connection to server localhost/127.0.0.1:42225. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:18,500 INFO   ||  Socket connection established to localhost/127.0.0.1:42225, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:18,500 INFO   ||  Accepted socket connection from /127.0.0.1:60616   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:18,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:18,500 INFO   ||  Client attempting to establish new session at /127.0.0.1:60616   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,500 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:18,528 INFO   ||  Established session 0x1670a4a43ff0000 with negotiated timeout 6000 for client /127.0.0.1:60616   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:18,528 INFO   ||  Session establishment complete on server localhost/127.0.0.1:42225, sessionid = 0x1670a4a43ff0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:18,528 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:18,540 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,563 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,641 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,683 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,701 INFO   ||  Cluster ID = DL-v6TsUS5aZCMy8JIqYuw   [kafka.server.KafkaServer]
2018-11-12 18:38:18,701 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:18,702 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,703 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,703 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:18,704 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:18,705 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:18,705 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:18,716 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,769 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:18,769 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:18,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,770 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:18,770 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:18,786 INFO   ||  Awaiting socket connections on localhost:38905.   [kafka.network.Acceptor]
2018-11-12 18:38:18,787 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:18,788 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,788 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,788 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,790 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:18,791 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:18,792 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,792 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,792 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:18,793 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:18,793 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:18,794 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:18,794 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:18,797 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:18,797 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:18,797 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:18,798 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,803 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:18,810 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:18,810 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:18,811 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:18,815 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:18,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,820 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:18,820 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,820 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x44 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,833 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:18,833 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,38905,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:18,833 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:18,839 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:18,839 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:18,840 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,840 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,840 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:18,840 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:18,840 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:18,840 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:38905 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:18,841 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,841 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,841 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,841 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:18,841 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:18,841 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:18,841 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,851 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,851 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,851 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:18,851 INFO   ||  Started Kafka server 1 at localhost:38905 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:18,852 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38905, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:18,852 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38905, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:18,852 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:38905]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:18,854 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,854 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,854 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38905]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,855 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,855 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,855 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:18,862 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:18,866 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:18,918 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,919 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:18,961 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:18,961 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:38905]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,962 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:18,963 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,963 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:18,963 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:18,964 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:setData cxid:0x54 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,971 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x55 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,982 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:18,990 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:18,990 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:18,990 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:18,990 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:18,991 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:18,991 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:18,992 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x5d zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:18,995 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x5e zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:19,019 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:19,019 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:19,022 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:19,022 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:19,024 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:19,024 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:19,024 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:19,024 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:19,067 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:19,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,069 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,069 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:19,070 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,070 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,077 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:setData cxid:0x68 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:19,102 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x69 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:19,118 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,119 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,128 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:19,144 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:19,146 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:19,146 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:19,146 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:19,146 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:19,147 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:19,147 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:19,148 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x71 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:19,150 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a43ff0000 type:create cxid:0x72 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:19,175 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:19,177 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:19,179 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:19,179 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:19,180 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:19,180 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:19,180 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:19,180 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:19,180 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:19,181 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:19,218 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,276 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38905 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,277 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,277 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,279 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,279 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,280 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,282 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:19,304 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,304 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,420 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,422 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,576 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,576 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,586 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:19,616 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:19,617 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38905, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:19,617 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38905, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:19,618 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,619 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,619 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,624 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38905 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,624 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,624 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,626 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,626 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,627 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,628 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,628 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,633 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,633 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,635 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,637 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,637 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,641 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38905 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,642 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,642 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,643 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,643 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,644 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,645 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,645 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,650 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,650 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,653 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,654 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,654 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,659 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38905 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,659 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,659 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,661 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,661 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,662 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,663 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,663 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,664 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,668 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,668 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,671 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,673 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,673 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,677 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:38905 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,678 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,678 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,679 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,680 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,680 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:19,681 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:19,686 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,686 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,689 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,691 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,691 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,697 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:38905, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:19,697 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:38905, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:19,698 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:38905]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:19,699 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,699 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,700 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:38905]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:19,701 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,701 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:19,704 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:19,705 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:19,705 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:19,707 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:19,708 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:19,709 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:19,711 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:19,711 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:19,712 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:19,713 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:19,713 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,768 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,792 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,792 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,792 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:19,792 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:19,792 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:19,792 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:19,792 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:19,792 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:19,792 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:19,792 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:19,793 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,811 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,871 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,921 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,961 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,961 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:19,992 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,992 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:19,992 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,070 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,080 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,080 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,080 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:20,080 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:20,080 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:20,080 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:20,080 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:20,080 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:20,080 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:20,080 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,127 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,127 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,127 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,188 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,188 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,188 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,189 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,189 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:20,212 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,214 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:20,214 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:20,215 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:20,215 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:20,215 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:20,215 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:20,219 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,270 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,271 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,313 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:20,313 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:20,313 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:20,313 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:20,314 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:20,314 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:20,314 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:20,314 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:20,314 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:20,315 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:20,315 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:20,316 INFO   ||  Processed session termination for sessionid: 0x1670a4a43ff0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,331 INFO   ||  Session: 0x1670a4a43ff0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:20,331 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,331 INFO   ||  EventThread shut down for session: 0x1670a4a43ff0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:20,331 INFO   ||  Closed socket connection for client /127.0.0.1:60616 which had sessionid 0x1670a4a43ff0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:20,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,471 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,520 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,522 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,522 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,562 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,574 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,613 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,703 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,704 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:20,715 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:20,716 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:20,716 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,716 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:20,717 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:20,718 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:20,718 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:20,719 INFO   ||  Stopped Kafka server 1 at localhost:38905   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:20,719 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:20,719 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,719 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,719 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:20,719 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,719 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:20,719 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,719 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:20,720 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:20,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,723 INFO   ||  binding to port localhost/127.0.0.1:46751   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:20,724 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,726 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,726 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 44213
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:46751
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:20,727 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:20,727 INFO   ||  Connecting to zookeeper on localhost:46751   [kafka.server.KafkaServer]
2018-11-12 18:38:20,728 INFO   ||  Initiating client connection, connectString=localhost:46751 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@3ebbb517   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:20,728 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:20,728 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:20,729 INFO   ||  Opening socket connection to server localhost/127.0.0.1:46751. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:20,729 INFO   ||  Accepted socket connection from /127.0.0.1:35206   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:20,729 INFO   ||  Socket connection established to localhost/127.0.0.1:46751, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:20,729 INFO   ||  Client attempting to establish new session at /127.0.0.1:35206   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,730 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:20,758 INFO   ||  Established session 0x1670a4a4cb40000 with negotiated timeout 6000 for client /127.0.0.1:35206   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:20,758 INFO   ||  Session establishment complete on server localhost/127.0.0.1:46751, sessionid = 0x1670a4a4cb40000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:20,759 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:20,771 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,773 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,794 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,824 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,867 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:20,870 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,932 INFO   ||  Cluster ID = fZb_wxX0TU292hUfU9ZARA   [kafka.server.KafkaServer]
2018-11-12 18:38:20,932 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:20,934 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,934 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,935 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:20,936 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:20,937 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:20,937 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:20,973 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:20,973 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,001 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:21,001 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:21,002 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:21,002 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:21,003 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:21,021 INFO   ||  Awaiting socket connections on localhost:44213.   [kafka.network.Acceptor]
2018-11-12 18:38:21,022 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:21,022 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,024 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,024 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,028 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:21,028 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:21,028 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,029 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,029 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,030 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,030 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,030 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:21,030 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:21,034 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:21,034 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:21,035 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:21,035 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:21,036 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,037 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:21,037 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:21,038 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:21,046 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:21,054 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:21,055 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x41 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,055 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x42 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,064 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:21,064 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,44213,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:21,064 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:21,068 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,069 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:21,070 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:21,070 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:44213 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:21,070 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:21,071 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,071 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:delete cxid:0x4e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,072 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,088 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,088 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,088 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:21,088 INFO   ||  Started Kafka server 1 at localhost:44213 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:21,094 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:21,095 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:setData cxid:0x52 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,100 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x53 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,112 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:21,118 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:44213]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:21,119 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:21,119 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:21,120 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,120 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:21,120 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,120 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,120 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:21,121 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,121 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x5b zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,122 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,124 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,124 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x5c zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,148 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:21,149 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:21,151 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:21,152 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:21,153 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:21,153 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:21,153 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:21,153 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:21,173 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,229 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:21,252 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:21,254 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:44213, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,254 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:44213, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,254 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:44213]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:21,256 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,256 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,256 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:44213]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,257 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,257 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,257 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:21,272 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,273 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,364 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,364 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:44213]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,365 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,365 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,365 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,365 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,365 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,366 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,366 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:21,366 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,366 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,366 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:21,366 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:21,422 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,423 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,423 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,470 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,471 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:44213]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:21,472 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:21,473 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,473 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,480 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,489 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,501 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:21,521 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:21,523 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,524 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:21,524 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:21,524 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:21,524 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,525 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:21,525 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:21,526 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,531 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a4cb40000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:21,556 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:21,557 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:21,559 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:21,560 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:21,560 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:21,560 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:21,560 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:21,560 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:21,561 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:21,562 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:21,566 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,573 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,576 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:44213 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,577 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,577 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,579 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,579 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,580 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,582 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:21,608 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,608 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,613 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,613 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,624 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,629 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:21,631 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:44213, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,631 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:44213, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:21,631 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:44213]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:21,633 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,633 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,638 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:44213 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,638 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,638 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,640 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,641 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,641 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,643 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,643 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,648 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,649 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,651 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:44213]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:21,653 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,653 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:21,657 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:44213 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,658 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,658 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,659 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,660 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,661 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,662 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:21,662 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:21,666 ERROR  ||  Unexpected exception while processing record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 0, CreateTime = 1542065901227, serialized key size = -1, serialized value size = 137, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"xxxDROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStopOnUnparseableSQL(KafkaDatabaseHistoryTest.java:254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 43 more
2018-11-12 18:38:21,667 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,667 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,670 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:21,670 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:21,673 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:21,674 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:21,674 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,674 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:21,676 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:21,676 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:21,677 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:21,679 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:21,679 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,769 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,825 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,829 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,829 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,829 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:21,829 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:21,829 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:21,829 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:21,829 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:21,829 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:21,829 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:21,829 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:21,830 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:21,873 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,873 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,875 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,877 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:21,927 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,029 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,029 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,029 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,060 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,060 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,060 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:22,060 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:22,060 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:22,060 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:22,060 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:22,060 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:22,060 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:22,060 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,066 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,066 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,066 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,077 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,124 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,124 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,127 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,174 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,223 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,223 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,223 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,224 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,224 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:22,266 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:22,266 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:22,266 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:22,266 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:22,266 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:22,266 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:22,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,325 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,370 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:22,370 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:22,370 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:22,370 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:22,370 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:22,371 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:22,371 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:22,371 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:22,371 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:22,371 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:22,371 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:22,372 INFO   ||  Processed session termination for sessionid: 0x1670a4a4cb40000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:22,374 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,374 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,375 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,389 INFO   ||  Closed socket connection for client /127.0.0.1:35206 which had sessionid 0x1670a4a4cb40000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:22,389 INFO   ||  Session: 0x1670a4a4cb40000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:22,389 INFO   ||  EventThread shut down for session: 0x1670a4a4cb40000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:22,389 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,425 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,428 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,478 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,625 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,627 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,728 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,925 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:22,934 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,934 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,934 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:22,935 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:22,947 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:22,948 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:22,948 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:22,948 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:22,949 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:22,949 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:22,949 INFO   ||  Stopped Kafka server 1 at localhost:44213   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:22,950 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:22,950 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,950 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,950 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:22,950 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:22,950 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:22,950 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:22,950 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:22,950 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:22,954 INFO   ||  binding to port localhost/127.0.0.1:33789   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:22,954 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,959 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,959 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35041
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:33789
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:22,960 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:22,960 INFO   ||  Connecting to zookeeper on localhost:33789   [kafka.server.KafkaServer]
2018-11-12 18:38:22,961 INFO   ||  Initiating client connection, connectString=localhost:33789 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6eb792e8   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:22,961 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:22,961 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:22,962 INFO   ||  Opening socket connection to server localhost/127.0.0.1:33789. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:22,962 INFO   ||  Socket connection established to localhost/127.0.0.1:33789, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:22,962 INFO   ||  Accepted socket connection from /127.0.0.1:47294   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:22,962 INFO   ||  Client attempting to establish new session at /127.0.0.1:47294   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,962 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:22,995 INFO   ||  Established session 0x1670a4a556b0000 with negotiated timeout 6000 for client /127.0.0.1:47294   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:22,995 INFO   ||  Session establishment complete on server localhost/127.0.0.1:33789, sessionid = 0x1670a4a556b0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:22,997 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:23,001 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:23,009 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,026 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,033 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,063 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,076 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,077 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,105 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,130 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,170 INFO   ||  Cluster ID = DAIHID2nQI6q5kdw7U9bAw   [kafka.server.KafkaServer]
2018-11-12 18:38:23,170 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:23,172 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:23,172 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:23,172 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:23,173 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:23,174 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:23,174 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:23,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,284 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:23,285 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:23,285 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:23,286 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:23,305 INFO   ||  Awaiting socket connections on localhost:35041.   [kafka.network.Acceptor]
2018-11-12 18:38:23,306 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:23,306 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,306 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,307 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,309 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:23,310 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:23,310 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,311 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,311 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:23,312 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:23,312 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:23,312 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:23,312 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:23,314 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:23,315 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:23,315 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:23,320 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:23,322 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,323 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:23,323 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:23,324 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:23,326 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,327 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,330 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,332 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:23,336 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:23,336 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,336 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x45 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,344 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:23,344 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,35041,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:23,345 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:23,351 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set(1)   [kafka.controller.KafkaController]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:23,352 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:23,353 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,353 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,353 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:23,353 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,353 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:23,353 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:35041 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:23,354 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,354 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,354 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,354 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:23,354 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:23,354 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,354 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:delete cxid:0x50 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,366 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,366 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,367 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:23,367 INFO   ||  Started Kafka server 1 at localhost:35041 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:23,374 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:23,376 INFO   ||  [Controller id=1] Newly added brokers: , deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:23,376 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:setData cxid:0x56 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,377 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,380 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x57 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,392 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:23,399 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35041, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:23,399 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35041, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:23,399 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:35041]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:23,400 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:23,400 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:23,400 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:23,400 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,400 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:23,401 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,401 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x5f zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,402 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,402 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,402 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35041]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,404 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,404 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,404 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:23,404 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x60 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,428 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:23,429 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:23,432 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:23,433 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:23,435 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:23,435 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:23,435 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:23,435 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:23,477 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,512 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:23,512 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:35041]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,513 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:23,514 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,514 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,514 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState(KafkaDatabaseHistoryTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:23,516 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:23,527 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,528 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,564 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,619 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:23,620 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:23,620 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35041]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:23,622 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,622 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:23,629 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:setData cxid:0x6b zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,638 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x6c zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,650 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:23,668 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:23,669 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:23,669 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:23,670 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:23,670 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,670 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:23,670 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:23,671 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x74 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,678 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,680 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a556b0000 type:create cxid:0x75 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:23,704 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:23,705 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:23,708 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:23,708 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:23,709 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:23,709 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:23,709 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:23,709 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:23,709 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:23,710 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:23,727 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35041 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:23,728 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:23,728 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:23,729 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:23,730 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,730 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:23,731 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:23,732 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:23,753 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:23,753 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:23,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,778 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,779 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,827 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,878 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,878 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,878 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,923 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,928 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:23,977 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,078 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,078 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,081 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,128 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,128 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,128 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,139 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:24,167 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:24,169 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:35041, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:24,169 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:35041, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:24,170 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35041]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:24,172 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,172 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,177 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35041 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,177 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,177 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,178 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,179 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,179 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,180 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,182 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,182 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,182 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,187 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,187 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,190 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35041]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:24,192 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,192 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,197 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35041 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,198 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,198 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,199 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,200 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,201 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,202 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,202 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,207 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,207 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,210 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35041]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:24,212 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,212 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,217 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35041 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,217 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,217 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,219 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,219 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,220 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,221 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,221 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,227 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,227 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,230 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35041]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:24,232 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,232 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:24,237 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:35041 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,237 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,237 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,239 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,240 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,241 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,242 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:24,242 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:24,248 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,248 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,250 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:24,250 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:24,253 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:24,254 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:24,254 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:24,257 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:24,257 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:24,257 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:24,258 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:24,258 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,278 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,279 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,311 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,311 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,311 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:24,311 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:24,311 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:24,311 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:24,311 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:24,311 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:24,311 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:24,311 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,311 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,379 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,457 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,511 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,511 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,511 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,529 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,529 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,639 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,639 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,640 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:24,640 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:24,640 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:24,640 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:24,640 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:24,640 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:24,640 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:24,640 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,647 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,647 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,647 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,707 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,707 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,707 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,707 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,707 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:24,708 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,716 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,724 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:24,724 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:24,724 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:24,724 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:24,724 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:24,724 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:24,730 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,816 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:24,816 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:24,816 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:24,816 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:24,816 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:24,817 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:24,817 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:24,817 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:24,817 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:24,817 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:24,818 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:24,819 INFO   ||  Processed session termination for sessionid: 0x1670a4a556b0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:24,829 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,830 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,831 INFO   ||  Session: 0x1670a4a556b0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:24,831 INFO   ||  EventThread shut down for session: 0x1670a4a556b0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:24,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,831 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:24,831 INFO   ||  Closed socket connection for client /127.0.0.1:47294 which had sessionid 0x1670a4a556b0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:24,880 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,929 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,933 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,933 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:24,979 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,024 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,030 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,080 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,080 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,080 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,130 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,172 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:25,188 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:25,189 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:25,190 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:25,190 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:25,190 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:25,190 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:25,191 INFO   ||  Stopped Kafka server 1 at localhost:35041   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:25,191 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:25,191 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,191 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,191 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:25,191 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,191 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:25,191 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,191 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:25,191 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:25,194 INFO   ||  binding to port localhost/127.0.0.1:42879   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:25,195 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,196 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,197 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 36821
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:42879
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:25,198 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:25,198 INFO   ||  Connecting to zookeeper on localhost:42879   [kafka.server.KafkaServer]
2018-11-12 18:38:25,198 INFO   ||  Initiating client connection, connectString=localhost:42879 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@f102f3e   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:25,198 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:25,199 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:25,199 INFO   ||  Opening socket connection to server localhost/127.0.0.1:42879. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:25,200 INFO   ||  Socket connection established to localhost/127.0.0.1:42879, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:25,200 INFO   ||  Accepted socket connection from /127.0.0.1:50726   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:25,200 INFO   ||  Client attempting to establish new session at /127.0.0.1:50726   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,200 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:25,209 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,230 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,231 INFO   ||  Established session 0x1670a4a5e2b0000 with negotiated timeout 6000 for client /127.0.0.1:50726   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:25,231 INFO   ||  Session establishment complete on server localhost/127.0.0.1:42879, sessionid = 0x1670a4a5e2b0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:25,232 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:25,240 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,264 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,294 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,331 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,331 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:25,336 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:25,354 INFO   ||  Cluster ID = xh-ah-ILTI6ym_EGhTgr3A   [kafka.server.KafkaServer]
2018-11-12 18:38:25,354 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:25,355 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,355 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,356 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:25,356 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:25,357 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:25,357 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:26,349 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:26,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,351 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,352 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,414 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:26,414 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:26,415 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:26,416 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:26,431 INFO   ||  Awaiting socket connections on localhost:36821.   [kafka.network.Acceptor]
2018-11-12 18:38:26,432 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:26,432 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,432 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,432 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,434 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:26,435 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:26,436 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,436 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:26,436 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,437 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:26,444 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:26,445 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:26,445 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:26,445 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:26,445 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:setData cxid:0x2a zxid:0x17 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,446 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:26,446 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:26,456 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:26,462 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:26,464 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:26,464 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:26,467 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,469 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:26,473 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,473 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,473 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:26,473 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,473 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:26,474 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,474 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:delete cxid:0x48 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,480 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:26,481 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:26,482 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,482 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,492 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:26,492 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,36821,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:26,493 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:26,494 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:26,496 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:38:26,496 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:26,497 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:36821 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:26,501 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,515 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,515 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,515 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:26,515 INFO   ||  Started Kafka server 1 at localhost:36821 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:26,517 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,529 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,546 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:26,553 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:36821]
	buffer.memory = 33554432
	client.id = intruder
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:26,554 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:26,554 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:26,554 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,555 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,555 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:26,555 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,555 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:26,555 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,556 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,557 WARN   ||  [Producer clientId=intruder] Error while fetching metadata with correlation id 1 : {schema-changes-topic=UNKNOWN_TOPIC_OR_PARTITION}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:26,558 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,582 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:26,583 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:26,585 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:26,586 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:26,587 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:26,587 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:26,587 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:26,587 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:26,661 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:26,684 INFO   ||  [Producer clientId=intruder] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:26,687 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36821, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:26,687 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36821, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:26,687 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:36821]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:26,689 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,689 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,689 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36821]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,690 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,690 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,690 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:26,796 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:26,796 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:36821]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:26,797 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:26,797 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,797 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,797 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:26,903 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:26,904 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36821]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:26,904 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:26,905 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,905 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:26,912 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:setData cxid:0x6a zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,918 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x6b zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,930 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:26,948 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:26,949 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:26,949 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:26,949 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:26,949 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,950 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:26,950 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:26,951 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x73 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,960 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a5e2b0000 type:create cxid:0x74 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:26,984 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:26,985 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:26,987 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:26,988 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:26,988 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:26,988 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:26,988 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:26,988 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:26,989 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:26,989 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:27,009 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36821 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,009 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,009 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,011 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,012 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,013 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,014 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:27,033 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,033 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,037 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,038 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065906678, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,038 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,038 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,039 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065906682, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,039 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065906683, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:125)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,040 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,040 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,055 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:27,056 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:36821, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,056 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:36821, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,057 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36821]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:27,058 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,058 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,063 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36821 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,063 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,063 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,065 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,065 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,067 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,068 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,073 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,073 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065906678, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,074 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,074 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,074 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065906682, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,075 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065906683, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:170)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,077 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,077 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,080 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36821]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:27,081 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,081 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,086 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36821 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,086 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,086 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,087 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,088 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,089 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,090 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,090 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,094 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,094 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065906678, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,095 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,095 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,095 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065906682, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,095 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065906683, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,096 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:176)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:27,098 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,098 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,101 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36821]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:27,102 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,102 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,107 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36821 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,107 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,107 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,108 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,109 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,109 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,110 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,111 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,114 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,114 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065906678, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,115 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,115 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,115 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065906682, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,115 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065906683, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,116 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:182)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:27,118 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,118 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,121 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:36821]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:27,122 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,122 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:27,126 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:36821 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,127 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,127 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,128 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,128 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,129 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,130 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:27,130 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:27,133 WARN   ||  Skipping null database history record. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,133 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 1, CreateTime = 1542065906678, serialized key size = -1, serialized value size = 0, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = )'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was null
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,134 WARN   ||  Skipping invalid database history record '{
  "position" : {
    "filename" : "my-txn-file.log",
    "position" : 39
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,134 WARN   ||  Skipping invalid database history record '{
  "source" : {
    "server" : "my-server"
  },
  "databaseName" : "db1",
  "ddl" : "DROP TABLE foo;"
}'. This is often not an issue, but if it happens repeatedly please check the 'schema-changes-topic' topic.   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:27,134 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 4, CreateTime = 1542065906682, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;")'   [io.debezium.relational.history.KafkaDatabaseHistory]
com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 1])
 at [Source: (String)"{"source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;""; line: 1, column: 267]
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:588)
	at com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:483)
	at com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:495)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2332)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:646)
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:191)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,136 ERROR  ||  Error while deserializing history record 'ConsumerRecord(topic = schema-changes-topic, partition = 0, offset = 5, CreateTime = 1542065906683, serialized key size = -1, serialized value size = 133, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = "source":{"server":"my-server"},"position":{"filename":"my-txn-file.log","position":39},"databaseName":"db1","ddl":"DROP TABLE foo;"})'   [io.debezium.relational.history.KafkaDatabaseHistory]
java.io.IOException: Expected data to start with an Object, but was VALUE_STRING
	at io.debezium.document.JacksonReader.parseDocument(JacksonReader.java:115)
	at io.debezium.document.JacksonReader.parse(JacksonReader.java:101)
	at io.debezium.document.JacksonReader.read(JacksonReader.java:56)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:231)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:27,138 WARN   ||  Ignoring unparseable statements 'xxxDROP TABLE foo;' stored in database history: {}   [io.debezium.relational.history.KafkaDatabaseHistory]
io.debezium.text.ParsingException: Failed to parse statement 'xxxDROP TABLE foo;'
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
	at io.debezium.relational.history.AbstractDatabaseHistory.lambda$recover$1(AbstractDatabaseHistory.java:75)
	at io.debezium.relational.history.KafkaDatabaseHistory.recoverRecords(KafkaDatabaseHistory.java:238)
	at io.debezium.relational.history.AbstractDatabaseHistory.recover(AbstractDatabaseHistory.java:63)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:188)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.shouldIgnoreUnparseableMessages(KafkaDatabaseHistoryTest.java:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.debezium.text.ParsingException: Expecting token type 128 at line 1, column 1 but found 'xxxDROP':  ===>> xxxDROP TABLE foo;
	at io.debezium.text.TokenStream.consume(TokenStream.java:737)
	at io.debezium.relational.ddl.DdlParser.consumeStatement(DdlParser.java:568)
	at io.debezium.relational.ddl.DdlParser.parseUnknownStatement(DdlParser.java:376)
	at io.debezium.relational.ddl.DdlParserSql2003.parseNextStatement(DdlParserSql2003.java:120)
	at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:286)
	... 42 more
2018-11-12 18:38:27,140 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,140 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,142 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:27,142 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:27,145 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:27,146 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:27,146 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:27,148 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:27,148 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:27,149 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:27,149 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:27,149 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,199 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,203 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,203 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,204 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,205 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,236 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,236 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,236 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:27,236 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:27,237 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:27,237 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:27,237 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:27,237 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:27,237 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:27,237 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,237 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,248 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,253 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,253 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,254 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,254 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,255 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,349 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,354 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,354 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,398 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,403 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,437 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,437 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,437 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,454 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,454 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,456 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,456 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,503 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,504 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,504 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,504 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,528 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,528 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,528 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:27,528 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:27,528 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:27,528 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:27,528 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:27,528 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:27,529 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:27,529 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,549 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,554 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,555 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,567 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,567 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,567 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,599 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,632 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,632 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,633 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,633 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,633 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:27,656 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:27,656 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:27,656 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:27,656 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:27,656 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:27,656 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:27,703 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:27,748 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:27,748 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:27,748 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:27,748 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:27,748 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:27,748 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:27,748 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:27,749 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:27,749 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:27,749 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:27,749 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:27,750 INFO   ||  Processed session termination for sessionid: 0x1670a4a5e2b0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:27,763 INFO   ||  Session: 0x1670a4a5e2b0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:27,763 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:27,763 INFO   ||  Closed socket connection for client /127.0.0.1:50726 which had sessionid 0x1670a4a5e2b0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:27,763 INFO   ||  EventThread shut down for session: 0x1670a4a5e2b0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:27,950 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,000 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,154 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,156 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,255 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,256 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,304 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,355 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,355 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,355 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,356 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,356 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,356 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,356 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:28,405 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,406 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,406 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,408 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,506 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,554 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,556 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,605 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,658 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,658 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,705 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,751 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:28,756 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,002 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,107 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,108 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,158 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,256 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,356 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:29,356 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:29,356 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:29,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,357 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,367 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:29,368 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:29,368 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:29,368 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:29,369 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:29,369 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:29,369 INFO   ||  Stopped Kafka server 1 at localhost:36821   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:29,369 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:29,369 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,369 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,369 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:29,369 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,370 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:29,370 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,370 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:29,370 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
2018-11-12 18:38:29,374 INFO   ||  binding to port localhost/127.0.0.1:36221   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:29,375 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,382 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,382 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 41147
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:36221
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:29,383 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:29,383 INFO   ||  Connecting to zookeeper on localhost:36221   [kafka.server.KafkaServer]
2018-11-12 18:38:29,384 INFO   ||  Initiating client connection, connectString=localhost:36221 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5a09baa   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:29,384 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:29,385 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:29,385 INFO   ||  Opening socket connection to server localhost/127.0.0.1:36221. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:29,385 INFO   ||  Socket connection established to localhost/127.0.0.1:36221, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:29,385 INFO   ||  Accepted socket connection from /127.0.0.1:56292   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:29,386 INFO   ||  Client attempting to establish new session at /127.0.0.1:56292   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,386 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:29,406 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,407 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,407 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,407 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,446 INFO   ||  Established session 0x1670a4a6e7f0000 with negotiated timeout 6000 for client /127.0.0.1:56292   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:29,446 INFO   ||  Session establishment complete on server localhost/127.0.0.1:36221, sessionid = 0x1670a4a6e7f0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:29,447 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:29,456 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,457 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,480 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:29,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,510 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,552 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,556 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,557 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,569 INFO   ||  Cluster ID = Tu5XyILOS864Tsvc8rX-JA   [kafka.server.KafkaServer]
2018-11-12 18:38:29,569 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:29,570 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:29,571 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:29,571 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:29,571 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:29,572 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:29,572 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:29,607 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,636 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:29,637 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:29,641 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:29,642 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:29,657 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,657 INFO   ||  Awaiting socket connections on localhost:41147.   [kafka.network.Acceptor]
2018-11-12 18:38:29,657 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,658 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:29,659 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,659 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,659 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,662 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:29,664 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:29,664 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,664 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,665 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:29,665 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:29,665 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:29,665 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:29,666 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:29,671 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:29,672 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:29,672 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:29,673 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,677 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:29,684 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:29,684 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:29,685 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:29,689 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,696 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:29,697 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,697 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,698 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:29,698 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,698 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:29,698 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,698 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,699 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,699 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:29,699 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:29,699 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,699 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,700 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:29,700 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,700 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,701 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:29,703 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,707 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:29,707 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,41147,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:29,708 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:29,709 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:29,710 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:38:29,711 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:29,712 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:41147 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:29,728 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,728 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,728 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:29,728 INFO   ||  Started Kafka server 1 at localhost:41147 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:29,728 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41147, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:29,729 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41147, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:29,729 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:41147]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:29,730 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,730 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,731 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41147]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,731 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,732 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,732 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,732 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:111)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:29,758 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,837 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:29,837 INFO   ||  AdminClientConfig values: 
	bootstrap.servers = [localhost:41147]
	client.id = my-db-history
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'value.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'batch.size' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'max.block.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'acks' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'buffer.memory' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'key.serializer' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 WARN   ||  The configuration 'linger.ms' was supplied but isn't a known config.   [org.apache.kafka.clients.admin.AdminClientConfig]
2018-11-12 18:38:29,838 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,838 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,838 WARN   ||  Error registering AppInfo mbean   [org.apache.kafka.common.utils.AppInfoParser]
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=my-db-history
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:378)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:329)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:51)
	at io.debezium.relational.history.KafkaDatabaseHistory.initializeStorage(KafkaDatabaseHistory.java:335)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testHistoryTopicContent(KafkaDatabaseHistoryTest.java:114)
	at io.debezium.relational.history.KafkaDatabaseHistoryTest.testExists(KafkaDatabaseHistoryTest.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:31)
	at xxl.java.junit.JUnitRunner.call(JUnitRunner.java:17)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-12 18:38:29,839 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:setData cxid:0x53 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/schema-changes-topic Error:KeeperErrorCode = NoNode for /config/topics/schema-changes-topic   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,857 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:29,881 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x54 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,893 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:29,900 INFO   ||  [Controller id=1] New topics: [Set(schema-changes-topic)], deleted topics: [Set()], new partition replica assignment [Map(schema-changes-topic-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:29,900 INFO   ||  [Controller id=1] New topic creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:29,900 INFO   ||  [Controller id=1] New partition creation callback for schema-changes-topic-0   [kafka.controller.KafkaController]
2018-11-12 18:38:29,900 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,901 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:29,901 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions schema-changes-topic-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,901 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x5c zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,905 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x5d zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/schema-changes-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/schema-changes-topic/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,929 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=schema-changes-topic,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:29,930 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions schema-changes-topic-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:29,932 INFO   ||  Loading producer state from offset 0 for partition schema-changes-topic-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:29,933 INFO   ||  Completed load of log schema-changes-topic-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:29,933 INFO   ||  Created log for partition [schema-changes-topic,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:29,934 INFO   ||  [Partition schema-changes-topic-0 broker=1] No checkpointed highwatermark is found for partition schema-changes-topic-0   [kafka.cluster.Partition]
2018-11-12 18:38:29,934 INFO   ||  Replica loaded for partition schema-changes-topic-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:29,934 INFO   ||  [Partition schema-changes-topic-0 broker=1] schema-changes-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:29,943 INFO   ||  Database history topic '(name=schema-changes-topic, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807})' created   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:29,944 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:29,944 INFO   ||  [Admin Manager on Broker 1]: Error processing create topic request for topic schema-changes-topic with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=delete, retention.ms=9223372036854775807})   [kafka.server.AdminManager]
org.apache.kafka.common.errors.TopicExistsException: Topic 'schema-changes-topic' already exists.
2018-11-12 18:38:29,946 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,946 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:29,952 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:setData cxid:0x67 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,959 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x68 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:29,987 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:29,995 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:29,996 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:29,996 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:29,996 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:29,996 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,997 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:29,997 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:29,998 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x70 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:30,001 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a6e7f0000 type:create cxid:0x71 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:30,004 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,025 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:30,026 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:30,028 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:30,028 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:30,029 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:30,029 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:30,029 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:30,029 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:30,029 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:30,030 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:30,050 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41147 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,051 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,051 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,052 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,053 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,054 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,055 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:30,072 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,072 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,110 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,160 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,208 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,208 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,257 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,259 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,308 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,309 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,310 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,451 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,451 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,458 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,460 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: schema-changes-topic-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:30,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,486 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:30,487 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41147, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:30,488 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41147, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:30,488 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,489 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,489 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,493 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41147 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,494 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,494 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,495 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 2 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,496 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,496 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 3   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,498 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 3   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,498 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,502 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 3 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,502 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 4 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,505 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,506 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,506 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,510 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41147 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,511 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,511 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,512 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 4 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,512 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,513 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 5   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,514 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 5   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,514 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,519 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 5 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,519 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 6 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,521 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,522 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,522 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,526 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41147 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,527 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,527 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,528 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 6 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,528 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,529 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 7   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,530 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 7   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,530 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,534 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 7 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,534 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 8 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,537 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,538 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,538 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Discovered group coordinator localhost:41147 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,542 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,543 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 8 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,544 INFO   ||  [GroupCoordinator 1]: Stabilized group my-db-history generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,544 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group my-db-history for generation 9   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Successfully joined group with generation 9   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:30,545 INFO   ||  [Consumer clientId=my-db-history, groupId=my-db-history] Setting newly assigned partitions [schema-changes-topic-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:30,550 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group my-db-history with old generation 9 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,550 INFO   ||  [GroupCoordinator 1]: Group my-db-history with generation 10 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,552 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,553 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,553 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,560 INFO   ||  KafkaDatabaseHistory Consumer config: {enable.auto.commit=false, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, group.id=my-db-history, auto.offset.reset=earliest, session.timeout.ms=50000, bootstrap.servers=localhost:41147, max.poll.interval.ms=100, client.id=my-db-history, key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, fetch.min.bytes=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:30,560 INFO   ||  KafkaDatabaseHistory Producer config: {bootstrap.servers=localhost:41147, value.serializer=org.apache.kafka.common.serialization.StringSerializer, buffer.memory=1048576, retries=1, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=my-db-history, linger.ms=0, batch.size=32768, max.block.ms=10000, acks=1}   [io.debezium.relational.history.KafkaDatabaseHistory]
2018-11-12 18:38:30,560 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:41147]
	buffer.memory = 1048576
	client.id = my-db-history
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:30,562 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,562 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,562 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:41147]
	check.crcs = true
	client.id = my-db-history
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-db-history
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 100
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 50000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:30,563 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,563 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:30,566 INFO   ||  [Producer clientId=my-db-history] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:30,567 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:30,567 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:30,569 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:30,570 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:30,571 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:30,573 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:30,573 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:30,573 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:30,574 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:30,574 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,608 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,611 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,660 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,665 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,665 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,665 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:30,665 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:30,665 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:30,665 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:30,665 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:30,665 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:30,665 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:30,665 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,665 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,672 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,709 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,709 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,709 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,759 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,773 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,855 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,859 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,865 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,865 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,865 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,944 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,944 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,944 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:30,944 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:30,944 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:30,944 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:30,944 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:30,944 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:30,944 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:30,944 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,955 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:30,987 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,987 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:30,987 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,023 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,024 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,059 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,059 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,059 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,060 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,060 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,060 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,060 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,085 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:31,085 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:31,085 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:31,085 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:31,085 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:31,085 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:31,159 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,160 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,161 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,183 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:31,183 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:31,183 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:31,183 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:31,184 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:31,184 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:31,184 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:31,184 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:31,184 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:31,184 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:31,184 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:31,185 INFO   ||  Processed session termination for sessionid: 0x1670a4a6e7f0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,210 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,235 INFO   ||  Session: 0x1670a4a6e7f0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:31,236 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,236 INFO   ||  Closed socket connection for client /127.0.0.1:56292 which had sessionid 0x1670a4a6e7f0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:31,236 INFO   ||  EventThread shut down for session: 0x1670a4a6e7f0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:31,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,261 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,312 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,312 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,410 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,410 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,411 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,424 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,461 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,475 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,560 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,571 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:31,582 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:31,583 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:31,583 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:31,584 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:31,585 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:31,585 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/history_cluster/kafka/broker1/schema-changes-topic-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:31,585 INFO   ||  Stopped Kafka server 1 at localhost:41147   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:31,586 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:31,586 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,586 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,586 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:31,586 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,586 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:31,586 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,586 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:31,586 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowAsynchronousProductionAndAutomaticConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
2018-11-12 18:38:31,596 INFO   ||  binding to port localhost/127.0.0.1:33935   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:31,596 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,598 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,599 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35187
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:33935
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:31,599 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:31,600 INFO   ||  Connecting to zookeeper on localhost:33935   [kafka.server.KafkaServer]
2018-11-12 18:38:31,600 INFO   ||  Initiating client connection, connectString=localhost:33935 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@67f2e0dc   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:31,600 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:31,600 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:31,601 INFO   ||  Opening socket connection to server localhost/127.0.0.1:33935. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:31,601 INFO   ||  Socket connection established to localhost/127.0.0.1:33935, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:31,601 INFO   ||  Accepted socket connection from /127.0.0.1:55714   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:31,601 INFO   ||  Client attempting to establish new session at /127.0.0.1:55714   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,601 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:31,610 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,628 INFO   ||  Established session 0x1670a4a772d0000 with negotiated timeout 6000 for client /127.0.0.1:55714   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:31,629 INFO   ||  Session establishment complete on server localhost/127.0.0.1:33935, sessionid = 0x1670a4a772d0000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:31,629 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:31,645 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,711 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,711 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,717 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,747 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,789 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,806 INFO   ||  Cluster ID = vifcfJLfT1ecxGhGkDadsw   [kafka.server.KafkaServer]
2018-11-12 18:38:31,807 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:31,808 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,808 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,808 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:31,809 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:31,810 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:31,810 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:31,857 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,874 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:31,874 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:31,875 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:31,875 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:31,891 INFO   ||  Awaiting socket connections on localhost:35187.   [kafka.network.Acceptor]
2018-11-12 18:38:31,892 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:31,892 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,892 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,893 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,894 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:31,895 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:31,896 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,896 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,896 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:31,897 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:31,897 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:31,897 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:31,897 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:31,904 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:31,905 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:31,905 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:31,909 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:31,911 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:setData cxid:0x2d zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,911 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:31,912 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:31,912 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:31,913 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:31,921 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:31,931 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:31,931 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x46 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,932 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x48 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,945 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:31,945 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,35187,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:31,945 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,945 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,945 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,945 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,945 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,946 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:31,946 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:31,946 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,946 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,946 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:31,947 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:31,947 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:31,948 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:delete cxid:0x4f zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,962 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:31,965 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:31,966 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:38:31,967 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:31,968 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:35187 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:31,970 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:31,970 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:31,970 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:31,970 INFO   ||  Started Kafka server 1 at localhost:35187 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:31,973 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topicA Error:KeeperErrorCode = NoNode for /config/topics/topicA   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,980 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:31,992 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:32,000 INFO   ||  [Controller id=1] New topics: [Set(topicA)], deleted topics: [Set()], new partition replica assignment [Map(topicA-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:32,000 INFO   ||  [Controller id=1] New topic creation callback for topicA-0   [kafka.controller.KafkaController]
2018-11-12 18:38:32,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:32,001 INFO   ||  [Controller id=1] New partition creation callback for topicA-0   [kafka.controller.KafkaController]
2018-11-12 18:38:32,001 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:32,001 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:32,002 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:32,003 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,005 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x5f zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,011 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35187]
	buffer.memory = 33554432
	client.id = manual
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:32,011 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:35187]
	check.crcs = true
	client.id = 4c3c6f1a-3694-4482-95a0-1e092b671c27
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4c3c6f1a-3694-4482-95a0-1e092b671c27
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:32,012 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,014 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:32,014 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:32,015 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:32,015 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:32,029 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:32,030 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions topicA-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:32,030 WARN   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] Error while fetching metadata with correlation id 2 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,030 WARN   ||  [Producer clientId=manual] Error while fetching metadata with correlation id 1 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,032 INFO   ||  Loading producer state from offset 0 for partition topicA-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:32,033 INFO   ||  Completed load of log topicA-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:32,034 INFO   ||  Created log for partition [topicA,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:32,034 INFO   ||  [Partition topicA-0 broker=1] No checkpointed highwatermark is found for partition topicA-0   [kafka.cluster.Partition]
2018-11-12 18:38:32,034 INFO   ||  Replica loaded for partition topicA-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:32,034 INFO   ||  [Partition topicA-0 broker=1] topicA-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:32,036 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:setData cxid:0x6e zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,046 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x6f zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,057 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,076 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:32,097 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:32,099 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:32,099 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:32,099 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:32,099 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:32,099 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:32,100 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:32,100 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x77 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,106 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a772d0000 type:create cxid:0x78 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,111 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,125 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,135 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: topicA-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:32,162 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,167 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:32,167 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:32,169 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:32,170 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:32,170 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:32,171 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:32,171 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:32,171 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:32,171 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:32,171 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:32,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,227 INFO   ||  [Producer clientId=manual] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:32,233 INFO   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] Discovered group coordinator localhost:35187 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:32,234 INFO   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:32,235 INFO   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:32,236 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 4c3c6f1a-3694-4482-95a0-1e092b671c27 with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,237 INFO   ||  [GroupCoordinator 1]: Stabilized group 4c3c6f1a-3694-4482-95a0-1e092b671c27 generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,238 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group 4c3c6f1a-3694-4482-95a0-1e092b671c27 for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,239 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:32,261 INFO   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:32,261 INFO   ||  [Consumer clientId=4c3c6f1a-3694-4482-95a0-1e092b671c27, groupId=4c3c6f1a-3694-4482-95a0-1e092b671c27] Setting newly assigned partitions [topicA-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:32,266 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 4c3c6f1a-3694-4482-95a0-1e092b671c27 with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,266 INFO   ||  [GroupCoordinator 1]: Group 4c3c6f1a-3694-4482-95a0-1e092b671c27 with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
The consumer completed normally in   0.26687s total;   1 samples;  0.26687s avg;  0.26687s min;  0.26687s max
2018-11-12 18:38:32,272 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:32,273 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:32,275 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:32,276 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:32,276 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:32,278 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:32,278 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:32,278 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:32,279 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:32,279 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,296 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,296 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,296 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:32,296 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:32,296 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:32,296 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:32,296 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:32,296 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:32,296 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:32,296 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,297 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,313 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,314 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,363 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,364 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,496 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,496 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,496 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,512 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,611 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,612 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,613 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,637 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,637 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,637 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:32,637 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:32,637 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:32,637 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:32,637 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:32,637 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:32,637 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:32,637 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,667 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,667 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,667 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,692 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,692 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,693 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,693 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,693 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:32,715 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,716 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:32,716 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:32,716 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:32,716 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:32,716 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:32,716 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:32,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,808 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,814 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:32,815 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:32,815 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:32,815 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:32,815 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:32,815 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:32,815 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:32,815 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:32,815 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:32,816 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:32,816 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:32,817 INFO   ||  Processed session termination for sessionid: 0x1670a4a772d0000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:32,832 INFO   ||  Session: 0x1670a4a772d0000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:32,832 INFO   ||  Closed socket connection for client /127.0.0.1:55714 which had sessionid 0x1670a4a772d0000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:32,832 INFO   ||  EventThread shut down for session: 0x1670a4a772d0000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:32,832 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:32,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:32,913 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,062 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,064 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,127 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,213 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,215 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,228 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,259 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,265 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,365 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,463 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,515 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,566 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,614 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,616 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,714 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,765 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:33,808 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:33,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,819 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:33,820 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:33,821 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:33,821 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:33,821 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:33,821 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:33,822 INFO   ||  Stopped Kafka server 1 at localhost:35187   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:33,823 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:33,823 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,823 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,823 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:33,823 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:33,823 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:33,823 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:33,823 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:33,823 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowProducersAndConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithMultipleBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowAsynchronousProductionAndAutomaticConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
2018-11-12 18:38:33,827 INFO   ||  binding to port localhost/127.0.0.1:41149   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:33,828 INFO   ||  Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/snapshot/version-2 snapdir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/zk/log/version-2   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,829 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,830 INFO   ||  KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 40373
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:41149
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
   [kafka.server.KafkaConfig]
2018-11-12 18:38:33,831 INFO   ||  starting   [kafka.server.KafkaServer]
2018-11-12 18:38:33,831 INFO   ||  Connecting to zookeeper on localhost:41149   [kafka.server.KafkaServer]
2018-11-12 18:38:33,831 INFO   ||  Initiating client connection, connectString=localhost:41149 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1c3d0025   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:33,831 INFO   ||  Starting ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:33,832 INFO   ||  Waiting for keeper state SyncConnected   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:33,832 INFO   ||  Opening socket connection to server localhost/127.0.0.1:41149. Will not attempt to authenticate using SASL (unknown error)   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:33,832 INFO   ||  Socket connection established to localhost/127.0.0.1:41149, initiating session   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:33,832 INFO   ||  Accepted socket connection from /127.0.0.1:47614   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:33,833 INFO   ||  Client attempting to establish new session at /127.0.0.1:47614   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,833 INFO   ||  Creating new log file: log.1   [org.apache.zookeeper.server.persistence.FileTxnLog]
2018-11-12 18:38:33,859 INFO   ||  Established session 0x1670a4a7fe40000 with negotiated timeout 6000 for client /127.0.0.1:47614   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:33,859 INFO   ||  Session establishment complete on server localhost/127.0.0.1:41149, sessionid = 0x1670a4a7fe40000, negotiated timeout = 6000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:33,859 INFO   ||  zookeeper state changed (SyncConnected)   [org.I0Itec.zkclient.ZkClient]
2018-11-12 18:38:33,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,865 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,866 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:33,871 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x5 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:33,895 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0xb zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:33,925 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x13 zxid:0xc txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,000 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:34,014 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,015 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x1f zxid:0x13 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,033 INFO   ||  Cluster ID = 606kp14PQoGhf6QRwE0uOA   [kafka.server.KafkaServer]
2018-11-12 18:38:34,033 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:34,043 INFO   ||  [ThrottledRequestReaper-Fetch]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:34,043 INFO   ||  [ThrottledRequestReaper-Produce]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:34,043 INFO   ||  [ThrottledRequestReaper-Request]: Starting   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:34,044 INFO   ||  Log directory '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1' not found, creating it.   [kafka.log.LogManager]
2018-11-12 18:38:34,045 INFO   ||  Loading logs.   [kafka.log.LogManager]
2018-11-12 18:38:34,045 INFO   ||  Logs loading complete in 0 ms.   [kafka.log.LogManager]
2018-11-12 18:38:34,065 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,108 INFO   ||  Starting log cleanup with a period of 300000 ms.   [kafka.log.LogManager]
2018-11-12 18:38:34,109 INFO   ||  Starting log flusher with a default period of 9223372036854775807 ms.   [kafka.log.LogManager]
2018-11-12 18:38:34,109 INFO   ||  Starting the log cleaner   [kafka.log.LogCleaner]
2018-11-12 18:38:34,110 INFO   ||  [kafka-log-cleaner-thread-0]: Starting   [kafka.log.LogCleaner]
2018-11-12 18:38:34,127 INFO   ||  Awaiting socket connections on localhost:40373.   [kafka.network.Acceptor]
2018-11-12 18:38:34,128 INFO   ||  [SocketServer brokerId=1] Started 1 acceptor threads   [kafka.network.SocketServer]
2018-11-12 18:38:34,129 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,129 INFO   ||  [ExpirationReaper-1-Produce]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,129 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,129 INFO   ||  [ExpirationReaper-1-Fetch]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,138 INFO   ||  [LogDirFailureHandler]: Starting   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:34,140 INFO   ||  [controller-event-thread]: Starting   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:34,140 INFO   ||  [ExpirationReaper-1-topic]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,140 INFO   ||  [ExpirationReaper-1-Heartbeat]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,141 INFO   ||  [ExpirationReaper-1-Rebalance]: Starting   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,141 INFO   ||  [GroupCoordinator 1]: Starting up.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,141 INFO   ||  Creating /controller (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:34,141 INFO   ||  [GroupCoordinator 1]: Startup complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,141 INFO   ||  [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:34,147 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:34,147 INFO   ||  [Controller id=1] 1 successfully elected as the controller   [kafka.controller.KafkaController]
2018-11-12 18:38:34,147 INFO   ||  [Controller id=1] Starting become controller state transition   [kafka.controller.KafkaController]
2018-11-12 18:38:34,153 INFO   ||  [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:34,153 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:setData cxid:0x2c zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,160 INFO   ||  [TransactionCoordinator id=1] Starting up.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:34,160 INFO   ||  [TransactionCoordinator id=1] Startup complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:34,161 INFO   ||  [Transaction Marker Channel Manager 1]: Starting   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:34,165 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,165 INFO   ||  [Controller id=1] Incremented epoch to 1   [kafka.controller.KafkaController]
2018-11-12 18:38:34,169 INFO   ||  [Controller id=1] Partitions being reassigned: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,169 INFO   ||  [Controller id=1] Partitions already reassigned: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] Resuming reassignment of partitions: Map()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] Currently active brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] Currently shutting brokers in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] Current list of topics in the cluster: Set()   [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] List of topics to be deleted:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] List of topics ineligible for deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,170 INFO   ||  [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:34,170 INFO   ||  [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,170 INFO   ||  [Controller id=1] Ready to serve as the new controller with epoch 1   [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [Controller id=1] Partitions undergoing preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [Controller id=1] Partitions that completed preferred replica election:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [Controller id=1] Resuming preferred replica election for partitions:    [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [Controller id=1] Starting preferred replica leader election for partitions    [kafka.controller.KafkaController]
2018-11-12 18:38:34,171 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions    [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,171 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:delete cxid:0x4a zxid:0x1a txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,171 INFO   ||  Creating /brokers/ids/1 (is it secure? false)   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:34,171 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x4b zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,171 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x4c zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,177 INFO   ||  [Controller id=1] Starting the controller scheduler   [kafka.controller.KafkaController]
2018-11-12 18:38:34,183 INFO   ||  Result of znode creation is: OK   [kafka.utils.ZKCheckedEphemeral]
2018-11-12 18:38:34,183 INFO   ||  Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(localhost,40373,ListenerName(PLAINTEXT),PLAINTEXT)   [kafka.utils.ZkUtils]
2018-11-12 18:38:34,183 WARN   ||  No meta.properties file under dir /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/meta.properties   [kafka.server.BrokerMetadataCheckpoint]
2018-11-12 18:38:34,184 INFO   ||  [Controller id=1] Newly added brokers: 1, deleted brokers: , all live brokers: 1   [kafka.controller.KafkaController]
2018-11-12 18:38:34,185 INFO   ||  [Controller id=1] New broker startup callback for 1   [kafka.controller.KafkaController]
2018-11-12 18:38:34,186 INFO   ||  [Controller-1-to-broker-1-send-thread]: Starting   [kafka.controller.RequestSendThread]
2018-11-12 18:38:34,186 INFO   ||  [Controller-1-to-broker-1-send-thread]: Controller 1 connected to localhost:40373 (id: 1 rack: null) for sending state change requests   [kafka.controller.RequestSendThread]
2018-11-12 18:38:34,200 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,201 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,201 INFO   ||  [KafkaServer id=1] started   [kafka.server.KafkaServer]
2018-11-12 18:38:34,201 INFO   ||  Started Kafka server 1 at localhost:40373 with storage in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:34,202 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:setData cxid:0x55 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topicA Error:KeeperErrorCode = NoNode for /config/topics/topicA   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,213 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x56 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,225 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:34,231 INFO   ||  ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:40373]
	buffer.memory = 33554432
	client.id = manual
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2018-11-12 18:38:34,231 INFO   ||  ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:40373]
	check.crcs = true
	client.id = 6bd00b7e-9571-47b6-998f-f017ecbdeee8
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6bd00b7e-9571-47b6-998f-f017ecbdeee8
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2018-11-12 18:38:34,232 INFO   ||  [Controller id=1] New topics: [Set(topicA)], deleted topics: [Set()], new partition replica assignment [Map(topicA-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:34,232 INFO   ||  [Controller id=1] New topic creation callback for topicA-0   [kafka.controller.KafkaController]
2018-11-12 18:38:34,232 INFO   ||  [Controller id=1] New partition creation callback for topicA-0   [kafka.controller.KafkaController]
2018-11-12 18:38:34,232 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,232 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:34,233 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions topicA-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,233 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x5e zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,234 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,234 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,234 INFO   ||  Kafka version : 1.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,234 INFO   ||  Kafka commitId : c0518aa65f25317e   [org.apache.kafka.common.utils.AppInfoParser]
2018-11-12 18:38:34,237 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x61 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/topicA/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/topicA/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,250 WARN   ||  [Producer clientId=manual] Error while fetching metadata with correlation id 1 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,250 WARN   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] Error while fetching metadata with correlation id 2 : {topicA=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,261 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=topicA,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:34,262 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions topicA-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:34,262 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:setData cxid:0x6d zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,267 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x6f zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,269 INFO   ||  Loading producer state from offset 0 for partition topicA-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:34,269 INFO   ||  Completed load of log topicA-0 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms   [kafka.log.Log]
2018-11-12 18:38:34,270 INFO   ||  Created log for partition [topicA,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:34,270 INFO   ||  [Partition topicA-0 broker=1] No checkpointed highwatermark is found for partition topicA-0   [kafka.cluster.Partition]
2018-11-12 18:38:34,270 INFO   ||  Replica loaded for partition topicA-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:34,270 INFO   ||  [Partition topicA-0 broker=1] topicA-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:34,285 INFO   ||  Topic creation {"version":1,"partitions":{"0":[1]}}   [kafka.admin.AdminUtils$]
2018-11-12 18:38:34,316 INFO   ||  [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful   [kafka.server.KafkaApis]
2018-11-12 18:38:34,316 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,317 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,317 INFO   ||  [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]   [kafka.controller.KafkaController]
2018-11-12 18:38:34,317 INFO   ||  [Controller id=1] New topic creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:34,318 INFO   ||  [Controller id=1] New partition creation callback for __consumer_offsets-0   [kafka.controller.KafkaController]
2018-11-12 18:38:34,318 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to NewPartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,318 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:34,319 INFO   ||  [PartitionStateMachine controllerId=1] Invoking state change to OnlinePartition for partitions __consumer_offsets-0   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:34,319 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x77 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,327 INFO   ||  Got user-level KeeperException when processing sessionid:0x1670a4a7fe40000 type:create cxid:0x78 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:34,351 INFO   ||  [ReplicaStateMachine controllerId=1] Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=0,Replica=1]   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:34,352 INFO   ||  [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:34,355 INFO   ||  Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2   [kafka.log.Log]
2018-11-12 18:38:34,356 INFO   ||  Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms   [kafka.log.Log]
2018-11-12 18:38:34,356 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: topicA-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:34,356 INFO   ||  Created log for partition [__consumer_offsets,0] in /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1 with properties {compression.type -> producer, message.format.version -> 1.0-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.   [kafka.log.LogManager]
2018-11-12 18:38:34,356 INFO   ||  [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0   [kafka.cluster.Partition]
2018-11-12 18:38:34,356 INFO   ||  Replica loaded for partition __consumer_offsets-0 with initial high watermark 0   [kafka.cluster.Replica]
2018-11-12 18:38:34,356 INFO   ||  [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1   [kafka.cluster.Partition]
2018-11-12 18:38:34,357 INFO   ||  [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:34,357 INFO   ||  [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.   [kafka.coordinator.group.GroupMetadataManager]
2018-11-12 18:38:34,367 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,368 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,380 INFO   ||  [Producer clientId=manual] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2018-11-12 18:38:34,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,415 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,430 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,454 INFO   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] Discovered group coordinator localhost:40373 (id: 2147483646 rack: null)   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:34,454 INFO   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] Revoking previously assigned partitions []   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:34,454 INFO   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:34,456 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 6bd00b7e-9571-47b6-998f-f017ecbdeee8 with old generation 0 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,457 INFO   ||  [GroupCoordinator 1]: Stabilized group 6bd00b7e-9571-47b6-998f-f017ecbdeee8 generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,459 INFO   ||  [GroupCoordinator 1]: Assignment received from leader for group 6bd00b7e-9571-47b6-998f-f017ecbdeee8 for generation 1   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,460 INFO   ||  Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.   [kafka.server.epoch.LeaderEpochFileCache]
2018-11-12 18:38:34,462 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,466 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,485 INFO   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] Successfully joined group with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2018-11-12 18:38:34,485 INFO   ||  [Consumer clientId=6bd00b7e-9571-47b6-998f-f017ecbdeee8, groupId=6bd00b7e-9571-47b6-998f-f017ecbdeee8] Setting newly assigned partitions [topicA-0]   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2018-11-12 18:38:34,491 INFO   ||  [GroupCoordinator 1]: Preparing to rebalance group 6bd00b7e-9571-47b6-998f-f017ecbdeee8 with old generation 1 (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,491 INFO   ||  [GroupCoordinator 1]: Group 6bd00b7e-9571-47b6-998f-f017ecbdeee8 with generation 2 is now empty (__consumer_offsets-0)   [kafka.coordinator.group.GroupCoordinator]
The consumer completed normally in   0.26262s total;   1 samples;  0.26262s avg;  0.26262s min;  0.26262s max
2018-11-12 18:38:34,494 INFO   ||  [KafkaServer id=1] shutting down   [kafka.server.KafkaServer]
2018-11-12 18:38:34,494 INFO   ||  [KafkaServer id=1] Starting controlled shutdown   [kafka.server.KafkaServer]
2018-11-12 18:38:34,496 INFO   ||  [Controller id=1] Shutting down broker 1   [kafka.controller.KafkaController]
2018-11-12 18:38:34,496 INFO   ||  [KafkaServer id=1] Controlled shutdown succeeded   [kafka.server.KafkaServer]
2018-11-12 18:38:34,497 INFO   ||  [SocketServer brokerId=1] Stopping socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:34,499 INFO   ||  [SocketServer brokerId=1] Stopped socket server request processors   [kafka.network.SocketServer]
2018-11-12 18:38:34,499 INFO   ||  [Kafka Request Handler on Broker 1], shutting down   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:34,499 INFO   ||  [Kafka Request Handler on Broker 1], shut down completely   [kafka.server.KafkaRequestHandlerPool]
2018-11-12 18:38:34,500 INFO   ||  [KafkaApi-1] Shutdown complete.   [kafka.server.KafkaApis]
2018-11-12 18:38:34,500 INFO   ||  [ExpirationReaper-1-topic]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,516 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,540 INFO   ||  [ExpirationReaper-1-topic]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,540 INFO   ||  [ExpirationReaper-1-topic]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,540 INFO   ||  [TransactionCoordinator id=1] Shutting down.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:34,540 INFO   ||  [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0   [kafka.coordinator.transaction.ProducerIdManager]
2018-11-12 18:38:34,540 INFO   ||  [Transaction State Manager 1]: Shutdown complete   [kafka.coordinator.transaction.TransactionStateManager]
2018-11-12 18:38:34,541 INFO   ||  [Transaction Marker Channel Manager 1]: Shutting down   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:34,541 INFO   ||  [Transaction Marker Channel Manager 1]: Stopped   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:34,541 INFO   ||  [Transaction Marker Channel Manager 1]: Shutdown completed   [kafka.coordinator.transaction.TransactionMarkerChannelManager]
2018-11-12 18:38:34,541 INFO   ||  [TransactionCoordinator id=1] Shutdown complete.   [kafka.coordinator.transaction.TransactionCoordinator]
2018-11-12 18:38:34,541 INFO   ||  [GroupCoordinator 1]: Shutting down.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,541 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,565 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,665 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,667 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,741 INFO   ||  [ExpirationReaper-1-Heartbeat]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,741 INFO   ||  [ExpirationReaper-1-Heartbeat]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,741 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,762 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,768 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,816 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,857 INFO   ||  [ExpirationReaper-1-Rebalance]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,857 INFO   ||  [ExpirationReaper-1-Rebalance]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,857 INFO   ||  [GroupCoordinator 1]: Shutdown complete.   [kafka.coordinator.group.GroupCoordinator]
2018-11-12 18:38:34,857 INFO   ||  [ReplicaManager broker=1] Shutting down   [kafka.server.ReplicaManager]
2018-11-12 18:38:34,857 INFO   ||  [LogDirFailureHandler]: Shutting down   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:34,857 INFO   ||  [LogDirFailureHandler]: Stopped   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:34,857 INFO   ||  [LogDirFailureHandler]: Shutdown completed   [kafka.server.ReplicaManager$LogDirFailureHandler]
2018-11-12 18:38:34,857 INFO   ||  [ReplicaFetcherManager on broker 1] shutting down   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:34,857 INFO   ||  [ReplicaFetcherManager on broker 1] shutdown completed   [kafka.server.ReplicaFetcherManager]
2018-11-12 18:38:34,857 INFO   ||  [ExpirationReaper-1-Fetch]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,866 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,867 INFO   ||  [ExpirationReaper-1-Fetch]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,867 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,867 INFO   ||  [ExpirationReaper-1-Fetch]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,867 INFO   ||  [ExpirationReaper-1-Produce]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,916 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,916 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:34,929 INFO   ||  [ExpirationReaper-1-Produce]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,929 INFO   ||  [ExpirationReaper-1-Produce]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,929 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutting down   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,929 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Stopped   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,929 INFO   ||  [ExpirationReaper-1-DeleteRecords]: Shutdown completed   [kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper]
2018-11-12 18:38:34,953 INFO   ||  [ReplicaManager broker=1] Shut down completely   [kafka.server.ReplicaManager]
2018-11-12 18:38:34,953 INFO   ||  Shutting down.   [kafka.log.LogManager]
2018-11-12 18:38:34,953 INFO   ||  Shutting down the log cleaner.   [kafka.log.LogCleaner]
2018-11-12 18:38:34,953 INFO   ||  [kafka-log-cleaner-thread-0]: Shutting down   [kafka.log.LogCleaner]
2018-11-12 18:38:34,953 INFO   ||  [kafka-log-cleaner-thread-0]: Stopped   [kafka.log.LogCleaner]
2018-11-12 18:38:34,953 INFO   ||  [kafka-log-cleaner-thread-0]: Shutdown completed   [kafka.log.LogCleaner]
2018-11-12 18:38:35,051 INFO   ||  Shutdown complete.   [kafka.log.LogManager]
2018-11-12 18:38:35,051 INFO   ||  [controller-event-thread]: Shutting down   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:35,051 INFO   ||  [controller-event-thread]: Stopped   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:35,051 INFO   ||  [controller-event-thread]: Shutdown completed   [kafka.controller.ControllerEventManager$ControllerEventThread]
2018-11-12 18:38:35,051 INFO   ||  [PartitionStateMachine controllerId=1] Stopped partition state machine   [kafka.controller.PartitionStateMachine]
2018-11-12 18:38:35,051 INFO   ||  [ReplicaStateMachine controllerId=1] Stopped replica state machine   [kafka.controller.ReplicaStateMachine]
2018-11-12 18:38:35,051 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutting down   [kafka.controller.RequestSendThread]
2018-11-12 18:38:35,051 INFO   ||  [Controller-1-to-broker-1-send-thread]: Stopped   [kafka.controller.RequestSendThread]
2018-11-12 18:38:35,051 INFO   ||  [Controller-1-to-broker-1-send-thread]: Shutdown completed   [kafka.controller.RequestSendThread]
2018-11-12 18:38:35,052 INFO   ||  [Controller id=1] Resigned   [kafka.controller.KafkaController]
2018-11-12 18:38:35,052 INFO   ||  Terminate ZkClient event thread.   [org.I0Itec.zkclient.ZkEventThread]
2018-11-12 18:38:35,053 INFO   ||  Processed session termination for sessionid: 0x1670a4a7fe40000   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:35,067 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,070 INFO   ||  Closed socket connection for client /127.0.0.1:47614 which had sessionid 0x1670a4a7fe40000   [org.apache.zookeeper.server.NIOServerCnxn]
2018-11-12 18:38:35,070 INFO   ||  Session: 0x1670a4a7fe40000 closed   [org.apache.zookeeper.ZooKeeper]
2018-11-12 18:38:35,070 INFO   ||  EventThread shut down for session: 0x1670a4a7fe40000   [org.apache.zookeeper.ClientCnxn]
2018-11-12 18:38:35,071 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:35,169 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,181 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,216 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,218 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,317 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,368 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,369 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,413 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,418 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,517 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,567 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,569 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,581 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,620 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,818 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,820 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:35,867 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,017 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,018 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Fetch]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Fetch]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Produce]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Produce]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Produce]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,043 INFO   ||  [ThrottledRequestReaper-Request]: Shutting down   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,044 INFO   ||  [ThrottledRequestReaper-Request]: Stopped   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,044 INFO   ||  [ThrottledRequestReaper-Request]: Shutdown completed   [kafka.server.ClientQuotaManager$ThrottledRequestReaper]
2018-11-12 18:38:36,044 INFO   ||  [SocketServer brokerId=1] Shutting down socket server   [kafka.network.SocketServer]
2018-11-12 18:38:36,054 INFO   ||  [SocketServer brokerId=1] Shutdown completed   [kafka.network.SocketServer]
2018-11-12 18:38:36,056 INFO   ||  [KafkaServer id=1] shut down completed   [kafka.server.KafkaServer]
2018-11-12 18:38:36,056 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:36,056 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/topicA-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:36,057 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.index   [kafka.log.OffsetIndex]
2018-11-12 18:38:36,057 INFO   ||  Deleting index /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/cluster/kafka/broker1/__consumer_offsets-0/00000000000000000000.timeindex   [kafka.log.TimeIndex]
2018-11-12 18:38:36,057 INFO   ||  Stopped Kafka server 1 at localhost:40373   [io.debezium.kafka.KafkaServer]
2018-11-12 18:38:36,057 INFO   ||  NIOServerCnxn factory exited run method   [org.apache.zookeeper.server.NIOServerCnxnFactory]
2018-11-12 18:38:36,057 INFO   ||  shutting down   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:36,057 ERROR  ||  ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes   [org.apache.zookeeper.server.ZooKeeperServer]
2018-11-12 18:38:36,057 INFO   ||  Shutting down   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:36,057 INFO   ||  Shutting down   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:36,057 INFO   ||  Shutting down   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:36,057 INFO   ||  PrepRequestProcessor exited loop!   [org.apache.zookeeper.server.PrepRequestProcessor]
2018-11-12 18:38:36,057 INFO   ||  SyncRequestProcessor exited!   [org.apache.zookeeper.server.SyncRequestProcessor]
2018-11-12 18:38:36,058 INFO   ||  shutdown of request processor complete   [org.apache.zookeeper.server.FinalRequestProcessor]
Skipped shouldStartClusterWithMultipleBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndRemoveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterWithOneBrokerAndLeaveData(io.debezium.kafka.KafkaClusterTest) because: long-running
Skipped shouldStartClusterAndAllowProducersAndConsumersToUseIt(io.debezium.kafka.KafkaClusterTest) because: long-running
18:38:36.062 [pool-58-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (54195 ms)
<> Total tests run: 34
<> Ignored tests: 0
<> Failed tests: 0

18:38:36.062 [pool-2-thread-1] ERROR fr.inria.lille.repair.nopol.NoPol - Error ExecutionException java.util.concurrent.ExecutionException: java.lang.RuntimeException: failingTestCasesValidated: nothing to repair, no failing test cases
18:38:36.062 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #655
18:38:36.062 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:388 which is executed by 23 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:388
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #656
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:387 which is executed by 23 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:387
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #657
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:383 which is executed by 23 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:383
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #658
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:35 which is executed by 14 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:35
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #659
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:364 which is executed by 14 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:364
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #660
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:133 which is executed by 14 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:133
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #661
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:98 which is executed by 25 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:98
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #662
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:92 which is executed by 25 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:92
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #663
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:85 which is executed by 25 tests
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:85
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #664
18:38:36.063 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:83 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:83
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #665
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:80 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:80
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #666
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:79 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:79
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #667
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:77 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:77
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #668
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:57 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:57
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #669
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:56 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:56
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #670
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:53 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:53
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #671
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:52 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:52
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #672
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:44 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:44
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #673
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:40 which is executed by 25 tests
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.recovery.ResilientOutputStreamBase:40
18:38:36.064 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #674
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:48 which is executed by 5 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:48
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #675
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Print:47 which is executed by 5 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Print:47
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #676
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:41 which is executed by 26 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:41
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #677
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:40 which is executed by 26 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:40
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #678
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:39 which is executed by 26 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:39
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #679
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:53 which is executed by 27 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:53
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #680
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Debug:84 which is executed by 3 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Debug:84
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #681
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.Testing$Debug:83 which is executed by 3 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.util.Testing$Debug:83
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #682
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:395 which is executed by 22 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:395
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #683
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:189 which is executed by 22 tests
18:38:36.065 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:189
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #684
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:157 which is executed by 22 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:157
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #685
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:396 which is executed by 23 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:396
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #686
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:102 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:102
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #687
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:439 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:439
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #688
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:188 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:188
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #689
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:184 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:184
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #690
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:182 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:182
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #691
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.MessageFormatter:156 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.MessageFormatter:156
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #692
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:74 which is executed by 24 tests
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:74
18:38:36.066 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #693
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:70 which is executed by 24 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:70
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #694
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.helpers.FormattingTuple:66 which is executed by 24 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.helpers.FormattingTuple:66
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #695
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:189 which is executed by 24 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:189
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #696
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.MessageConverter:23 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.MessageConverter:23
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #697
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.MessageConverter:26 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.MessageConverter:26
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #698
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:59 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:59
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #699
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:168 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:168
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #700
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.PatternLayout:165 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.PatternLayout:165
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #701
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:318 which is executed by 25 tests
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:318
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #702
18:38:36.067 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:315 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:315
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #703
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:310 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:310
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #704
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:309 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:309
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #705
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:308 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:308
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #706
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:307 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:307
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #707
2018-11-12 18:38:36,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:293 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:293
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #708
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:292 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:292
2018-11-12 18:38:36,068 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #709
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:283 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:283
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #710
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:282 which is executed by 25 tests
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:282
18:38:36.068 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #711
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:278 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:278
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #712
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:232 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:232
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #713
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:209 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:209
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #714
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:208 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:208
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #715
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:206 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:206
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #716
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:205 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:205
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #717
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:182 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:182
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #718
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:163 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:163
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #719
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:161 which is executed by 25 tests
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:161
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #720
18:38:36.069 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:160 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:160
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #721
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:152 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:152
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #722
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:148 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:148
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #723
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:23 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:23
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #724
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:26 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThreadConverter:26
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #725
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:57 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:57
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #726
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:51 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.FilterAttachableImpl:51
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #727
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:35 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:35
2018-11-12 18:38:36,070 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #728
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:101 which is executed by 25 tests
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:101
18:38:36.070 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #729
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:100 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.ThrowableProxyConverter:100
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #730
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.LayoutBase:44 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.LayoutBase:44
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #731
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:34 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:34
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #732
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:33 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:33
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #733
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:29 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:29
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #734
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:28 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator:28
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #735
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:781 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:781
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #736
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:442 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:442
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #737
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:441 which is executed by 25 tests
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:441
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #738
18:38:36.071 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:440 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:440
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #739
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:274 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:274
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #740
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:272 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:272
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #741
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:271 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:271
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #742
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:268 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:268
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #743
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:265 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:265
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #744
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:260 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:260
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #745
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:259 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:259
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #746
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:258 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:258
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #747
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Logger:257 which is executed by 25 tests
18:38:36.072 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Logger:257
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #748
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:51 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:51
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #749
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:49 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:49
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #750
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:48 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:48
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #751
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:47 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:47
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #752
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.util.CachingDateFormatter:46 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.util.CachingDateFormatter:46
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #753
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:45 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:45
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #754
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:44 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:44
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #755
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:37 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:37
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #756
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:36 which is executed by 25 tests
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.joran.spi.ConsoleTarget$1:36
18:38:36.073 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #757
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:43 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:43
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #758
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.Converter:42 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.Converter:42
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #759
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:71 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:71
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #760
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:62 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:62
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #761
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:56 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:56
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #762
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:54 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:54
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #763
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:48 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:48
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #764
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:45 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:45
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #765
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:44 which is executed by 25 tests
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:44
18:38:36.074 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #766
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:37 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:37
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #767
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:220 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:220
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #768
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:219 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:219
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #769
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:213 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:213
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #770
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:212 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:212
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #771
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:211 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:211
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #772
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:206 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:206
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #773
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:205 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:205
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #774
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:200 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:200
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #775
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:189 which is executed by 25 tests
18:38:36.075 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:189
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #776
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:188 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:188
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #777
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:104 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:104
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #778
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:103 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:103
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #779
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.OutputStreamAppender:99 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.OutputStreamAppender:99
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #780
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:137 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:137
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #781
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:117 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:117
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #782
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:97 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:97
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #783
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:96 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:96
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #784
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:95 which is executed by 25 tests
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:95
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #785
18:38:36.076 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:74 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:74
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #786
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:72 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:72
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #787
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:67 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.UnsynchronizedAppenderBase:67
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #788
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:215 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:215
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #789
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:162 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:162
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #790
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:161 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:161
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #791
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:51 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:51
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #792
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:49 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:49
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #793
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:48 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:48
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #794
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:47 which is executed by 25 tests
18:38:36.077 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:47
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #795
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:46 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.spi.AppenderAttachableImpl:46
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #796
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:122 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:122
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #797
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:120 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:120
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #798
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:119 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:119
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #799
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:118 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:118
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #800
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:117 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:117
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #801
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:116 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.PatternLayoutBase:116
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #802
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.LiteralConverter:25 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.LiteralConverter:25
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #803
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormatInfo:114 which is executed by 25 tests
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormatInfo:114
18:38:36.078 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #804
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormatInfo:106 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormatInfo:106
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #805
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LoggerConverter:21 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LoggerConverter:21
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #806
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:23 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:23
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #807
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:63 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:63
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #808
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.DateConverter:62 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.DateConverter:62
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #809
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:138 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:138
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #810
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:137 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:137
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #811
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:136 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:136
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #812
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:135 which is executed by 25 tests
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:135
18:38:36.079 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #813
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:134 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:134
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #814
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:122 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:122
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #815
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:121 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.encoder.LayoutWrappingEncoder:121
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #816
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:18 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:18
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #817
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:53 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:53
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #818
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:50 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:50
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #819
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.NamedConverter:48 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.NamedConverter:48
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #820
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:208 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:208
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #821
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:206 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:206
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #822
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.FileAppender:203 which is executed by 25 tests
18:38:36.080 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.FileAppender:203
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #823
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:19 which is executed by 25 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:19
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #824
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:22 which is executed by 25 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LineSeparatorConverter:22
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #825
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LevelConverter:23 which is executed by 25 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LevelConverter:23
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #826
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.pattern.LevelConverter:26 which is executed by 25 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.pattern.LevelConverter:26
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #827
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.Level:99 which is executed by 26 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.Level:99
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #828
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.spi.LoggingEvent:316 which is executed by 20 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.spi.LoggingEvent:316
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #829
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.core.pattern.FormattingConverter:69 which is executed by 23 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.core.pattern.FormattingConverter:69
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #830
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDateTimeCodec:28 which is executed by 2 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDateTimeCodec:28
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #831
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDateTimeCodec:31 which is executed by 2 tests
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDateTimeCodec:31
18:38:36.081 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #832
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:61 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:61
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #833
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:1099 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:1099
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #834
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:925 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:925
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #835
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:924 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:924
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #836
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:922 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:922
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #837
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:921 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:921
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #838
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:920 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:920
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #839
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:620 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:620
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #840
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:619 which is executed by 2 tests
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:619
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #841
18:38:36.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:618 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:618
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #842
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:612 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:612
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #843
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:608 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:608
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #844
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:604 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:604
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #845
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:600 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:600
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #846
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:596 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:596
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #847
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:592 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:592
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #848
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:588 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:588
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #849
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:584 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:584
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #850
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:580 which is executed by 2 tests
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:580
18:38:36.083 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #851
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:579 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:579
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #852
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:578 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:578
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #853
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:577 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:577
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #854
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:576 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:576
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #855
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:573 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:573
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #856
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:569 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:569
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #857
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:568 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:568
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #858
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:566 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:566
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #859
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:565 which is executed by 2 tests
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:565
18:38:36.084 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #860
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:564 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:564
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #861
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:498 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:498
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #862
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:495 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:495
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #863
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:494 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:494
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #864
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:491 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:491
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #865
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:488 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:488
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #866
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:484 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:484
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #867
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:483 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:483
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #868
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:478 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:478
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #869
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:476 which is executed by 2 tests
18:38:36.085 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:476
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #870
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:475 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:475
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #871
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:474 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:474
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #872
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:473 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:473
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #873
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:369 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:369
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #874
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:365 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:365
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #875
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:364 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:364
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #876
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:360 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:360
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #877
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:359 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:359
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #878
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:312 which is executed by 2 tests
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:312
18:38:36.086 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #879
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:308 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:308
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #880
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:305 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:305
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #881
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:304 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:304
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #882
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:303 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:303
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #883
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:302 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:302
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #884
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:298 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:298
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #885
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:293 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:293
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #886
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:292 which is executed by 2 tests
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:292
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #887
18:38:36.087 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:288 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:288
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #888
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:284 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:284
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #889
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:283 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:283
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #890
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:282 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:282
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #891
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:280 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:280
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #892
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:275 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:275
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #893
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:270 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:270
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #894
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:259 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:259
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #895
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:256 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:256
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #896
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:252 which is executed by 2 tests
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:252
18:38:36.088 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #897
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:251 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:251
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #898
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:247 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:247
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #899
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:243 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:243
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #900
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:242 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:242
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #901
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:241 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:241
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #902
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:240 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:240
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #903
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:236 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:236
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #904
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:169 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:169
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #905
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:168 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:168
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #906
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:167 which is executed by 2 tests
18:38:36.089 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:167
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #907
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:157 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:157
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #908
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:156 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:156
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #909
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:155 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:155
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #910
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:150 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:150
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #911
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:149 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:149
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #912
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:148 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:148
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #913
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:146 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:146
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #914
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:145 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:145
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #915
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:143 which is executed by 2 tests
18:38:36.090 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:143
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #916
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:142 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:142
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #917
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:140 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:140
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #918
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:139 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:139
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #919
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:136 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:136
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #920
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:135 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:135
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #921
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:134 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:134
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #922
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:133 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:133
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #923
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:128 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:128
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #924
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:127 which is executed by 2 tests
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:127
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #925
18:38:36.091 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:122 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:122
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #926
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:121 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:121
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #927
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:119 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:119
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #928
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:118 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:118
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #929
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:115 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:115
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #930
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:114 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:114
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #931
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:113 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:113
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #932
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:109 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:109
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #933
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:107 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:107
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #934
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:105 which is executed by 2 tests
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:105
18:38:36.092 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #935
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader:102 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader:102
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #936
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonTypeCodecMap:56 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonTypeCodecMap:56
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #937
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader$Context:843 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader$Context:843
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #938
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader$Context:834 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader$Context:834
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #939
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:60 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:60
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #940
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:49 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.ChildCodecRegistry:49
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #941
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDoubleCodec:28 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDoubleCodec:28
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #942
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDoubleCodec:31 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDoubleCodec:31
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #943
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:775 which is executed by 2 tests
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:775
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #944
18:38:36.093 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:771 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:771
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #945
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:770 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:770
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #946
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:768 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:768
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #947
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:767 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:767
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #948
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:764 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:764
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #949
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:755 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:755
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #950
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:751 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:751
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #951
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:743 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:743
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #952
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:742 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:742
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #953
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:733 which is executed by 2 tests
18:38:36.094 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:733
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #954
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:725 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:725
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #955
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:724 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:724
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #956
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:720 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:720
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #957
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:696 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:696
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #958
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:691 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:691
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #959
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:688 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:688
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #960
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:684 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:684
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #961
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:682 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:682
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #962
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:681 which is executed by 2 tests
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:681
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #963
18:38:36.095 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:587 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:587
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #964
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:586 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:586
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #965
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:582 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:582
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #966
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:579 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:579
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #967
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:461 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:461
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #968
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:460 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:460
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #969
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:459 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:459
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #970
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:455 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:455
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #971
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:454 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:454
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #972
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:453 which is executed by 2 tests
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:453
18:38:36.096 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #973
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:452 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:452
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #974
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:448 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:448
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #975
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:447 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:447
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #976
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:446 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:446
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #977
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:445 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:445
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #978
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:369 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:369
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #979
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:368 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:368
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #980
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:367 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:367
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #981
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:363 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:363
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #982
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:362 which is executed by 2 tests
18:38:36.097 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:362
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #983
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:360 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:360
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #984
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:356 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:356
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #985
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:353 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:353
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #986
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:349 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:349
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #987
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:346 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:346
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #988
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:342 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:342
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #989
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:341 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:341
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #990
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:339 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:339
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #991
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:335 which is executed by 2 tests
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:335
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #992
18:38:36.098 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:332 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:332
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #993
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:329 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:329
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #994
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:326 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:326
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #995
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:321 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:321
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #996
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:320 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:320
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #997
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:319 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:319
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #998
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:314 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:314
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #999
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:313 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:313
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1000
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:312 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:312
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1001
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:106 which is executed by 2 tests
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:106
18:38:36.099 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1002
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:91 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:91
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1003
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:90 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:90
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1004
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:82 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:82
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1005
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:81 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:81
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1006
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:72 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:72
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1007
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:66 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:66
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1008
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:65 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:65
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1009
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.AbstractBsonReader:48 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.AbstractBsonReader:48
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1010
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:530 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:530
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1011
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:527 which is executed by 2 tests
18:38:36.100 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:527
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1012
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:524 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:524
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1013
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:523 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:523
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1014
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:521 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:521
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1015
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:520 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:520
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1016
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:473 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:473
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1017
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:472 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:472
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1018
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:469 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:469
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1019
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:451 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:451
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1020
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:449 which is executed by 2 tests
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:449
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1021
18:38:36.101 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:448 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:448
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1022
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:447 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:447
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1023
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:445 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:445
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1024
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:444 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:444
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1025
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:443 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:443
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1026
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:442 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:442
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1027
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:438 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:438
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1028
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:354 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:354
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1029
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:353 which is executed by 2 tests
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:353
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1030
18:38:36.102 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:352 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:352
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1031
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:351 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:351
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1032
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:349 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:349
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1033
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:348 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:348
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1034
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:338 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:338
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1035
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:332 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:332
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1036
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:331 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:331
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1037
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:330 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:330
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1038
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:322 which is executed by 2 tests
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:322
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1039
18:38:36.103 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:321 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:321
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1040
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:320 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:320
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1041
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:319 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:319
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1042
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:306 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:306
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1043
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:305 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:305
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1044
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:303 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:303
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1045
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:268 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:268
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1046
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:267 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:267
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1047
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:259 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:259
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1048
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:257 which is executed by 2 tests
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:257
18:38:36.104 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1049
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:256 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:256
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1050
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:252 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:252
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1051
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:248 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:248
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1052
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:243 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:243
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1053
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:242 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:242
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1054
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:240 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:240
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1055
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:236 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:236
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1056
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:234 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:234
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1057
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:103 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:103
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1058
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:102 which is executed by 2 tests
18:38:36.105 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:102
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1059
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:98 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:98
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1060
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:95 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:95
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1061
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:93 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:93
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1062
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:87 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:87
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1063
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:85 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:85
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1064
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:83 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:83
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1065
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:81 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:81
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1066
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:79 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:79
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1067
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:75 which is executed by 2 tests
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:75
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1068
18:38:36.106 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:73 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:73
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1069
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:72 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:72
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1070
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonScanner:71 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonScanner:71
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1071
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:66 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:66
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1072
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:59 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:59
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1073
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:46 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:46
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1074
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:43 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:43
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1075
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:41 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:41
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1076
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:40 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:40
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1077
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonToken:36 which is executed by 2 tests
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonToken:36
18:38:36.107 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1078
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.DecoderContext$Builder:47 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.DecoderContext$Builder:47
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1079
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:37 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:37
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1080
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:102 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:102
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1081
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:72 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:72
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1082
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:70 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:70
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1083
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:67 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:67
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1084
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:66 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:66
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1085
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:65 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:65
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1086
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonArrayCodec:63 which is executed by 2 tests
18:38:36.108 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonArrayCodec:63
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1087
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader$Context:1133 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader$Context:1133
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1088
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonReader$Context:1129 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonReader$Context:1129
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1089
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonStringCodec:28 which is executed by 4 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonStringCodec:28
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1090
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonStringCodec:31 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonStringCodec:31
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1091
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:41 which is executed by 4 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:41
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1092
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:101 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:101
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1093
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:89 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:89
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1094
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:87 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:87
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1095
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:85 which is executed by 2 tests
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:85
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1096
18:38:36.109 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:84 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:84
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1097
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:83 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:83
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1098
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:82 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:82
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1099
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:81 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:81
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1100
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonDocumentCodec:79 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonDocumentCodec:79
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1101
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonElement:55 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonElement:55
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1102
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonElement:46 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonElement:46
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1103
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonValueCodecProvider:74 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonValueCodecProvider:74
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1104
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonInt32Codec:28 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonInt32Codec:28
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1105
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.BsonInt32Codec:31 which is executed by 2 tests
18:38:36.110 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.BsonInt32Codec:31
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1106
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.DecoderContext:32 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.DecoderContext:32
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1107
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:60 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:60
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1108
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:53 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:53
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1109
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:51 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:51
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1110
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:50 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:50
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1111
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:49 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:49
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1112
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:44 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:44
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1113
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:40 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:40
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1114
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:38 which is executed by 2 tests
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:38
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1115
18:38:36.111 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.json.JsonBuffer:30 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.json.JsonBuffer:30
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1116
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.LazyCodec:55 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.LazyCodec:55
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1117
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.LazyCodec:52 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.LazyCodec:52
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1118
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.LazyCodec:51 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.LazyCodec:51
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1119
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.codecs.configuration.LazyCodec:47 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.codecs.configuration.LazyCodec:47
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1120
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.bson.BsonDocument:62 which is executed by 2 tests
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.bson.BsonDocument:62
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1121
18:38:36.112 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.jdbc.TemporalPrecisionMode:41 which is executed by 2 tests
2018-11-12 18:38:36,168 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,183 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,265 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,270 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,271 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,271 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,272 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,320 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-173296864
2018-11-12 18:38:36,372 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:36.374 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1122
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:44 which is executed by 3 tests
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:44
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1123
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:32 which is executed by 3 tests
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:32
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1124
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.junit.SkipTestRule:31 which is executed by 3 tests
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation io.debezium.junit.SkipTestRule:31
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1125
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:250 which is executed by 98 tests
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:250
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1126
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.LoggerContext:249 which is executed by 98 tests
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.LoggerContext:249
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1127
18:38:36.375 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.data.VariableScaleDecimal:50 which is executed by 1 tests
2018-11-12 18:38:36,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,471 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,500 INFO   ||  SessionTrackerImpl exited loop!   [org.apache.zookeeper.server.SessionTrackerImpl]
2018-11-12 18:38:36,521 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-1999701148
18:38:36.556 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1128
18:38:36.556 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.data.VariableScaleDecimal:35 which is executed by 1 tests
2018-11-12 18:38:36,584 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,666 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-1999701148
18:38:36.727 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1129
18:38:36.727 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:75 which is executed by 1 tests
2018-11-12 18:38:36,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,820 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:36,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:36.900 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1130
18:38:36.900 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:74 which is executed by 1 tests
2018-11-12 18:38:36,970 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,020 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,021 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,036 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:37.082 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:37,170 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,171 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,173 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:37,224 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,268 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,272 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,322 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,322 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,324 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,372 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,472 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,474 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,474 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,521 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,553 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:37,568 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,573 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,575 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,624 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,673 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,737 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,822 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,871 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,922 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,925 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:37,972 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,022 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,075 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,138 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,175 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,225 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,373 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,373 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,374 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,376 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,419 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,422 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,470 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,476 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,524 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,525 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,526 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,526 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,573 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,688 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,723 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,749 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:38,807 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:38,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,827 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,832 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:38,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:38,924 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,718 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,719 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,720 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,721 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,767 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,769 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,770 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,918 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,940 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:39,940 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:39,940 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:39,970 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,971 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:39,973 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,108 WARN   SimpleSourceConnectorOutputTest|runner|gen-expected  Flush of EmbeddedConnector{id=simple-connector-1} offsets interrupted, cancelling   [io.debezium.embedded.EmbeddedEngine]
2018-11-12 18:38:40,108 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:40.115 [pool-186-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (3012 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-11-12 18:38:40,570 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
2018-11-12 18:38:40,572 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:40.572 [pool-189-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (81 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:40.574 [pool-190-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:40.576 [pool-191-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:40.577 [pool-192-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:40.578 [pool-193-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:40.580 [pool-194-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:40.581 [pool-185-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:40.581 [pool-185-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:40.582 [pool-185-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:74.
18:38:40.582 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1131
18:38:40.582 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:73 which is executed by 1 tests
2018-11-12 18:38:40,621 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,671 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,674 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,674 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,722 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:40.760 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:40,771 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,772 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,775 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,821 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,825 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:40,871 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,872 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,873 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,873 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,921 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,922 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,923 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,923 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,923 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:40,927 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,024 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,124 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,198 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:41,576 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,578 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,578 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,623 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,676 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,726 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,728 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,774 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,775 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,776 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,778 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,823 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,824 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,825 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,828 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,873 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,874 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,876 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,924 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,925 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,928 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,974 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:41,978 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,025 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,028 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,029 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,125 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,129 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,176 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,226 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,293 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:42,348 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:42,371 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:42,475 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,580 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,628 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,628 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,670 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:42,670 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:42,670 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:42,675 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,676 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,725 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,818 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:42.819 [pool-196-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (2039 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-11-12 18:38:42,826 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,875 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,879 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,926 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,927 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,928 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,928 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,930 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,977 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,978 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:42,980 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,027 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,028 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,077 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,077 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-11-12 18:38:43,131 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,131 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,179 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:43.193 [pool-199-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (82 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:43.195 [pool-200-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:43.196 [pool-201-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:43.198 [pool-202-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:43.199 [pool-203-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:43.200 [pool-204-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:43.202 [pool-195-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:43.202 [pool-195-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:43.202 [pool-195-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:73.
18:38:43.202 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1132
18:38:43.202 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:72 which is executed by 1 tests
-936045199
18:38:43.378 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:43,379 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,477 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:43,577 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,632 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,681 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,682 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,727 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,780 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,781 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,797 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:43,828 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,829 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,829 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,829 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,830 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,881 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,932 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,979 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,979 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,981 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:43,983 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,028 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,029 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,079 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,130 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,179 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,180 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,233 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,379 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,531 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,579 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,583 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,632 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,679 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,731 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,732 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,733 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,783 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,829 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,830 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,831 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,834 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,882 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,893 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:44,932 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,933 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,933 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,934 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:44,941 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:44,962 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:45,030 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,031 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,034 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,081 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,081 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,082 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,083 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,131 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,131 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,135 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,260 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:45,261 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:45,261 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:45,282 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,406 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:45.407 [pool-206-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (2010 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-11-12 18:38:45,431 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,435 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,482 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,531 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,534 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,581 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-11-12 18:38:45,735 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,735 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:45.769 [pool-209-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (83 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:45.771 [pool-210-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:45.773 [pool-211-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:45.774 [pool-212-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:45.776 [pool-213-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:45.778 [pool-214-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:45.780 [pool-205-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:45.780 [pool-205-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:45.780 [pool-205-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:72.
18:38:45.780 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1133
18:38:45.780 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:71 which is executed by 1 tests
2018-11-12 18:38:45,782 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,782 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,784 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,832 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,835 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,883 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,885 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,885 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,934 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:45.951 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1134
18:38:45.951 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:70 which is executed by 1 tests
2018-11-12 18:38:45,983 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,985 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:45,985 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,033 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,033 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,034 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,034 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,035 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,037 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,085 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:46.121 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:46,132 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,134 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,183 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:46,334 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,383 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,384 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,385 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,387 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,433 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,483 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,533 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:46,586 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,637 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,683 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,736 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,737 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,786 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,787 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,836 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,885 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,885 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,887 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,888 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,986 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:46,986 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,036 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,084 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,086 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,087 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,087 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,087 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,136 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,136 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,137 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,137 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,186 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,234 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,285 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,386 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,437 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,535 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,629 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:47,635 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,638 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,638 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,676 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:47,685 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,689 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,698 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:47,737 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,737 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,788 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,887 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,888 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,888 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,938 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,938 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,938 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,987 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,988 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,988 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:47,996 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:47,997 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:47,997 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:48,038 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,040 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,134 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:48.135 [pool-216-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (1995 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-11-12 18:38:48,186 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,188 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,189 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,236 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,289 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,289 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,290 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,336 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,338 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,339 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-11-12 18:38:48,487 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,490 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:48.495 [pool-219-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (82 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:48.497 [pool-220-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:48.499 [pool-221-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:48.501 [pool-222-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:48.503 [pool-223-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:48.505 [pool-224-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:48.506 [pool-215-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:48.506 [pool-215-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:48.506 [pool-215-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:70.
18:38:48.507 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1135
18:38:48.507 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:69 which is executed by 1 tests
2018-11-12 18:38:48,588 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,589 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,590 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,590 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:48.679 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:48,687 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,690 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,740 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:48,840 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,840 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,890 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,890 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,890 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,940 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,940 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,940 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,991 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:48,992 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,040 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,041 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,042 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,091 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,120 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:49,188 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,191 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,192 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,389 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,391 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,391 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,391 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,442 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,539 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,541 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,541 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,542 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,639 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,642 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,792 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,792 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,792 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,842 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,844 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,889 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,892 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,942 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,943 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,992 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,992 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:49,994 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,092 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,092 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,094 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,142 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,143 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,243 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,293 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,294 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,316 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:50,343 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,363 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:50,385 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:50,391 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,393 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,393 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,393 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,491 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,544 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,544 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,591 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,591 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,640 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,643 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,643 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,685 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:50,685 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:50,685 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:50,741 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,743 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,743 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,745 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,828 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:50.830 [pool-226-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (2121 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-11-12 18:38:50,844 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,844 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,944 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,944 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:50,946 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,044 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,094 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,144 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,145 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,195 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,195 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-11-12 18:38:51,245 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,247 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:51.270 [pool-229-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (92 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:51.272 [pool-230-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:51.274 [pool-231-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:51.276 [pool-232-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:51.278 [pool-233-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:51.279 [pool-234-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:51.281 [pool-225-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:51.281 [pool-225-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:51.281 [pool-225-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:69.
18:38:51.281 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1136
18:38:51.281 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.util.LoggingContext:68 which is executed by 1 tests
2018-11-12 18:38:51,295 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,295 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,296 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,442 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,442 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
-936045199
18:38:51.492 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:51,493 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,496 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,496 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,543 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,545 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,593 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,596 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,596 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,597 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
2018-11-12 18:38:51,696 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,746 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,746 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,794 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,797 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,846 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,846 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,847 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,895 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,946 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,947 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:51,960 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:51,998 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,147 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,147 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,148 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,197 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,197 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,197 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,297 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,398 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,447 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,447 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,495 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,545 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,548 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,548 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,549 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,595 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,597 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,648 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,695 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,695 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,698 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,746 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,748 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,749 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,797 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,798 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,798 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,847 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,847 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,848 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,848 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:52,999 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,050 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,098 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,098 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,156 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:53,208 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Starting FileOffsetBackingStore with file /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/target/data/integration-test-connector-offsets.data   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
2018-11-12 18:38:53,340 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:53,347 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,347 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,348 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,396 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,397 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,498 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,498 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,498 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,547 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,547 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,597 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,597 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,597 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,598 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,640 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:53,640 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroConverterConfig values: 
	schema.registry.url = [http://fake-url]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
   [io.confluent.connect.avro.AvroConverterConfig]
2018-11-12 18:38:53,641 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  AvroDataConfig values: 
	schemas.cache.config = 1000
	enhanced.avro.schema.support = false
	connect.meta.data = true
   [io.confluent.connect.avro.AvroDataConfig]
2018-11-12 18:38:53,648 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,697 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,698 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,698 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,699 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,747 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,748 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,773 INFO   SimpleSourceConnectorOutputTest|runner|gen-expected  Stopped FileOffsetBackingStore   [org.apache.kafka.connect.storage.FileOffsetBackingStore]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:53.775 [pool-236-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (2263 ms)
<> Total tests run: 5
<> Ignored tests: 0
<> Failed tests: 3
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

2018-11-12 18:38:53,798 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,798 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,848 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,898 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,899 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,948 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,998 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:53,999 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,049 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:54.131 [pool-239-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (81 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:54.133 [pool-240-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:54.135 [pool-241-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)
18:38:54.136 [pool-242-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInTwoSteps(SimpleSourceConnectorOutputTest.java:101)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)
18:38:54.138 [pool-243-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (1 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStepWithTimestamps(SimpleSourceConnectorOutputTest.java:122)

java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)
    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)
18:38:54.139 [pool-244-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (0 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest)
[Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
java.lang.AssertionError: Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)    at org.junit.Assert.fail(Assert.java:88)
    at io.debezium.embedded.ConnectorOutputTest$TestSpecification.withConfiguration(ConnectorOutputTest.java:336)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:642)
    at io.debezium.embedded.ConnectorOutputTest.usingSpec(ConnectorOutputTest.java:620)
    at io.debezium.embedded.ConnectorOutputTest.runConnector(ConnectorOutputTest.java:713)
    at io.debezium.connector.simple.SimpleSourceConnectorOutputTest.shouldRunConnectorFromFilesInOneStep(SimpleSourceConnectorOutputTest.java:93)

18:38:54.141 [pool-235-thread-1] DEBUG tests.output - Failing tests with false: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:54.141 [pool-235-thread-1] DEBUG tests.output - Failing tests with true: 
[shouldRunConnectorFromFilesInTwoSteps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/b/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStepWithTimestamps(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/d/connector.properties (No such file or directory), shouldRunConnectorFromFilesInOneStep(io.debezium.connector.simple.SimpleSourceConnectorOutputTest): Failed to read the configuration file '/tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties': /tmp/Nopol_Bears_debezium-debezium_351465554-354212780/src/test/resources/simple/test/a/connector.properties (No such file or directory)]
18:38:54.141 [pool-235-thread-1] INFO  f.i.l.r.n.synth.SMTNopolSynthesizer - Not enough specifications: 0. A trivial patch is "true" or "false", please write new tests specifying SourceLocation io.debezium.util.LoggingContext:68.
18:38:54.141 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1137
18:38:54.141 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:188 which is executed by 1 tests
18:38:54.141 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:188
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1138
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:184 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:184
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1139
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:117 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:117
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1140
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:116 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:116
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1141
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:112 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:112
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1142
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation org.slf4j.MDC:109 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation org.slf4j.MDC:109
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1143
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:187 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:187
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1144
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:186 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:186
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1145
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:185 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:185
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1146
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:184 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:184
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1147
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:111 which is executed by 1 tests
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:111
18:38:54.142 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1148
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:109 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:109
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1149
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:108 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:108
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1150
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:107 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:107
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1151
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:106 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:106
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1152
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:105 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:105
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1153
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:103 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:103
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1154
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:102 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:102
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1155
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:98 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:98
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1156
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:83 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:83
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1157
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:82 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:82
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1158
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:74 which is executed by 1 tests
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:74
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1159
18:38:54.143 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:73 which is executed by 1 tests
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:73
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1160
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:69 which is executed by 1 tests
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:69
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1161
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:65 which is executed by 1 tests
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:65
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1162
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:64 which is executed by 1 tests
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:64
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1163
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:63 which is executed by 1 tests
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - cannot spoon SourceLocation ch.qos.logback.classic.util.LogbackMDCAdapter:63
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1164
18:38:54.144 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.relational.ddl.DataType:28 which is executed by 1 tests
2018-11-12 18:38:54,249 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,249 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
1853714981
18:38:54.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1165
18:38:54.316 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.ParsingException:50 which is executed by 1 tests
2018-11-12 18:38:54,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,350 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,400 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,449 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,450 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,450 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2127061920
18:38:54.483 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1166
18:38:54.483 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.TokenStream:2699 which is executed by 1 tests
2018-11-12 18:38:54,500 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,500 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,550 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,600 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,601 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,650 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,650 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,650 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,651 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,699 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,700 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,700 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,700 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,701 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,701 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
175865530
18:38:54.742 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - statement #1167
18:38:54.742 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - Analysing SourceLocation io.debezium.text.TokenStream:737 which is executed by 1 tests
2018-11-12 18:38:54,849 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,900 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:54,951 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
175865530
18:38:54.986 [pool-2-thread-1] DEBUG fr.inria.lille.repair.nopol.NoPol - looking with class fr.inria.lille.repair.nopol.spoon.smt.ConditionalAdder
2018-11-12 18:38:55,000 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,101 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,102 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,152 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,201 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,252 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,252 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2018-11-12 18:38:55,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,353 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,402 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,402 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,453 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,501 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,502 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,504 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,553 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,554 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,554 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:55.562 [pool-246-thread-1] INFO  xxl.java.junit.TestCasesListener - Tests run finished (560 ms)
<> Total tests run: 86
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

2018-11-12 18:38:55,602 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,655 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,655 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,753 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,753 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,803 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,804 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,852 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,953 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,953 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:55,954 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,056 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,106 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,205 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,256 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,305 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,306 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,307 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,355 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,356 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,356 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,356 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,456 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,507 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,557 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,606 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,656 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,657 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,657 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,706 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,706 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,808 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,808 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,856 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,857 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,908 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:56,958 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,007 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,157 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,157 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,207 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,308 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,409 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,458 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,459 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-11-12 18:38:57,509 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,510 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-11-12 18:38:57,559 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,560 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,560 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:57.584 [pool-247-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (120 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 0

io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
18:38:57.590 [pool-248-thread-1] INFO  f.i.l.c.t.SpecificationTestCasesListener - Tests run finished (4 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

18:38:57.597 [pool-249-thread-1] INFO  f.i.l.r.n.s.ConstraintModelBuilder$1PassingListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 0

io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
18:38:57.601 [pool-250-thread-1] INFO  f.i.l.r.n.s.ConstraintModelBuilder$1PassingListener - Tests run finished (3 ms)
<> Total tests run: 1
<> Ignored tests: 0
<> Failed tests: 1
~ shouldParseAlterTableStatementAddConstraintUniqueKey(io.debezium.connector.mysql.MySqlDdlParserTest)
[Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');']
io.debezium.text.ParsingException: Failed to parse statement 'ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');'    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:292)
    at io.debezium.relational.ddl.DdlParser.parse(DdlParser.java:267)
    at io.debezium.connector.mysql.MySqlDdlParserTest.shouldParseAlterTableStatementAddConstraintUniqueKey(MySqlDdlParserTest.java:475)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

2018-11-12 18:38:57,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,609 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,659 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,661 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,661 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,709 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,709 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,710 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,760 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,810 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,811 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,861 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:57,910 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,061 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:58.083 [pool-245-thread-1] DEBUG f.i.l.c.s.ConstraintBasedSynthesis - Operators:
2018-11-12 18:38:58,113 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,161 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:58.221 [pool-245-thread-1] DEBUG f.i.l.c.s.ConstraintBasedSynthesis - Successful code synthesis: msg.length()==0
18:38:58.223 [pool-245-thread-1] INFO  f.i.l.repair.nopol.patch.TestPatch - Applying patch: io.debezium.text.TokenStream:737: PRECONDITION msg.length()==0
2018-11-12 18:38:58,262 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,311 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,361 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,362 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,412 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,511 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,513 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,563 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,662 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,762 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,763 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,767 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,813 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,817 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,862 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,867 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:58,914 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,114 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,213 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,213 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,218 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,264 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,414 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,464 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,468 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,514 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,515 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,615 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,616 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,617 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,618 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,619 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,663 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,664 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,669 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,717 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,765 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,767 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,769 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,814 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,816 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,819 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,867 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,915 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:38:59,915 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:38:59.935 [pool-245-thread-1] INFO  f.i.l.repair.nopol.patch.TestPatch - Running test suite to check the patch "msg.length()==0" is working
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/tdurieux/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/tdurieux/defects4j4repair/script/../repair_tools/nopol.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
18:39:00.065 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - ----INFORMATION----
2018-11-12 18:39:00,221 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,267 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,317 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,366 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,371 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,466 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,467 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb classes : 243
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb methods : 2200
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb Statements Analyzed : 16
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb Statements with Angelic Value Found : 1
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb inputs in SMT : 2
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb SMT level: 1
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb SMT components: [0] []
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb variables in SMT : 40
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - NoPol Execution time : 123415ms
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - 
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - ----PATCH FOUND----
18:39:00.474 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - msg.length()==0
18:39:00.500 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - Nb test that executes the patch: 1
18:39:00.500 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - io.debezium.text.TokenStream:737: PRECONDITION
2018-11-12 18:39:00,521 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
18:39:00.550 [pool-2-thread-1] INFO  fr.inria.lille.repair.nopol.NoPol - --- a/debezium-core/src/main/java/io/debezium/text/TokenStream.java
+++ b/debezium-core/src/main/java/io/debezium/text/TokenStream.java
@@ -736,3 +736,5 @@
                     + "': " + fragment;
-            throw new ParsingException(pos, msg);
+            if (msg.length()==0) {
+                throw new ParsingException(pos, msg);
+            }
         }

2018-11-12 18:39:00,566 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,653 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,653 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,654 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
2018-11-12 18:39:00,655 WARN   ||  [AdminClient clientId=my-db-history] Connection to node 1 could not be established. Broker may not be available.   [org.apache.kafka.clients.NetworkClient]
PATCH
245.92user 11.62system 2:06.70elapsed 203%CPU (0avgtext+0avgdata 3326564maxresident)k
103416inputs+23240outputs (64major+1003911minor)pagefaults 0swaps


Node: granduc-16.luxembourg.grid5000.fr



Date: Mon Nov 12 18:39:01 EST 2018

